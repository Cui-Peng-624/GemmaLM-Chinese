{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoDL官方学术资源加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from rouge_chinese import Rouge # type: ignore\n",
    "from nltk.translate.bleu_score import sentence_bleu # type: ignore\n",
    "import jieba # type: ignore\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到Python路径\n",
    "project_root = \"/home/cuipeng/Gemma\"\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# 导入必要模块\n",
    "from src.core.model.model_initializer import initialize_model_and_tokenizer\n",
    "from src.core.utils.model_utils import generate_response, apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self):\n",
    "        # 初始化基础模型和微调后的模型\n",
    "        print(\"正在加载模型...\")\n",
    "        self.base_model, self.base_tokenizer = initialize_model_and_tokenizer(\n",
    "            model_path=\"google/gemma-2-9b\",\n",
    "            cache_dir=\"/root/autodl-tmp/gemma\",\n",
    "            use_quantization=True\n",
    "        )\n",
    "        \n",
    "        self.ft_model, self.ft_tokenizer = initialize_model_and_tokenizer(\n",
    "            model_path=\"google/gemma-2-9b\",\n",
    "            cache_dir=\"/root/autodl-tmp/gemma\",\n",
    "            lora_path=\"/root/autodl-tmp/models/stage1/checkpoints/gemma-base-zh/checkpoint-43500\",\n",
    "            use_quantization=True\n",
    "        )\n",
    "        \n",
    "        self.rouge = Rouge()\n",
    "        print(\"模型加载完成！\")\n",
    "    \n",
    "    def load_evaluation_datasets(self, test_size=100):\n",
    "        \"\"\"加载多个评估数据集\"\"\"\n",
    "        evaluation_data = []\n",
    "        \n",
    "        # 1. LCCC对话数据集\n",
    "        print(\"加载LCCC数据集...\")\n",
    "        lccc = load_dataset(\"silver/lccc\", \"base\", split=\"test\")\n",
    "        lccc_samples = self._process_lccc_data(lccc, test_size)\n",
    "        evaluation_data.extend(lccc_samples)\n",
    "        \n",
    "        # # 2. MKQA多语言问答数据集\n",
    "        # print(\"加载MKQA数据集...\")\n",
    "        # mkqa = load_dataset(\"mkqa\", split=\"test\")\n",
    "        # mkqa_samples = self._process_mkqa_data(mkqa, test_size)\n",
    "        # evaluation_data.extend(mkqa_samples)\n",
    "        \n",
    "        # 3. CMRC2018阅读理解数据集\n",
    "        print(\"加载CMRC2018数据集...\")\n",
    "        cmrc = load_dataset(\"hfl/cmrc2018\", split=\"test\")\n",
    "        cmrc_samples = self._process_cmrc_data(cmrc, test_size)\n",
    "        evaluation_data.extend(cmrc_samples)\n",
    "        \n",
    "        return evaluation_data\n",
    "    \n",
    "    def _process_lccc_data(self, dataset, size):\n",
    "        \"\"\"处理LCCC对话数据集\"\"\"\n",
    "        samples = []\n",
    "        for item in dataset[:size]:\n",
    "            if len(item['conversation']) >= 2:\n",
    "                samples.append({\n",
    "                    'instruction': item['conversation'][-2],\n",
    "                    'target': item['conversation'][-1],\n",
    "                    'type': 'dialogue'\n",
    "                })\n",
    "        return samples\n",
    "    \n",
    "    # def _process_mkqa_data(self, dataset, size):\n",
    "    #     \"\"\"处理MKQA问答数据集\"\"\"\n",
    "    #     samples = []\n",
    "    #     for item in dataset[:size]:\n",
    "    #         if item['queries']['zh_cn'] and item['answers']['zh_cn']:\n",
    "    #             samples.append({\n",
    "    #                 'instruction': item['queries']['zh_cn'],\n",
    "    #                 'target': item['answers']['zh_cn'][0]['text'],\n",
    "    #                 'type': 'qa'\n",
    "    #             })\n",
    "    #     return samples\n",
    "    \n",
    "    def _process_cmrc_data(self, dataset, size):\n",
    "        \"\"\"处理CMRC2018阅读理解数据集\"\"\"\n",
    "        samples = []\n",
    "        for item in dataset[:size]:\n",
    "            context = item['context']\n",
    "            for qa in item['qas']:\n",
    "                samples.append({\n",
    "                    'instruction': f\"请根据以下文章回答问题：\\n\\n{context}\\n\\n问题：{qa['question']}\",\n",
    "                    'target': qa['answers'][0],\n",
    "                    'type': 'reading_comprehension'\n",
    "                })\n",
    "        return samples\n",
    "    \n",
    "    def generate_responses(self, prompt: str, model, tokenizer) -> str:\n",
    "        \"\"\"生成模型回答\"\"\"\n",
    "        return generate_response(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    def calculate_metrics(self, reference: str, hypothesis: str) -> Dict:\n",
    "        \"\"\"计算评估指标\"\"\"\n",
    "        # 分词\n",
    "        ref_tokens = ' '.join(jieba.cut(reference))\n",
    "        hyp_tokens = ' '.join(jieba.cut(hypothesis))\n",
    "        \n",
    "        # 计算ROUGE分数\n",
    "        rouge_scores = self.rouge.get_scores(hyp_tokens, ref_tokens)[0]\n",
    "        \n",
    "        # 计算BLEU分数\n",
    "        ref_tokens = list(jieba.cut(reference))\n",
    "        hyp_tokens = list(jieba.cut(hypothesis))\n",
    "        bleu_score = sentence_bleu([ref_tokens], hyp_tokens)\n",
    "        \n",
    "        return {\n",
    "            'rouge-1': rouge_scores['rouge-1']['f'],\n",
    "            'rouge-2': rouge_scores['rouge-2']['f'],\n",
    "            'rouge-l': rouge_scores['rouge-l']['f'],\n",
    "            'bleu': bleu_score\n",
    "        }\n",
    "    \n",
    "    def evaluate_models(self, evaluation_data):\n",
    "        \"\"\"评估模型性能\"\"\"\n",
    "        results = {\n",
    "            'dialogue': {'base': [], 'ft': []},\n",
    "            'qa': {'base': [], 'ft': []},\n",
    "            'reading_comprehension': {'base': [], 'ft': []}\n",
    "        }\n",
    "        \n",
    "        for item in tqdm(evaluation_data, desc=\"评估进度\"):\n",
    "            # 构建对话\n",
    "            dialogue = [\n",
    "                {\"role\": \"system\", \"content\": \"你是一个专业、友好的AI助手。\"},\n",
    "                {\"role\": \"user\", \"content\": item['instruction']}\n",
    "            ]\n",
    "            prompt = apply_chat_template(dialogue)\n",
    "            \n",
    "            # 生成回答\n",
    "            base_response = self.generate_responses(prompt, self.base_model, self.base_tokenizer)\n",
    "            ft_response = self.generate_responses(prompt, self.ft_model, self.ft_tokenizer)\n",
    "            \n",
    "            # 计算指标\n",
    "            base_metric = self.calculate_metrics(item['target'], base_response)\n",
    "            ft_metric = self.calculate_metrics(item['target'], ft_response)\n",
    "            \n",
    "            # 按任务类型存储结果\n",
    "            results[item['type']]['base'].append(base_metric)\n",
    "            results[item['type']]['ft'].append(ft_metric)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_metrics(metrics_list):\n",
    "    \"\"\"计算平均指标\"\"\"\n",
    "    if not metrics_list:\n",
    "        return None\n",
    "    return {k: np.mean([m[k] for m in metrics_list]) for k in metrics_list[0].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 初始化评估器\n",
    "    evaluator = ModelEvaluator()\n",
    "    \n",
    "    # 加载评估数据集\n",
    "    print(\"\\n加载评估数据集...\")\n",
    "    evaluation_data = evaluator.load_evaluation_datasets(test_size=50)  # 每个数据集50个样本\n",
    "    \n",
    "    # 评估模型\n",
    "    print(\"\\n开始评估模型...\")\n",
    "    results = evaluator.evaluate_models(evaluation_data)\n",
    "    \n",
    "    # 输出结果\n",
    "    task_types = ['dialogue', 'qa', 'reading_comprehension']\n",
    "    \n",
    "    for task in task_types:\n",
    "        print(f\"\\n=== {task} 任务评估结果 ===\")\n",
    "        \n",
    "        base_avg = calculate_average_metrics(results[task]['base'])\n",
    "        ft_avg = calculate_average_metrics(results[task]['ft'])\n",
    "        \n",
    "        if base_avg and ft_avg:\n",
    "            print(f\"\\n基础模型评估结果:\")\n",
    "            for metric, score in base_avg.items():\n",
    "                print(f\"{metric}: {score:.4f}\")\n",
    "                \n",
    "            print(f\"\\n微调后模型评估结果:\")\n",
    "            for metric, score in ft_avg.items():\n",
    "                print(f\"{metric}: {score:.4f}\")\n",
    "                \n",
    "            print(f\"\\n性能提升:\")\n",
    "            for metric in base_avg.keys():\n",
    "                improvement = ((ft_avg[metric] - base_avg[metric]) / base_avg[metric]) * 100\n",
    "                print(f\"{metric}: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071f90c68ad4499282faa66584793006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598d266056f04d80885cddfce7c01348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/Gemma/lib/python3.12/site-packages/peft/tuners/adalora/config.py:78: UserWarning: Note that `r` is not used in AdaLora and will be ignored.If you intended to set the initial rank, use `init_r` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成！\n",
      "\n",
      "加载评估数据集...\n",
      "加载LCCC数据集...\n"
     ]
    },
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'thu-coai/lccc-base' doesn't exist on the Hub or cannot be accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 加载评估数据集\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m加载评估数据集...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m evaluation_data \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mload_evaluation_datasets(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)  \u001b[38;5;66;03m# 每个数据集50个样本\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m开始评估模型...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mModelEvaluator.load_evaluation_datasets\u001b[0;34m(self, test_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 1. LCCC对话数据集\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m加载LCCC数据集...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m lccc \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthu-coai/lccc-base\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m lccc_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_lccc_data(lccc, test_size)\n\u001b[1;32m     29\u001b[0m evaluation_data\u001b[38;5;241m.\u001b[39mextend(lccc_samples)\n",
      "File \u001b[0;32m~/miniconda3/envs/Gemma/lib/python3.12/site-packages/datasets/load.py:2132\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2128\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2129\u001b[0m )\n\u001b[1;32m   2131\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2132\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   2133\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   2134\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   2135\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   2136\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   2137\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   2138\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m   2139\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   2140\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   2141\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   2142\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   2143\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   2144\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m   2145\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2146\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[1;32m   2147\u001b[0m )\n\u001b[1;32m   2149\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/Gemma/lib/python3.12/site-packages/datasets/load.py:1853\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1852\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1853\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1854\u001b[0m     path,\n\u001b[1;32m   1855\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1856\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1857\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1858\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1859\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1860\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1861\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m   1862\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39m_require_default_config_name,\n\u001b[1;32m   1863\u001b[0m     _require_custom_configs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(config_kwargs),\n\u001b[1;32m   1864\u001b[0m )\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/Gemma/lib/python3.12/site-packages/datasets/load.py:1717\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[0;32m-> 1717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/miniconda3/envs/Gemma/lib/python3.12/site-packages/datasets/load.py:1643\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[1;32m   1640\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1641\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hub or cannot be accessed.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1645\u001b[0m     dataset_script_path \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[1;32m   1646\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   1647\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m   1651\u001b[0m     )\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'thu-coai/lccc-base' doesn't exist on the Hub or cannot be accessed."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
