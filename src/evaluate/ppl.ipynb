{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到Python路径\n",
    "project_root = \"/home/cuipeng/Gemma\"\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# 导入必要模块\n",
    "from src.core.model.model_initializer import initialize_model_and_tokenizer\n",
    "from src.core.utils.model_utils import generate_response, apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ppl(model, tokenizer, dataset_path, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    计算模型在给定数据集上的困惑度(PPL)\n",
    "    \"\"\"\n",
    "    # 加载数据集\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_length = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(dataset, desc=\"计算PPL\"):\n",
    "            # 获取输入文本\n",
    "            input_text = item[\"text\"]  # 根据你的数据集格式调整\n",
    "            # 编码输入\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # 获取模型输出\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # 手动计算 cross entropy loss\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = inputs[\"input_ids\"][..., 1:].contiguous()\n",
    "            \n",
    "            # 计算损失\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                          shift_labels.view(-1))\n",
    "            \n",
    "            # 累加 loss 和 token 数量\n",
    "            total_loss += loss.item()\n",
    "            total_length += shift_labels.numel()\n",
    "    \n",
    "    # 计算平均困惑度\n",
    "    avg_loss = total_loss / total_length\n",
    "    ppl = np.exp(avg_loss)\n",
    "    \n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_ppl():\n",
    "    # 设置路径\n",
    "    base_model_path = \"google/gemma-2-9b\"\n",
    "    cache_dir = \"/root/autodl-tmp/gemma\"\n",
    "    lora_path = \"/root/autodl-tmp/models/stage1/checkpoints/gemma-base-zh/checkpoint-43500\"\n",
    "    eval_data_path = \"../data_processing/stage1/data_final/valid.json\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # 加载基础模型\n",
    "    print(\"加载基础模型...\")\n",
    "    base_model, tokenizer = initialize_model_and_tokenizer(\n",
    "        model_path=base_model_path,\n",
    "        cache_dir=cache_dir,\n",
    "        use_quantization=True\n",
    "    )\n",
    "    base_model.eval()\n",
    "    \n",
    "    # 计算基础模型的PPL\n",
    "    print(\"计算基础模型PPL...\")\n",
    "    base_ppl = calculate_ppl(base_model, tokenizer, eval_data_path, device)\n",
    "    print(f\"基础模型PPL: {base_ppl:.4f}\")\n",
    "    \n",
    "    # 加载训练后的模型\n",
    "    print(\"加载微调后的模型...\")\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        lora_path,\n",
    "        is_trainable=False  # 设置为评估模式\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    # 计算微调后模型的PPL\n",
    "    print(\"计算微调后模型PPL...\")\n",
    "    trained_ppl = calculate_ppl(model, tokenizer, eval_data_path, device)\n",
    "    print(f\"微调后模型PPL: {trained_ppl:.4f}\")\n",
    "    \n",
    "    # 计算改进百分比\n",
    "    improvement = ((base_ppl - trained_ppl) / base_ppl) * 100\n",
    "    print(f\"PPL改进比例: {improvement:.2f}%\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载基础模型...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa143f70aece4c4abe0ad2d2bc7fe36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算基础模型PPL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算PPL:   0%|          | 0/1000 [00:00<?, ?it/s]The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n",
      "计算PPL: 100%|██████████| 1000/1000 [02:17<00:00,  7.26it/s]\n",
      "/root/miniconda3/envs/Gemma/lib/python3.12/site-packages/peft/tuners/adalora/config.py:78: UserWarning: Note that `r` is not used in AdaLora and will be ignored.If you intended to set the initial rank, use `init_r` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基础模型PPL: 16.5727\n",
      "加载微调后的模型...\n",
      "计算微调后模型PPL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算PPL: 100%|██████████| 1000/1000 [02:28<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "微调后模型PPL: 3.8577\n",
      "PPL改进比例: 76.72%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    eval_model_ppl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
