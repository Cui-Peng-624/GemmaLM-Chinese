{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoDL官方学术资源加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到Python路径\n",
    "project_root = \"/home/cuipeng/Gemma\"\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# 现在可以正常导入src下的模块\n",
    "from src.core.model.model_initializer import initialize_model_and_tokenizer\n",
    "from src.core.utils.model_utils import generate_response, apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainSpecificBaseDatasetBuilder:\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化专业领域数据集构建器\"\"\"\n",
    "        self.datasets = {}\n",
    "        self.processed_data = []\n",
    "        \n",
    "    def load_task_datasets(self):\n",
    "        \"\"\"加载专业领域相关的数据集\"\"\"\n",
    "        print(\"正在加载专业领域数据集...\")\n",
    "        \n",
    "        try:\n",
    "            # 中国现代诗词数据集 - 246568行\n",
    "            self.datasets['modern_poetry'] = load_dataset('Iess/chinese_modern_poetry', split='train')\n",
    "            print(\"✓ 中国现代诗词数据集加载完成\") # https://huggingface.co/datasets/Iess/chinese_modern_poetry\n",
    "\n",
    "            # 中国古代诗词数据集 - 193808行\n",
    "            self.datasets['ancient_poetry'] = load_dataset('ddnoodle/chinese_poetry', split='train')\n",
    "            print(\"✓ 中国古代诗词数据集加载完成\") # https://huggingface.co/datasets/ddnoodle/chinese_poetry\n",
    "            \n",
    "            # 历史文献数据集 - 文言文，中国历史数据文言文比较多，增强文言文的理解能力 - 765,294行\n",
    "            self.datasets['historical_documents'] = load_dataset('HistoryTrans/Dataset', split='train')\n",
    "            print(\"✓ 历史文献数据集加载完成\") # https://huggingface.co/datasets/HistoryTrans/Dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"加载数据集时出错: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def preprocess_data(self, max_samples_per_task=15000):\n",
    "        \"\"\"预处理各个任务数据集\"\"\"\n",
    "        for dataset_name, dataset in self.datasets.items():\n",
    "            print(f\"处理 {dataset_name} 数据集...\")\n",
    "            \n",
    "            if dataset_name == 'modern_poetry':\n",
    "                processed = self._process_modern_poetry(dataset, max_samples_per_task)\n",
    "            elif dataset_name == 'ancient_poetry':\n",
    "                processed = self._process_ancient_poetry(dataset, max_samples_per_task)\n",
    "            elif dataset_name == 'historical_documents':\n",
    "                processed = self._process_historical(dataset, max_samples_per_task)\n",
    "                \n",
    "            self.processed_data.extend(processed)\n",
    "            print(f\"✓ {dataset_name} 处理完成，添加 {len(processed)} 条数据\")\n",
    "    \n",
    "    def _process_modern_poetry(self, dataset, max_samples):\n",
    "        \"\"\"处理现代诗词数据集\"\"\"\n",
    "        dataset = dataset[:3*max_samples]\n",
    "        processed = []\n",
    "        # print(type(dataset)) # <class 'dict'>\n",
    "        # print(dataset.keys(), type(dataset[\"prompt\"]), type(dataset[\"response\"][0])) # dict_keys(['uuid', 'prompt', 'response']) <class 'list'> <class 'str'>\n",
    "\n",
    "        # 处理所有数据\n",
    "        for i in range(len(dataset['uuid'])):\n",
    "            # print(type(item), item) # <class 'str'> uuid - 哈？着什么玩意\n",
    "            # 创作任务\n",
    "            processed.append({\n",
    "                'messages': [\n",
    "                    {\"role\": \"user\", \"content\": dataset['prompt'][i]},\n",
    "                    {\"role\": \"model\", \"content\": dataset['response'][i]}\n",
    "                ],\n",
    "                'type': 'modern_poetry_creation'\n",
    "            })\n",
    "        \n",
    "        print(f\"现代诗词数据总量: {len(processed)} 条\")\n",
    "        \n",
    "        # 如果数据量超过max_samples，随机采样\n",
    "        if len(processed) > max_samples:\n",
    "            processed = np.random.choice(processed, max_samples, replace=False).tolist()\n",
    "            print(f\"采样后数据量: {len(processed)} 条\")\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def _process_ancient_poetry(self, dataset, max_samples):\n",
    "        \"\"\"处理古代诗词数据集\"\"\"\n",
    "        dataset = dataset[:3*max_samples]\n",
    "        processed = []\n",
    "        \n",
    "        # 处理所有数据\n",
    "        for i in range(len(dataset['instruction'])):\n",
    "            # 创作任务\n",
    "            processed.append({\n",
    "                'messages': [\n",
    "                    {\"role\": \"user\", \"content\": f\"请根据以下题目：{dataset['context'][i]}，{dataset['instruction'][i]}。\"},\n",
    "                    {\"role\": \"model\", \"content\": dataset['response'][i]}\n",
    "                ],\n",
    "                'type': 'ancient_poetry_creation'\n",
    "            })\n",
    "        \n",
    "        print(f\"古代诗词数据总量: {len(processed)} 条\")\n",
    "        \n",
    "        # 如果数据量超过max_samples，随机采样\n",
    "        if len(processed) > max_samples:\n",
    "            processed = np.random.choice(processed, max_samples, replace=False).tolist()\n",
    "            print(f\"采样后数据量: {len(processed)} 条\")\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def _process_historical(self, dataset, max_samples):\n",
    "        \"\"\"处理历史文献数据集\"\"\"\n",
    "        dataset = dataset[:3*max_samples]\n",
    "        processed = []\n",
    "        \n",
    "        # 处理所有数据\n",
    "        for i in range(len(dataset['inputs'])):\n",
    "            # 文言文翻译\n",
    "            processed.append({\n",
    "                'messages': [\n",
    "                    {\"role\": \"user\", \"content\": f\"请将以下文言文翻译成现代文：\\n\\n{dataset['inputs'][i]}\"},\n",
    "                    {\"role\": \"model\", \"content\": dataset['truth'][i]}\n",
    "                ],\n",
    "                'type': 'classical_translation'\n",
    "            })\n",
    "        \n",
    "        print(f\"历史文献数据总量: {len(processed)} 条\")\n",
    "        \n",
    "        # 如果数据量超过max_samples，随机采样\n",
    "        if len(processed) > max_samples:\n",
    "            processed = np.random.choice(processed, max_samples, replace=False).tolist()\n",
    "            print(f\"采样后数据量: {len(processed)} 条\")\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def format_for_training(self):\n",
    "        \"\"\"将数据格式化为训练所需的格式\"\"\"\n",
    "        formatted_data = []\n",
    "        \n",
    "        for item in self.processed_data:\n",
    "            dialogue_str = apply_chat_template(item['messages'])\n",
    "            formatted_data.append({\n",
    "                'text': dialogue_str,\n",
    "                'type': item['type']  # 保持原始任务类型\n",
    "            })\n",
    "            \n",
    "        return formatted_data\n",
    "    \n",
    "    def save_dataset(self, formatted_data, output_path = \"stage3/data_raw\", train_ratio=0.95, train_size=30000, valid_size=1000):\n",
    "        \"\"\"保存数据集为训练集和验证集\n",
    "        \n",
    "        Args:\n",
    "            formatted_data: 格式化后的数据\n",
    "            train_ratio: 训练集比例\n",
    "        \"\"\"\n",
    "        # 创建输出目录\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        # 随机打乱数据\n",
    "        np.random.shuffle(formatted_data)\n",
    "        \n",
    "        # 分割训练集和验证集\n",
    "        split_idx = int(len(formatted_data) * train_ratio)\n",
    "        train_data = formatted_data[:split_idx]\n",
    "        valid_data = formatted_data[split_idx:]\n",
    "\n",
    "        # 从训练集中随机选择指定数量的数据\n",
    "        if len(train_data) > train_size:\n",
    "            train_indices = np.random.choice(len(train_data), train_size, replace=False)\n",
    "            train_data = [train_data[i] for i in train_indices]\n",
    "\n",
    "        # 从验证集中随机选择指定数量的数据\n",
    "        if len(valid_data) > valid_size:\n",
    "            valid_indices = np.random.choice(len(valid_data), valid_size, replace=False)\n",
    "            valid_data = [valid_data[i] for i in valid_indices]\n",
    "        \n",
    "        # 保存数据集\n",
    "        train_path = os.path.join(output_path, 'train.json')\n",
    "        valid_path = os.path.join(output_path, 'valid.json')\n",
    "        \n",
    "        with open(train_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        with open(valid_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(valid_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"\\n数据集已保存：\")\n",
    "        print(f\"- 训练集: {train_path} ({len(train_data)} 条数据)\")\n",
    "        print(f\"- 验证集: {valid_path} ({len(valid_data)} 条数据)\")\n",
    "        \n",
    "        # 输出数据集统计信息\n",
    "        self._print_dataset_stats(formatted_data)\n",
    "    \n",
    "    def _print_dataset_stats(self, data):\n",
    "        \"\"\"打印数据集统计信息\"\"\"\n",
    "        print(\"\\n数据集统计信息:\")\n",
    "        \n",
    "        # 统计不同类型的数据数量\n",
    "        type_counts = {}\n",
    "        for item in data:\n",
    "            type_counts[item['type']] = type_counts.get(item['type'], 0) + 1\n",
    "            \n",
    "        print(\"\\n数据类型分布:\")\n",
    "        for data_type, count in type_counts.items():\n",
    "            percentage = (count / len(data)) * 100\n",
    "            print(f\"- {data_type}: {count} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. 初始化数据集构建器\n",
    "    builder = DomainSpecificBaseDatasetBuilder()\n",
    "    \n",
    "    try:\n",
    "        # 2. 加载数据集\n",
    "        builder.load_task_datasets()\n",
    "        \n",
    "        # 3. 处理数据集\n",
    "        builder.preprocess_data(max_samples_per_task=20000)\n",
    "        \n",
    "        # 4. 格式化数据\n",
    "        formatted_data = builder.format_for_training()\n",
    "        \n",
    "        # 5. 保存数据集\n",
    "        builder.save_dataset(formatted_data)\n",
    "        \n",
    "        print(\"\\n专业领域阶段基础数据集构建完成！\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"构建数据集时出错: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载专业领域数据集...\n",
      "✓ 中国现代诗词数据集加载完成\n",
      "✓ 中国古代诗词数据集加载完成\n",
      "✓ 历史文献数据集加载完成\n",
      "处理 modern_poetry 数据集...\n",
      "现代诗词数据总量: 60000 条\n",
      "采样后数据量: 20000 条\n",
      "✓ modern_poetry 处理完成，添加 20000 条数据\n",
      "处理 ancient_poetry 数据集...\n",
      "古代诗词数据总量: 60000 条\n",
      "采样后数据量: 20000 条\n",
      "✓ ancient_poetry 处理完成，添加 20000 条数据\n",
      "处理 historical_documents 数据集...\n",
      "历史文献数据总量: 60000 条\n",
      "采样后数据量: 20000 条\n",
      "✓ historical_documents 处理完成，添加 20000 条数据\n",
      "\n",
      "数据集已保存：\n",
      "- 训练集: stage3/data_raw/train.json (30000 条数据)\n",
      "- 验证集: stage3/data_raw/valid.json (1000 条数据)\n",
      "\n",
      "数据集统计信息:\n",
      "\n",
      "数据类型分布:\n",
      "- modern_poetry_creation: 20000 (33.33%)\n",
      "- classical_translation: 20000 (33.33%)\n",
      "- ancient_poetry_creation: 20000 (33.33%)\n",
      "\n",
      "专业领域阶段基础数据集构建完成！\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
