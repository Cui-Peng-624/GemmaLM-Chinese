{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoDL官方学术资源加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到Python路径\n",
    "project_root = \"/home/cuipeng/Gemma\"\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# 现在可以正常导入src下的模块\n",
    "from src.core.model.model_initializer import initialize_model_and_tokenizer\n",
    "from src.core.utils.model_utils import generate_response, apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "from typing import List, Dict # type: ignore\n",
    "import json # type: ignore\n",
    "import os # type: ignore\n",
    "from tqdm import tqdm # type: ignore\n",
    "import random\n",
    "from openai import OpenAI # type: ignore\n",
    "\n",
    "# config.py\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "\n",
    "# 加载 .env 文件\n",
    "load_dotenv()\n",
    "\n",
    "# 读取\n",
    "ZetaTechs_api_key = os.getenv('ZETATECHS_API_KEY')\n",
    "ZetaTechs_api_base = os.getenv('ZETATECHS_API_BASE')\n",
    "\n",
    "# 初始化OpenAI客户端\n",
    "client = OpenAI(api_key=ZetaTechs_api_key, base_url=ZetaTechs_api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskSpecificBaseDatasetBuilder:\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化任务特定数据集构建器\"\"\"\n",
    "        self.datasets = {}\n",
    "        self.processed_data = []\n",
    "\n",
    "    def load_task_datasets(self):\n",
    "        \"\"\"加载特定任务所需的数据集\"\"\"\n",
    "        print(\"正在加载任务特定数据集...\")\n",
    "        \n",
    "        try:\n",
    "            # 翻译任务数据集\n",
    "            self.datasets['translation'] = load_dataset('Helsinki-NLP/news_commentary', \"en-zh\", split='train')\n",
    "            print(\"✓ 翻译数据集加载完成\") # https://huggingface.co/datasets/Helsinki-NLP/news_commentary\n",
    "            \n",
    "            # 对话任务数据集\n",
    "            self.datasets['dialogue'] = load_dataset('Hello-SimpleAI/HC3-Chinese', \"all\", split='train', trust_remote_code=True)\n",
    "            print(\"✓ 对话数据集加载完成\") # https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese\n",
    "            \n",
    "            # 故事创作数据集\n",
    "            self.datasets['story'] = load_dataset('YeungNLP/firefly-train-1.1M', split='train')\n",
    "            print(\"✓ 故事创作数据集加载完成\") # https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"加载数据集时出错: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def preprocess_data(self, max_samples_per_task=20000):\n",
    "        \"\"\"预处理各个任务数据集\"\"\"\n",
    "        for dataset_name, dataset in self.datasets.items():\n",
    "            print(f\"处理 {dataset_name} 数据集...\")\n",
    "            \n",
    "            if dataset_name == 'translation':\n",
    "                processed = self._process_translation(dataset, max_samples_per_task)\n",
    "            elif dataset_name == 'dialogue':\n",
    "                processed = self._process_dialogue(dataset, max_samples_per_task)\n",
    "            elif dataset_name == 'story':\n",
    "                processed = self._process_story(dataset, max_samples_per_task)\n",
    "                \n",
    "            self.processed_data.extend(processed)\n",
    "            print(f\"✓ {dataset_name} 处理完成，添加 {len(processed)} 条数据\")\n",
    "    \n",
    "    def _process_translation(self, dataset, max_samples):\n",
    "        \"\"\"处理翻译数据集\"\"\"\n",
    "        dataset = dataset[:5*max_samples] # 中译英，英译中，对应的，所以适当增加处理的数据，避免出现中译英，英译中在一块的情况\n",
    "        processed = []\n",
    "        \n",
    "        # 处理所有数据\n",
    "        for i in range(len(dataset['translation'])):\n",
    "            # 中译英\n",
    "            processed.append({\n",
    "                'messages': [\n",
    "                    {\"role\": \"user\", \"content\": f\"请将以下中文翻译成英文：\\n\\n{dataset['translation'][i]['zh']}\"},\n",
    "                    {\"role\": \"model\", \"content\": dataset['translation'][i]['en']}\n",
    "                ],\n",
    "                'type': 'translation'\n",
    "            })\n",
    "            \n",
    "            # 英译中\n",
    "            processed.append({\n",
    "                'messages': [\n",
    "                    {\"role\": \"user\", \"content\": f\"请将以下英文翻译成中文：\\n\\n{dataset['translation'][i]['en']}\"},\n",
    "                    {\"role\": \"model\", \"content\": dataset['translation'][i]['zh']}\n",
    "                ],\n",
    "                'type': 'translation'\n",
    "            })\n",
    "        \n",
    "        print(f\"翻译数据总量: {len(processed)} 条\")\n",
    "        \n",
    "        # 如果数据量超过max_samples，随机采样\n",
    "        if len(processed) > max_samples:\n",
    "            processed = np.random.choice(processed, max_samples, replace=False).tolist()\n",
    "            print(f\"采样后数据量: {len(processed)} 条\")\n",
    "        \n",
    "        return processed\n",
    "\n",
    "    def _process_dialogue(self, dataset, max_samples):\n",
    "        \"\"\"处理对话数据集 - 这个数据集总共有12.9k条数据\"\"\"\n",
    "        dataset = dataset[:5*max_samples]\n",
    "        processed = []\n",
    "        \n",
    "        # 处理所有数据\n",
    "        for i in range(len(dataset['question'])):\n",
    "            if dataset['chatgpt_answers'][i]:  # 确保有chatgpt的回答\n",
    "                processed.append({\n",
    "                    'messages': [\n",
    "                        {\"role\": \"user\", \"content\": dataset['question'][i]},\n",
    "                        {\"role\": \"model\", \"content\": dataset['chatgpt_answers'][i][0]}\n",
    "                    ],\n",
    "                    'type': 'dialogue'\n",
    "                })\n",
    "        \n",
    "        print(f\"对话数据总量: {len(processed)} 条\")\n",
    "        \n",
    "        # 如果数据量超过max_samples，随机采样\n",
    "        if len(processed) > max_samples:\n",
    "            processed = np.random.choice(processed, max_samples, replace=False).tolist()\n",
    "            print(f\"采样后数据量: {len(processed)} 条\")\n",
    "        \n",
    "        return processed\n",
    "\n",
    "    def _process_story(self, dataset, max_samples):\n",
    "        \"\"\"处理故事创作数据集\"\"\"\n",
    "        all_data = dataset.to_pandas()\n",
    "        story_data = all_data[all_data['kind'] == 'StoryGeneration']\n",
    "        # 这个数据集总共有19048条故事生成的数据\n",
    "        story_data = story_data[:5*max_samples]\n",
    "        \n",
    "        processed = []\n",
    "        # 处理所有数据\n",
    "        for _, row in story_data.iterrows():\n",
    "            processed.append({\n",
    "                'messages': [\n",
    "                    {\"role\": \"user\", \"content\": f\"请根据以下提示生成一个故事：\\n\\n{row['input']}\"},\n",
    "                    {\"role\": \"model\", \"content\": row['target']}\n",
    "                ],\n",
    "                'type': 'story_generation'\n",
    "            })\n",
    "        \n",
    "        print(f\"故事生成数据总量: {len(processed)} 条\")\n",
    "        \n",
    "        # 如果数据量超过max_samples，随机采样\n",
    "        if len(processed) > max_samples:\n",
    "            processed = np.random.choice(processed, max_samples, replace=False).tolist()\n",
    "            print(f\"采样后数据量: {len(processed)} 条\")\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def format_for_training(self):\n",
    "        \"\"\"将数据格式化为训练所需的格式\"\"\"\n",
    "        formatted_data = []\n",
    "        \n",
    "        for item in self.processed_data:\n",
    "            dialogue_str = apply_chat_template(item['messages'])\n",
    "            formatted_data.append({\n",
    "                'text': dialogue_str,\n",
    "                'type': item['type']  # 保持原始任务类型\n",
    "            })\n",
    "            \n",
    "        return formatted_data\n",
    "    \n",
    "    def save_dataset(self, formatted_data, output_path='./stage2/data_raw', train_ratio=0.8, valid_size=1000):\n",
    "        \"\"\"保存处理后的数据集\"\"\"\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        # 随机打乱数据\n",
    "        np.random.shuffle(formatted_data)\n",
    "        \n",
    "        # 划分训练集和验证集\n",
    "        split_idx = int(len(formatted_data) * train_ratio)\n",
    "        train_data = formatted_data[:split_idx]\n",
    "        valid_data = formatted_data[split_idx:]\n",
    "        \n",
    "        # 从验证集中随机选择指定数量的数据\n",
    "        if len(valid_data) > valid_size:\n",
    "            valid_indices = np.random.choice(len(valid_data), valid_size, replace=False)\n",
    "            valid_data = [valid_data[i] for i in valid_indices]\n",
    "        \n",
    "        # 保存训练集和验证集\n",
    "        train_path = os.path.join(output_path, 'train.json')\n",
    "        valid_path = os.path.join(output_path, 'valid.json')\n",
    "        \n",
    "        with open(train_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        with open(valid_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(valid_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"\\n数据集已保存：\")\n",
    "        print(f\"- 训练集: {train_path} ({len(train_data)} 条数据)\")\n",
    "        print(f\"- 验证集: {valid_path} ({len(valid_data)} 条数据)\")\n",
    "        \n",
    "        # 输出数据集统计信息\n",
    "        self._print_dataset_stats(formatted_data)\n",
    "    \n",
    "    def _print_dataset_stats(self, data):\n",
    "        \"\"\"打印数据集统计信息\"\"\"\n",
    "        print(\"\\n数据集统计信息:\")\n",
    "        \n",
    "        # 统计不同类型的数据数量\n",
    "        type_counts = {}\n",
    "        for item in data:\n",
    "            type_counts[item['type']] = type_counts.get(item['type'], 0) + 1\n",
    "            \n",
    "        print(\"\\n数据类型分布:\")\n",
    "        for data_type, count in type_counts.items():\n",
    "            percentage = (count / len(data)) * 100\n",
    "            print(f\"- {data_type}: {count} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. 初始化数据集构建器\n",
    "    builder = TaskSpecificBaseDatasetBuilder()\n",
    "    \n",
    "    try:\n",
    "        # 2. 加载数据集\n",
    "        builder.load_task_datasets()\n",
    "        \n",
    "        # 3. 处理数据集\n",
    "        builder.preprocess_data(max_samples_per_task=20000)\n",
    "        \n",
    "        # 4. 格式化数据\n",
    "        formatted_data = builder.format_for_training()\n",
    "        \n",
    "        # 5. 保存数据集\n",
    "        builder.save_dataset(formatted_data)\n",
    "        \n",
    "        print(\"\\n特定任务阶段基础数据集构建完成！\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"构建数据集时出错: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载任务特定数据集...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 翻译数据集加载完成\n",
      "✓ 对话数据集加载完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 故事创作数据集加载完成\n",
      "处理 translation 数据集...\n",
      "翻译数据总量: 138412 条\n",
      "采样后数据量: 20000 条\n",
      "✓ translation 处理完成，添加 20000 条数据\n",
      "处理 dialogue 数据集...\n",
      "对话数据总量: 12853 条\n",
      "✓ dialogue 处理完成，添加 12853 条数据\n",
      "处理 story 数据集...\n",
      "故事生成数据总量: 19048 条\n",
      "✓ story 处理完成，添加 19048 条数据\n",
      "\n",
      "数据集已保存：\n",
      "- 训练集: ./stage2/data_raw/train.json (41520 条数据)\n",
      "- 验证集: ./stage2/data_raw/valid.json (1000 条数据)\n",
      "\n",
      "数据集统计信息:\n",
      "\n",
      "数据类型分布:\n",
      "- story_generation: 19048 (36.70%)\n",
      "- translation: 20000 (38.53%)\n",
      "- dialogue: 12853 (24.76%)\n",
      "\n",
      "特定任务阶段基础数据集构建完成！\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
