{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoDL官方学术资源加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到Python路径\n",
    "project_root = \"/home/cuipeng/Gemma\"\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# 现在可以正常导入src下的模块\n",
    "from src.core.model.model_initializer import initialize_model_and_tokenizer\n",
    "from src.core.utils.model_utils import generate_response, apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "from typing import List, Dict # type: ignore\n",
    "import json # type: ignore\n",
    "import os # type: ignore\n",
    "from tqdm import tqdm # type: ignore\n",
    "import random\n",
    "from openai import OpenAI # type: ignore\n",
    "\n",
    "# config.py\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "\n",
    "# 加载 .env 文件\n",
    "load_dotenv()\n",
    "\n",
    "# 读取\n",
    "ZetaTechs_api_key = os.getenv('ZETATECHS_API_KEY')\n",
    "ZetaTechs_api_base = os.getenv('ZETATECHS_API_BASE')\n",
    "\n",
    "# 初始化OpenAI客户端\n",
    "client = OpenAI(api_key=ZetaTechs_api_key, base_url=ZetaTechs_api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDatasetBuilder:\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化基础数据集构建器\"\"\"\n",
    "        self.datasets = {}\n",
    "        self.processed_data = []\n",
    "\n",
    "    def load_base_datasets(self):\n",
    "        \"\"\"加载基础阶段所需的数据集\"\"\"\n",
    "        print(\"正在加载基础数据集...\")\n",
    "        \n",
    "        try:\n",
    "            # 通用指令数据集\n",
    "            self.datasets['belle'] = load_dataset(\"BelleGroup/train_1M_CN\", trust_remote_code=True)\n",
    "            print(\"✓ Belle数据集加载完成\")\n",
    "            \n",
    "            # Alpaca中文数据集\n",
    "            self.datasets['alpaca'] = load_dataset('silk-road/alpaca-data-gpt4-chinese')\n",
    "            print(\"✓ Alpaca中文数据集加载完成\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"加载数据集时出错: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def preprocess_data(self, max_samples_per_dataset=50000):\n",
    "        \"\"\"预处理各个数据集\"\"\"\n",
    "        for dataset_name, dataset in self.datasets.items():\n",
    "            print(f\"处理 {dataset_name} 数据集...\")\n",
    "            \n",
    "            if dataset_name == 'belle':\n",
    "                processed = self._process_belle(dataset, max_samples_per_dataset)\n",
    "            elif dataset_name == 'alpaca':\n",
    "                processed = self._process_alpaca(dataset, max_samples_per_dataset)\n",
    "                \n",
    "            self.processed_data.extend(processed)\n",
    "            print(f\"✓ {dataset_name} 处理完成，添加 {len(processed)} 条数据\")\n",
    "    \n",
    "    def _process_belle(self, dataset, max_samples):\n",
    "        \"\"\"处理Belle数据集\"\"\"\n",
    "        samples = dataset[\"train\"][:max_samples]\n",
    "        processed_data = []\n",
    "\n",
    "        for i in range(len(samples['instruction'])):\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": samples['instruction'][i]},\n",
    "                {\"role\": \"model\", \"content\": samples['output'][i]}\n",
    "            ]\n",
    "            \n",
    "            processed_data.append({'messages': messages})\n",
    "            \n",
    "            # 添加进度显示\n",
    "            # if (i + 1) % 1000 == 0:\n",
    "            #     print(f\"Belle数据集处理进度: {i + 1}/{len(samples)}\")\n",
    "        \n",
    "        return processed_data\n",
    "    \n",
    "    def _process_alpaca(self, dataset, max_samples):\n",
    "        \"\"\"处理Alpaca数据集\"\"\"\n",
    "        samples = dataset[\"train\"][:max_samples]\n",
    "        processed_data = []\n",
    "\n",
    "        for i in range(len(samples['instruction_zh'])):\n",
    "            # 合并instruction和input（如果有）\n",
    "            user_content = (f\"{samples['instruction_zh'][i]}\\n\\n\"\n",
    "                          f\"输入：{samples['input_zh'][i]}\" if samples['input_zh'][i] \n",
    "                          else samples['instruction_zh'][i])\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "                {\"role\": \"model\", \"content\": samples['output_zh'][i]}\n",
    "            ]\n",
    "            \n",
    "            processed_data.append({'messages': messages})\n",
    "            \n",
    "            # 添加进度显示\n",
    "            # if (i + 1) % 1000 == 0:\n",
    "            #     print(f\"Alpaca数据集处理进度: {i + 1}/{len(samples)}\")\n",
    "        \n",
    "        return processed_data\n",
    "    \n",
    "    def format_for_training(self):\n",
    "        \"\"\"将数据格式化为训练所需的格式\"\"\"\n",
    "        formatted_data = []\n",
    "        \n",
    "        for item in self.processed_data:\n",
    "            dialogue_str = apply_chat_template(item['messages'])\n",
    "            formatted_data.append({\n",
    "                'text': dialogue_str,\n",
    "                'type': 'instruction'\n",
    "            })\n",
    "            \n",
    "        return formatted_data\n",
    "    \n",
    "    def save_dataset(self, formatted_data, output_path='./stage1/data_raw', train_ratio=0.8, valid_size=1000):\n",
    "        \"\"\"保存处理后的数据集\n",
    "        \n",
    "        Args:\n",
    "            formatted_data: 格式化后的数据\n",
    "            output_path: 输出路径\n",
    "            train_ratio: 训练集占比\n",
    "            valid_size: 验证集最终保留的数据量\n",
    "        \"\"\"\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        # 随机打乱数据\n",
    "        np.random.shuffle(formatted_data)\n",
    "        \n",
    "        # 按原比例划分训练集和验证集\n",
    "        split_idx = int(len(formatted_data) * train_ratio)\n",
    "        train_data = formatted_data[:split_idx]\n",
    "        valid_data = formatted_data[split_idx:]\n",
    "        \n",
    "        # 从验证集中随机选择1000条数据\n",
    "        if len(valid_data) > valid_size:\n",
    "            # 随机选择指定数量的验证数据\n",
    "            valid_indices = np.random.choice(len(valid_data), valid_size, replace=False)\n",
    "            valid_data = [valid_data[i] for i in valid_indices]\n",
    "        \n",
    "        # 保存训练集\n",
    "        train_path = os.path.join(output_path, 'train.json')\n",
    "        with open(train_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        # 保存验证集\n",
    "        valid_path = os.path.join(output_path, 'valid.json')\n",
    "        with open(valid_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(valid_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(f\"\\n数据集已保存：\")\n",
    "        print(f\"- 训练集: {train_path} ({len(train_data)} 条数据)\")\n",
    "        print(f\"- 验证集: {valid_path} ({len(valid_data)} 条数据)\")\n",
    "        print(f\"- 原始验证集大小: {len(formatted_data) - split_idx}\")\n",
    "        print(f\"- 最终验证集大小: {len(valid_data)}\")\n",
    "        \n",
    "        # 输出数据集统计信息\n",
    "        self._print_dataset_stats(formatted_data)\n",
    "    \n",
    "    def _print_dataset_stats(self, data):\n",
    "        \"\"\"打印数据集统计信息\"\"\"\n",
    "        print(\"\\n数据集统计信息:\")\n",
    "        print(f\"总数据量: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. 初始化数据集构建器\n",
    "    builder = BaseDatasetBuilder()\n",
    "    \n",
    "    try:\n",
    "        # 2. 加载数据集\n",
    "        builder.load_base_datasets()\n",
    "        \n",
    "        # 3. 处理数据集\n",
    "        builder.preprocess_data(max_samples_per_dataset=50000)\n",
    "        \n",
    "        # 4. 格式化数据\n",
    "        formatted_data = builder.format_for_training()     \n",
    "        \n",
    "        # 5. 保存数据集\n",
    "        builder.save_dataset(formatted_data)\n",
    "        \n",
    "        print(\"\\n基础阶段数据集构建完成！\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"构建数据集时出错: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载基础数据集...\n",
      "✓ Belle数据集加载完成\n",
      "✓ Alpaca中文数据集加载完成\n",
      "处理 belle 数据集...\n",
      "✓ belle 处理完成，添加 50000 条数据\n",
      "处理 alpaca 数据集...\n",
      "✓ alpaca 处理完成，添加 50000 条数据\n",
      "\n",
      "数据集已保存：\n",
      "- 训练集: ./stage1/data_raw/train.json (80000 条数据)\n",
      "- 验证集: ./stage1/data_raw/valid.json (1000 条数据)\n",
      "- 原始验证集大小: 20000\n",
      "- 最终验证集大小: 1000\n",
      "\n",
      "数据集统计信息:\n",
      "总数据量: 100000\n",
      "\n",
      "基础阶段数据集构建完成！\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
