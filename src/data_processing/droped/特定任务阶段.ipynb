{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoDL官方学术资源加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskSpecificDatasetBuilder:\n",
    "    def __init__(self, base_data_path='./base_stage_data'):\n",
    "        self.datasets = {}\n",
    "        self.task_data = []  # 存储处理后的任务数据\n",
    "        self.base_data = []  # 存储基础数据\n",
    "        self.processed_data = []  # 存储最终混合后的数据\n",
    "        self.base_data_path = base_data_path\n",
    "        \n",
    "    def load_task_datasets(self):\n",
    "        \"\"\"加载特定任务所需的数据集\"\"\"\n",
    "        print(\"正在加载任务特定数据集...\")\n",
    "        \n",
    "        try:\n",
    "            # 翻译任务数据集\n",
    "            self.datasets['translation'] = load_dataset('Helsinki-NLP/news_commentary', \"en-zh\", split='train') # 69.2k rows\n",
    "            print(\"✓ 翻译数据集加载完成\") # https://huggingface.co/datasets/Helsinki-NLP/news_commentary\n",
    "            \n",
    "            # 对话任务数据集 - QA。这个数据集还包gpt的回答\n",
    "            self.datasets['dialogue'] = load_dataset('Hello-SimpleAI/HC3-Chinese', \"all\", split='train', trust_remote_code=True) # 12.9k rows\n",
    "            print(\"✓ 对话数据集加载完成\") # https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese\n",
    "            \n",
    "            # 故事创作数据集 - 这个数据集包含很多kind的数据，所以我们首先要提取kind = StoryGeneration的数据\n",
    "            self.datasets['story'] = load_dataset('YeungNLP/firefly-train-1.1M', split='train') # 1.65M rows\n",
    "            print(\"✓ 故事创作数据集加载完成\") # https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"加载数据集时出错: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def load_base_data(self):\n",
    "        \"\"\"加载基础阶段的数据\"\"\"\n",
    "        print(\"加载基础阶段数据...\")\n",
    "        \n",
    "        train_path = os.path.join(self.base_data_path, 'train.json')\n",
    "        with open(train_path, 'r', encoding='utf-8') as f:\n",
    "            self.base_data = json.load(f)\n",
    "        print(f\"✓ 成功加载基础数据: {len(self.base_data)} 条\")\n",
    "    \n",
    "    def preprocess_task_data(self, max_samples_per_task=20000):\n",
    "        \"\"\"预处理特定任务的数据集\"\"\"\n",
    "        print(\"处理特定任务数据集...\")\n",
    "        \n",
    "        for dataset_name, dataset in self.datasets.items():\n",
    "            print(f\"处理 {dataset_name} 数据集...\")\n",
    "            \n",
    "            if dataset_name == 'translation':\n",
    "                processed = self._process_translation(dataset, max_samples_per_task)\n",
    "            elif dataset_name == 'dialogue':\n",
    "                processed = self._process_dialogue(dataset, max_samples_per_task)\n",
    "            elif dataset_name == 'story':\n",
    "                processed = self._process_story(dataset, max_samples_per_task)\n",
    "                \n",
    "            self.task_data.extend(processed)\n",
    "            print(f\"✓ {dataset_name} 处理完成，添加 {len(processed)} 条数据\")\n",
    "            \n",
    "        return self.task_data # task_data 的结构类似：[{'type': 'translation', 'instruction': '请将以下中文翻译成英文：', 'input': '今天天气很好', 'output': 'The weather is nice today'}, {'type': 'translation', 'instruction': '请将以下英文翻译成中文：', 'input': 'The weather is nice today', 'output': '今天天气很好'}, ...]\n",
    "    \n",
    "    def mix_datasets(self, task_stage_data, total_samples=100000, base_ratio=0.7):\n",
    "        \"\"\"\n",
    "        混合基础数据集和特定任务数据集\n",
    "        \n",
    "        Args:\n",
    "            total_samples: 最终需要的总数据量\n",
    "            base_ratio: 基础数据集占比\n",
    "        \"\"\"\n",
    "        print(f\"\\n混合数据集 (目标总量: {total_samples}, 基础数据比例: {base_ratio})\")\n",
    "        \n",
    "        # 计算基础数据和任务数据的目标数量\n",
    "        base_samples = int(total_samples * base_ratio)\n",
    "        task_samples = total_samples - base_samples\n",
    "        \n",
    "        # 随机采样\n",
    "        selected_base = random.sample(self.base_data, min(base_samples, len(self.base_data)))\n",
    "        selected_task = random.sample(task_stage_data, min(task_samples, len(task_stage_data)))\n",
    "        \n",
    "        # 合并数据\n",
    "        self.processed_data = selected_base + selected_task\n",
    "        \n",
    "        print(f\"- 选择的基础数据量: {len(selected_base)}\")\n",
    "        print(f\"- 选择的任务数据量: {len(selected_task)}\")\n",
    "        print(f\"- 最终数据总量: {len(self.processed_data)}\")\n",
    "        \n",
    "        return self.processed_data\n",
    "    \n",
    "    def _process_translation(self, dataset, max_samples):\n",
    "        \"\"\"处理翻译数据集\"\"\"\n",
    "        samples = dataset[:max_samples]\n",
    "        processed = []\n",
    "        \n",
    "        # print(type(samples), samples.keys()) # <class 'dict'> dict_keys(['id', 'translation'])\n",
    "        # print(type(samples['translation'])) # <class 'list'>\n",
    "        # print(type(samples['translation'][0])) # <class 'dict'>\n",
    "\n",
    "        for i in range(len(samples['translation'])):\n",
    "            processed.append({\n",
    "                'type': 'translation',\n",
    "                'instruction': '请将以下中文翻译成英文：',\n",
    "                'input': samples['translation'][i]['zh'],\n",
    "                'output': samples['translation'][i]['en']\n",
    "            })\n",
    "            \n",
    "            # 添加英译中的样本\n",
    "            processed.append({\n",
    "                'type': 'translation',\n",
    "                'instruction': '请将以下英文翻译成中文：',\n",
    "                'input': samples['translation'][i]['en'],\n",
    "                'output': samples['translation'][i]['zh']\n",
    "            })\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def _process_dialogue(self, dataset, max_samples):\n",
    "        \"\"\"处理对话数据集\"\"\"\n",
    "        samples = dataset[:max_samples]\n",
    "        return [{\n",
    "            'type': 'dialogue',\n",
    "            'instruction': '请针对以下问题进行回答：',\n",
    "            'input': samples['question'][i],\n",
    "            'output': samples['human_answers'][i][0] if samples['human_answers'][i] else '' # samples['human_answers'][i] 是一个 list\n",
    "        } for i in range(len(samples['question']))]\n",
    "    \n",
    "    # 由于我们选择的是\"YeungNLP/firefly-train-1.1M\"这个数据集，所以首先需要提取kind = StoryGeneration的数据\n",
    "    def _process_story(self, dataset, max_samples):\n",
    "        \"\"\"处理故事创作数据集\"\"\"\n",
    "        # 首先将数据集转换为列表形式\n",
    "        all_data = dataset.to_pandas()\n",
    "        \n",
    "        # 筛选出kind为StoryGeneration的数据\n",
    "        story_data = all_data[all_data['kind'] == 'StoryGeneration']\n",
    "        print(f\"故事生成数据总量: {len(story_data)} 条\")\n",
    "        \n",
    "        # 如果数据量超过max_samples，则随机采样\n",
    "        if len(story_data) > max_samples:\n",
    "            story_data = story_data.sample(n=max_samples, random_state=42)\n",
    "            print(f\"采样后数据量: {len(story_data)} 条\")\n",
    "        \n",
    "        # print(type(story_data)) # <class 'pandas.core.frame.DataFrame'>\n",
    "\n",
    "        # 转换为所需格式\n",
    "        processed = []\n",
    "        for _, row in story_data.iterrows():\n",
    "            processed.append({\n",
    "                'type': 'story_generation',\n",
    "                'instruction': \"请根据以下提示生成一个故事：\",\n",
    "                'input': row['input'],\n",
    "                'output': row['target']\n",
    "            })\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def format_for_gemma(self):\n",
    "        \"\"\"将数据格式化为Gemma模型训练所需的格式\"\"\"\n",
    "        formatted_data = []\n",
    "        \n",
    "        USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}\\n<end_of_turn><eos>\\n\"\n",
    "        MODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{response}\\n<end_of_turn><eos>\\n\"\n",
    "        \n",
    "        # print(type(self.processed_data), self.processed_data[0].keys()) # 期待：<class 'list'> dict_keys(['text', 'prompt', 'completion', 'type'])\n",
    "        # 实际上\n",
    "        # print(type(self.processed_data)) # <class 'list'>\n",
    "        # print(self.processed_data) # []\n",
    "        # print(type(self.processed_data[0]), self.processed_data[0]) # 报错\n",
    "        \n",
    "        for item in self.task_data:\n",
    "\n",
    "            # if item['type'] == 'translation':\n",
    "            #     print(item) # {'type': 'translation', 'instruction': '请将以下中文翻译成英文：', 'input': '1929年还是1989年?', 'output': '1929 or 1989?'}\n",
    "            #     break\n",
    "\n",
    "            # 构建用户提示\n",
    "            if item['input']:\n",
    "                user_prompt = f\"{item['instruction']}\\n\\n输入：{item['input']}\"\n",
    "            else:\n",
    "                user_prompt = item['instruction']\n",
    "            \n",
    "            # 格式化用户输入和模型输出\n",
    "            formatted_prompt = USER_CHAT_TEMPLATE.format(prompt=user_prompt)\n",
    "            formatted_response = MODEL_CHAT_TEMPLATE.format(response=item['output'])\n",
    "            \n",
    "            formatted_data.append({\n",
    "                'text': formatted_prompt + formatted_response,\n",
    "                'prompt': formatted_prompt,\n",
    "                'completion': formatted_response,\n",
    "                'type': item['type']\n",
    "            })\n",
    "        \n",
    "        return formatted_data\n",
    "    \n",
    "    def save_dataset(self, formatted_data, output_path='./task_stage_data', train_ratio=0.8):\n",
    "        \"\"\"保存处理后的数据集\"\"\"\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        # 随机打乱数据\n",
    "        np.random.shuffle(formatted_data)\n",
    "        \n",
    "        # 划分训练集和验证集\n",
    "        split_idx = int(len(formatted_data) * train_ratio)\n",
    "        train_data = formatted_data[:split_idx]\n",
    "        valid_data = formatted_data[split_idx:]\n",
    "        \n",
    "        # 保存训练集\n",
    "        train_path = os.path.join(output_path, 'train.json')\n",
    "        with open(train_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        # 保存验证集\n",
    "        valid_path = os.path.join(output_path, 'valid.json')\n",
    "        with open(valid_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(valid_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(f\"数据集已保存：\")\n",
    "        print(f\"- 训练集: {train_path} ({len(train_data)} 条数据)\")\n",
    "        print(f\"- 验证集: {valid_path} ({len(valid_data)} 条数据)\")\n",
    "        \n",
    "        # 输出数据集统计信息\n",
    "        self._print_dataset_stats(formatted_data)\n",
    "    \n",
    "    def _print_dataset_stats(self, data):\n",
    "        \"\"\"打印数据集统计信息\"\"\"\n",
    "        print(\"\\n数据集统计信息:\")\n",
    "        \n",
    "        # 统计不同类型的数据数量\n",
    "        type_counts = {}\n",
    "        for item in data:\n",
    "            type_counts[item['type']] = type_counts.get(item['type'], 0) + 1\n",
    "            \n",
    "        print(\"\\n数据类型分布:\")\n",
    "        for data_type, count in type_counts.items():\n",
    "            percentage = (count / len(data)) * 100\n",
    "            print(f\"- {data_type}: {count} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. 初始化数据集构建器\n",
    "    builder = TaskSpecificDatasetBuilder()\n",
    "    \n",
    "    try:\n",
    "        # 2. 加载任务数据集\n",
    "        builder.load_task_datasets()\n",
    "        \n",
    "        # 3. 处理特定任务数据集\n",
    "        builder.preprocess_task_data(max_samples_per_task=20000)\n",
    "        \n",
    "        # 4. 将任务数据转化为大模型格式\n",
    "        task_stage_data = builder.format_for_gemma()\n",
    "        \n",
    "        # 5. 加载基础数据\n",
    "        builder.load_base_data()\n",
    "        \n",
    "        # 6. 混合数据集\n",
    "        mixed_data = builder.mix_datasets(task_stage_data, total_samples=80000, base_ratio=0.7)\n",
    "        \n",
    "        # 7. 保存数据集\n",
    "        builder.save_dataset(mixed_data)\n",
    "        \n",
    "        print(\"\\n任务特定阶段数据集构建完成！\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"构建数据集时出错: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载任务特定数据集...\n",
      "✓ 翻译数据集加载完成\n",
      "✓ 对话数据集加载完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 故事创作数据集加载完成\n",
      "处理特定任务数据集...\n",
      "处理 translation 数据集...\n",
      "✓ translation 处理完成，添加 40000 条数据\n",
      "处理 dialogue 数据集...\n",
      "✓ dialogue 处理完成，添加 12853 条数据\n",
      "处理 story 数据集...\n",
      "故事生成数据总量: 19048 条\n",
      "✓ story 处理完成，添加 19048 条数据\n",
      "加载基础阶段数据...\n",
      "✓ 成功加载基础数据: 80000 条\n",
      "\n",
      "混合数据集 (目标总量: 80000, 基础数据比例: 0.7)\n",
      "- 选择的基础数据量: 56000\n",
      "- 选择的任务数据量: 24000\n",
      "- 最终数据总量: 80000\n",
      "数据集已保存：\n",
      "- 训练集: ./task_stage_data/train.json (64000 条数据)\n",
      "- 验证集: ./task_stage_data/valid.json (16000 条数据)\n",
      "\n",
      "数据集统计信息:\n",
      "\n",
      "数据类型分布:\n",
      "- instruction: 56000 (70.00%)\n",
      "- translation: 13317 (16.65%)\n",
      "- story_generation: 6420 (8.03%)\n",
      "- dialogue: 4263 (5.33%)\n",
      "\n",
      "任务特定阶段数据集构建完成！\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
