{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoDL官方学术资源加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到Python路径\n",
    "project_root = \"/home/cuipeng/Gemma\"\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# 现在可以正常导入src下的模块\n",
    "from src.core.model.model_initializer import initialize_model_and_tokenizer\n",
    "from src.core.utils.model_utils import generate_response, apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "from typing import List, Dict # type: ignore\n",
    "import json # type: ignore\n",
    "import os # type: ignore\n",
    "from tqdm import tqdm # type: ignore\n",
    "import random\n",
    "from openai import OpenAI # type: ignore\n",
    "\n",
    "# config.py\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "\n",
    "# 加载 .env 文件\n",
    "load_dotenv()\n",
    "\n",
    "# 读取\n",
    "ZetaTechs_api_key = os.getenv('ZETATECHS_API_KEY')\n",
    "ZetaTechs_api_base = os.getenv('ZETATECHS_API_BASE')\n",
    "\n",
    "# 初始化OpenAI客户端\n",
    "client = OpenAI(api_key=ZetaTechs_api_key, base_url=ZetaTechs_api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDatasetBuilder:\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化基础数据集构建器\"\"\"\n",
    "        self.datasets = {}\n",
    "        self.processed_data = []\n",
    "        # 添加多样化的系统提示词\n",
    "        self.system_prompts = [\n",
    "            \"你是一个有帮助的AI助手。\",\n",
    "            \"作为一个AI助手，你会提供准确、有帮助的回答。\",\n",
    "            \"你是一个知识渊博的AI助手，善于解释复杂概念。\",\n",
    "            \"你是一个友好的AI助手，会用通俗易懂的方式回答问题。\",\n",
    "            \"作为AI助手，你会以专业、客观的方式提供帮助。\",\n",
    "            \"你是一个可靠的AI助手，始终保持耐心和专注。\",\n",
    "            \"作为AI助手，你擅长提供清晰、结构化的解答。\",\n",
    "            \"你是一个认真负责的AI助手，会仔细理解并回答问题。\",\n",
    "            \"作为AI助手，你会以严谨的态度提供准确的信息。\",\n",
    "            \"你是一个智能AI助手，能够理解上下文并给出恰当的回应。\"\n",
    "            \"作为一个AI助理，我会专业、友善地回答你的问题。\",\n",
    "            \"我是一个知识渊博的AI，很高兴能帮助你解决问题。\",\n",
    "            \"让我们一起探讨这个问题，我会尽我所能提供帮助。\",\n",
    "            \"我是一个智能助手，擅长理解和解答各类问题。\",\n",
    "            \"作为你的AI伙伴，我会用清晰简洁的方式回答问题。\",\n",
    "            \"我是一个全能型AI助手，可以处理各种类型的任务。\",\n",
    "            \"作为AI助手，我会以严谨专业的态度回应你的需求。\",\n",
    "            \"我是一个AI助理，会用通俗易懂的方式解释复杂概念。\",\n",
    "            \"作为你的AI搭档，我会用准确和有见地的方式回答问题。\"\n",
    "        ]\n",
    "\n",
    "    def _generate_dynamic_prompt(self, user_content: str, model_content: str) -> str:\n",
    "        \"\"\"使用GPT生成动态系统提示词\"\"\"\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"你是一个系统提示词生成器。根据给定的用户问题和AI回答，生成一个合适的系统提示词。提示词应该简洁、专业，并且能够指导AI更好地回答类似的问题。直接返回提示词内容，不要包含任何解释或额外的文字。\"},\n",
    "                {\"role\": \"user\", \"content\": f\"用户问题：{user_content}\\n\\n问题回答：{model_content}\\n\\n请根据上述对话生成一个合适的系统提示词。\"}\n",
    "            ]\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"生成动态提示词时出错: {e}\")\n",
    "            # 如果API调用失败，返回一个固定的提示词\n",
    "            return np.random.choice(self.system_prompts)\n",
    "        \n",
    "    def _get_random_system_prompt(self, user_content: str = \"\", model_content: str = \"\") -> str:\n",
    "        \"\"\"根据概率选择系统提示词的生成方式\"\"\"\n",
    "        rand = np.random.random()\n",
    "        \n",
    "        if rand < 0.40:  # 40%概率返回空提示词\n",
    "            return \"\"\n",
    "        elif rand < 0.995:  # 59.5%概率使用固定提示词\n",
    "            return np.random.choice(self.system_prompts)\n",
    "        else:  # 0.5%概率使用GPT生成动态提示词 # 注意，每10%的概率就是1万条数据\n",
    "            return self._generate_dynamic_prompt(user_content, model_content)\n",
    "        \n",
    "    def load_base_datasets(self):\n",
    "        \"\"\"加载基础阶段所需的数据集\"\"\"\n",
    "        print(\"正在加载基础数据集...\")\n",
    "        \n",
    "        try:\n",
    "            # 通用指令数据集 - 啥样的都有\n",
    "            self.datasets['belle'] = load_dataset(\"BelleGroup/train_1M_CN\", trust_remote_code=True) # 917,424 rows\n",
    "            print(\"✓ Belle数据集加载完成\") # https://huggingface.co/datasets/BelleGroup/train_1M_CN\n",
    "            \n",
    "            # 中文维基百科 - 我认为要使用此数据集,还需要进行一些额外的处理,参考:基础阶段_wiki.ipynb, 基础阶段_wiki_异步.ipynb\n",
    "            # 这里我们暂不使用该数据集\n",
    "            # self.datasets['wiki'] = load_dataset('wikimedia/wikipedia', '20231101.zh') # 1,380,000 rows\n",
    "            # print(\"✓ 维基百科数据集加载完成\") # https://huggingface.co/datasets/wikimedia/wikipedia\n",
    "            \n",
    "            # Alpaca中文数据集 - 也都是啥都有,相比Belle, 此数据集的input_zh可能不是空的\n",
    "            self.datasets['alpaca'] = load_dataset('silk-road/alpaca-data-gpt4-chinese') # 52,000 rows\n",
    "            print(\"✓ Alpaca中文数据集加载完成\") # https://huggingface.co/datasets/silk-road/alpaca-data-gpt4-chinese\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"加载数据集时出错: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def preprocess_data(self, max_samples_per_dataset=50000):\n",
    "        \"\"\"预处理各个数据集\"\"\"\n",
    "        for dataset_name, dataset in self.datasets.items():\n",
    "            print(f\"处理 {dataset_name} 数据集...\")\n",
    "            \n",
    "            if dataset_name == 'belle':\n",
    "                processed = self._process_belle(dataset, max_samples_per_dataset)\n",
    "            # elif dataset_name == 'wiki':\n",
    "            #     processed = self._process_wiki(dataset, max_samples_per_dataset)\n",
    "            elif dataset_name == 'alpaca':\n",
    "                processed = self._process_alpaca(dataset, max_samples_per_dataset)\n",
    "                \n",
    "            self.processed_data.extend(processed)\n",
    "            print(f\"✓ {dataset_name} 处理完成，添加 {len(processed)} 条数据\")\n",
    "    \n",
    "    def _process_belle(self, dataset, max_samples):\n",
    "        \"\"\"处理Belle数据集\"\"\"\n",
    "        samples = dataset[\"train\"][:max_samples]\n",
    "        processed_data = []\n",
    "\n",
    "        # print(type(dataset), type(samples)) # <class 'datasets.dataset_dict.DatasetDict'> <class 'dict'>\n",
    "        # print(samples.keys()) # dict_keys(['instruction', 'input', 'output'])\n",
    "        # print(samples[\"instruction\"][0], type(samples[\"instruction\"])) #  <class 'list'>\n",
    "\n",
    "        # # 打印第一条数据的结构\n",
    "        # print(\"Belle数据集结构示例：\")\n",
    "        # print(samples[0])\n",
    "        # print(\"\\n数据类型：\", type(samples[0]))\n",
    "\n",
    "        # # 获取数据集的列名\n",
    "        # print(\"\\n数据集列名：\")\n",
    "        # print(samples.features)\n",
    "\n",
    "        for i in range(len(samples['instruction'])):\n",
    "            messages = []\n",
    "            # 传入用户问题和模型回答来生成系统提示词\n",
    "            system_prompt = self._get_random_system_prompt(\n",
    "                user_content=samples['instruction'][i],\n",
    "                model_content=samples['output'][i]\n",
    "            )\n",
    "            \n",
    "            if system_prompt:\n",
    "                messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "                \n",
    "            messages.extend([\n",
    "                {\"role\": \"user\", \"content\": samples['instruction'][i]},\n",
    "                {\"role\": \"model\", \"content\": samples['output'][i]}\n",
    "            ])\n",
    "            \n",
    "            processed_data.append({'messages': messages})\n",
    "            \n",
    "            # 添加简单的进度显示\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Belle数据集处理进度: {i + 1}/{len(samples)}\")\n",
    "        \n",
    "        return processed_data\n",
    "    \n",
    "    # def _process_wiki(self, dataset, max_samples): # 这个数据集的处理有点意思\n",
    "    #     \"\"\"处理维基百科数据集\"\"\"\n",
    "    #     samples = dataset[\"train\"][:max_samples]\n",
    "    #     processed = []\n",
    "        \n",
    "    #     for i in range(len(samples['text'])):\n",
    "    #         text = samples['text'][i]\n",
    "    #         if len(text) < 100:  # 过滤过短的文本\n",
    "    #             continue\n",
    "                \n",
    "    #         processed.append({\n",
    "    #             'type': 'knowledge',\n",
    "    #             'instruction': '请解释以下内容：',\n",
    "    #             'input': text[:500],  # 取前500字符作为输入\n",
    "    #             'output': text[500:1000] if len(text) > 500 else ''  # 后续内容作为输出\n",
    "    #         })\n",
    "        \n",
    "    #     return processed\n",
    "    \n",
    "    def _process_alpaca(self, dataset, max_samples):\n",
    "        \"\"\"处理Alpaca数据集\"\"\"\n",
    "        samples = dataset[\"train\"][:max_samples]\n",
    "        processed_data = []\n",
    "\n",
    "        for i in range(len(samples['instruction_zh'])):\n",
    "            messages = []\n",
    "            user_content = (f\"{samples['instruction_zh'][i]}\\n\\n\"\n",
    "                          f\"输入：{samples['input_zh'][i]}\" if samples['input_zh'][i] \n",
    "                          else samples['instruction_zh'][i])\n",
    "            \n",
    "            # 传入用户问题和模型回答来生成系统提示词\n",
    "            system_prompt = self._get_random_system_prompt(\n",
    "                user_content=user_content,\n",
    "                model_content=samples['output_zh'][i]\n",
    "            )\n",
    "            \n",
    "            if system_prompt:\n",
    "                messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "                \n",
    "            messages.extend([\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "                {\"role\": \"model\", \"content\": samples['output_zh'][i]}\n",
    "            ])\n",
    "            \n",
    "            processed_data.append({'messages': messages})\n",
    "            \n",
    "            # 添加简单的进度显示\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Alpaca数据集处理进度: {i + 1}/{len(samples)}\")\n",
    "        \n",
    "        return processed_data\n",
    "    \n",
    "    def format_for_training(self):\n",
    "        \"\"\"将数据格式化为训练所需的格式\"\"\"\n",
    "        formatted_data = []\n",
    "        \n",
    "        for item in self.processed_data:\n",
    "            dialogue_str = apply_chat_template(item['messages'])\n",
    "            formatted_data.append({\n",
    "                'text': dialogue_str,\n",
    "                'type': 'instruction'  # 保留type字段以便统计\n",
    "            })\n",
    "            \n",
    "        return formatted_data\n",
    "    \n",
    "    def save_dataset(self, formatted_data, output_path='./base_stage_data', train_ratio=0.8):\n",
    "        \"\"\"保存处理后的数据集\"\"\"\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        # 随机打乱数据\n",
    "        np.random.shuffle(formatted_data)\n",
    "        \n",
    "        # 划分训练集和验证集\n",
    "        split_idx = int(len(formatted_data) * train_ratio)\n",
    "        train_data = formatted_data[:split_idx]\n",
    "        valid_data = formatted_data[split_idx:]\n",
    "        \n",
    "        # 保存训练集\n",
    "        train_path = os.path.join(output_path, 'train.json')\n",
    "        with open(train_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        # 保存验证集\n",
    "        valid_path = os.path.join(output_path, 'valid.json')\n",
    "        with open(valid_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(valid_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(f\"数据集已保存：\")\n",
    "        print(f\"- 训练集: {train_path} ({len(train_data)} 条数据)\")\n",
    "        print(f\"- 验证集: {valid_path} ({len(valid_data)} 条数据)\")\n",
    "        \n",
    "        # 输出数据集统计信息\n",
    "        self._print_dataset_stats(formatted_data)\n",
    "    \n",
    "    def _print_dataset_stats(self, data):\n",
    "        \"\"\"打印数据集统计信息\"\"\"\n",
    "        print(\"\\n数据集统计信息:\")\n",
    "        \n",
    "        # 统计不同类型的数据数量\n",
    "        type_counts = {}\n",
    "        for item in data:\n",
    "            type_counts[item['type']] = type_counts.get(item['type'], 0) + 1\n",
    "            \n",
    "        print(\"\\n数据类型分布:\")\n",
    "        for data_type, count in type_counts.items():\n",
    "            percentage = (count / len(data)) * 100\n",
    "            print(f\"- {data_type}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "def main():\n",
    "    # 1. 初始化数据集构建器\n",
    "    builder = BaseDatasetBuilder()\n",
    "    \n",
    "    # try:\n",
    "    # 2. 加载数据集\n",
    "    builder.load_base_datasets()\n",
    "        \n",
    "    # 3. 处理数据集\n",
    "    builder.preprocess_data(max_samples_per_dataset=50000)\n",
    "        \n",
    "    # 4. 格式化数据\n",
    "    formatted_data = builder.format_for_training()     \n",
    "        \n",
    "    # 5. 保存数据集\n",
    "    builder.save_dataset(formatted_data)\n",
    "        \n",
    "    print(\"\\n基础阶段数据集构建完成！\")\n",
    "        \n",
    "    # except Exception as e:\n",
    "    #     print(f\"构建数据集时出错: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载基础数据集...\n",
      "✓ Belle数据集加载完成\n",
      "✓ Alpaca中文数据集加载完成\n",
      "处理 belle 数据集...\n",
      "Belle数据集处理进度: 100/3\n",
      "Belle数据集处理进度: 200/3\n",
      "Belle数据集处理进度: 300/3\n",
      "Belle数据集处理进度: 400/3\n",
      "Belle数据集处理进度: 500/3\n",
      "Belle数据集处理进度: 600/3\n",
      "Belle数据集处理进度: 700/3\n",
      "Belle数据集处理进度: 800/3\n",
      "Belle数据集处理进度: 900/3\n",
      "Belle数据集处理进度: 1000/3\n",
      "Belle数据集处理进度: 1100/3\n",
      "Belle数据集处理进度: 1200/3\n",
      "Belle数据集处理进度: 1300/3\n",
      "Belle数据集处理进度: 1400/3\n",
      "Belle数据集处理进度: 1500/3\n",
      "Belle数据集处理进度: 1600/3\n",
      "Belle数据集处理进度: 1700/3\n",
      "Belle数据集处理进度: 1800/3\n",
      "Belle数据集处理进度: 1900/3\n",
      "Belle数据集处理进度: 2000/3\n",
      "Belle数据集处理进度: 2100/3\n",
      "Belle数据集处理进度: 2200/3\n",
      "Belle数据集处理进度: 2300/3\n",
      "Belle数据集处理进度: 2400/3\n",
      "Belle数据集处理进度: 2500/3\n",
      "Belle数据集处理进度: 2600/3\n",
      "Belle数据集处理进度: 2700/3\n",
      "Belle数据集处理进度: 2800/3\n",
      "Belle数据集处理进度: 2900/3\n",
      "Belle数据集处理进度: 3000/3\n",
      "Belle数据集处理进度: 3100/3\n",
      "Belle数据集处理进度: 3200/3\n",
      "Belle数据集处理进度: 3300/3\n",
      "Belle数据集处理进度: 3400/3\n",
      "Belle数据集处理进度: 3500/3\n",
      "Belle数据集处理进度: 3600/3\n",
      "Belle数据集处理进度: 3700/3\n",
      "Belle数据集处理进度: 3800/3\n",
      "Belle数据集处理进度: 3900/3\n",
      "Belle数据集处理进度: 4000/3\n",
      "Belle数据集处理进度: 4100/3\n",
      "Belle数据集处理进度: 4200/3\n",
      "Belle数据集处理进度: 4300/3\n",
      "Belle数据集处理进度: 4400/3\n",
      "Belle数据集处理进度: 4500/3\n",
      "Belle数据集处理进度: 4600/3\n",
      "Belle数据集处理进度: 4700/3\n",
      "Belle数据集处理进度: 4800/3\n",
      "Belle数据集处理进度: 4900/3\n",
      "Belle数据集处理进度: 5000/3\n",
      "Belle数据集处理进度: 5100/3\n",
      "Belle数据集处理进度: 5200/3\n",
      "Belle数据集处理进度: 5300/3\n",
      "Belle数据集处理进度: 5400/3\n",
      "Belle数据集处理进度: 5500/3\n",
      "Belle数据集处理进度: 5600/3\n",
      "Belle数据集处理进度: 5700/3\n",
      "Belle数据集处理进度: 5800/3\n",
      "Belle数据集处理进度: 5900/3\n",
      "Belle数据集处理进度: 6000/3\n",
      "Belle数据集处理进度: 6100/3\n",
      "Belle数据集处理进度: 6200/3\n",
      "Belle数据集处理进度: 6300/3\n",
      "Belle数据集处理进度: 6400/3\n",
      "Belle数据集处理进度: 6500/3\n",
      "Belle数据集处理进度: 6600/3\n",
      "Belle数据集处理进度: 6700/3\n",
      "Belle数据集处理进度: 6800/3\n",
      "Belle数据集处理进度: 6900/3\n",
      "Belle数据集处理进度: 7000/3\n",
      "Belle数据集处理进度: 7100/3\n",
      "Belle数据集处理进度: 7200/3\n",
      "Belle数据集处理进度: 7300/3\n",
      "Belle数据集处理进度: 7400/3\n",
      "Belle数据集处理进度: 7500/3\n",
      "Belle数据集处理进度: 7600/3\n",
      "Belle数据集处理进度: 7700/3\n",
      "Belle数据集处理进度: 7800/3\n",
      "Belle数据集处理进度: 7900/3\n",
      "Belle数据集处理进度: 8000/3\n",
      "Belle数据集处理进度: 8100/3\n",
      "Belle数据集处理进度: 8200/3\n",
      "Belle数据集处理进度: 8300/3\n",
      "Belle数据集处理进度: 8400/3\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
