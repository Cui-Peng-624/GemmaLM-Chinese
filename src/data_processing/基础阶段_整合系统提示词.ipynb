{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此ipynb文件的目的是合并train.json, valid.json和生成的系统提示词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoDL官方学术资源加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# # 读取train.json\n",
    "# with open('stage1/data_raw/train.json', 'r') as file:\n",
    "#     train_data = json.load(file) # list, [{}, {}, ...]\n",
    "\n",
    "# # 读取valid.json\n",
    "# with open('stage1/data_raw/valid.json', 'r') as file:\n",
    "#     valid_data = json.load(file) # list, [{}, {}, ...]\n",
    "# print(len(valid_data))\n",
    "\n",
    "# # 读取生成的训练阶段的系统提示词\n",
    "# with open('stage1/system_prompts_gpt/system_prompts_batch_0.json', 'r') as file: # train_system_prompt.json\n",
    "#     train_system_prompt = json.load(file) # []\n",
    "\n",
    "# 读取生成的验证阶段的系统提示词\n",
    "# with open('stage1/system_prompts_gpt/valid_system_prompt.json', 'r') as file:\n",
    "#     valid_system_prompt = json.load(file) # []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_train_data, len_train_system_prompt = len(train_data), len(train_system_prompt)\n",
    "# len_valid_data, len_valid_system_prompt = len(valid_data), len(valid_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到Python路径\n",
    "project_root = \"/home/cuipeng/Gemma\"\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# 现在可以正常导入src下的模块\n",
    "from src.core.model.model_initializer import initialize_model_and_tokenizer\n",
    "from src.core.utils.model_utils import generate_response, apply_chat_template, format_system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并train.json和train_system_prompt.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed_system_prompts = [\n",
    "#     \"你是一个有帮助的AI助手。\",\n",
    "#     \"作为一个AI助手，你会提供准确、有帮助的回答。\",\n",
    "#     \"你是一个知识渊博的AI助手，善于解释复杂概念。\",\n",
    "#     \"你是一个友好的AI助手，会用通俗易懂的方式回答问题。\",\n",
    "#     \"作为AI助手，你会以专业、客观的方式提供帮助。\",\n",
    "#     \"你是一个可靠的AI助手，始终保持耐心和专注。\",\n",
    "#     \"作为AI助手，你擅长提供清晰、结构化的解答。\",\n",
    "#     \"你是一个认真负责的AI助手，会仔细理解并回答问题。\",\n",
    "#     \"作为AI助手，你会以严谨的态度提供准确的信息。\",\n",
    "#     \"你是一个智能AI助手，能够理解上下文并给出恰当的回应。\"\n",
    "#     \"作为一个AI助理，我会专业、友善地回答你的问题。\",\n",
    "#     \"我是一个知识渊博的AI，很高兴能帮助你解决问题。\",\n",
    "#     \"让我们一起探讨这个问题，我会尽我所能提供帮助。\",\n",
    "#     \"我是一个智能助手，擅长理解和解答各类问题。\",\n",
    "#     \"作为你的AI伙伴，我会用清晰简洁的方式回答问题。\",\n",
    "#     \"我是一个全能型AI助手，可以处理各种类型的任务。\",\n",
    "#     \"作为AI助手，我会以严谨专业的态度回应你的需求。\",\n",
    "#     \"我是一个AI助理，会用通俗易懂的方式解释复杂概念。\",\n",
    "#     \"作为你的AI搭档，我会用准确和有见地的方式回答问题。\"\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required_fixed_prompt_num = len_train_data * 0.4\n",
    "\n",
    "# for i in range(len(train_data)):\n",
    "#     if i < len_train_system_prompt: # 添加由gpt生成的系统提示词\n",
    "#         train_data[i]['text'] = format_model(train_system_prompt[i]) + train_data[i]['text']\n",
    "#     elif i < len_train_system_prompt + required_fixed_prompt_num: # 添加固定的系统提示词\n",
    "#         train_data[i]['text'] = format_model(np.random.choice(fixed_system_prompts)) + train_data[i]['text']\n",
    "#     else: # 不添加提示词\n",
    "#         train_data[i]['text'] = train_data[i]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 统计添加的不同类型的系统提示词的数量\n",
    "# print(f\"添加由gpt生成的系统提示词的数量: {len_train_system_prompt} 条，占比: {(len_train_system_prompt / len_train_data) * 100}%\")\n",
    "# print(f\"添加固定系统提示词的数量: {required_fixed_prompt_num} 条，占比: {(required_fixed_prompt_num / len_train_data) * 100}%\")\n",
    "# print(f\"未添加系统提示词的数量: {len_train_data - len_train_system_prompt - required_fixed_prompt_num} 条，占比: {((len_train_data - len_train_system_prompt - required_fixed_prompt_num) / len_train_data) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 保存\n",
    "# import os\n",
    "\n",
    "# # 确保目录存在\n",
    "# os.makedirs('stage1/data_final', exist_ok=True)\n",
    "\n",
    "# with open('stage1/data_final/train.json', 'w') as file:\n",
    "#     json.dump(train_data, file, indent=4) # indent=4 表示缩进4个空格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并valid.json和valid_system_prompt.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数化之后的方法 - 最终使用的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIntegrator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_raw_path: str,\n",
    "        valid_raw_path: str,\n",
    "        system_prompts_dir: str,\n",
    "        output_dir: str,\n",
    "        prompt_ratio: dict = {\"gpt\": 0.4, \"fixed\": 0.4, \"none\": 0.2}\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化数据整合器\n",
    "        \n",
    "        Args:\n",
    "            train_raw_path: 原始训练数据路径\n",
    "            valid_raw_path: 原始验证数据路径\n",
    "            system_prompts_dir: GPT生成的系统提示词目录\n",
    "            output_dir: 输出目录\n",
    "            prompt_ratio: 三种提示词的目标比例，和需要为1\n",
    "        \"\"\"\n",
    "        self.train_raw_path = train_raw_path\n",
    "        self.valid_raw_path = valid_raw_path\n",
    "        self.system_prompts_dir = system_prompts_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.prompt_ratio = prompt_ratio\n",
    "        \n",
    "        # 验证比例和是否为1\n",
    "        if abs(sum(prompt_ratio.values()) - 1) > 1e-6:\n",
    "            raise ValueError(\"提示词比例之和必须为1\")\n",
    "        \n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # 固定的系统提示词列表\n",
    "        self.fixed_system_prompts = [\n",
    "            \"你是一个有帮助的AI助手。\",\n",
    "            \"作为一个AI助手，你会提供准确、有帮助的回答。\",\n",
    "            \"你是一个知识渊博的AI助手，善于解释复杂概念。\",\n",
    "            \"你是一个友好的AI助手，会用通俗易懂的方式回答问题。\",\n",
    "            \"作为AI助手，你会以专业、客观的方式提供帮助。\",\n",
    "            \"你是一个可靠的AI助手，始终保持耐心和专注。\",\n",
    "            \"作为AI助手，你擅长提供清晰、结构化的解答。\",\n",
    "            \"你是一个认真负责的AI助手，会仔细理解并回答问题。\",\n",
    "            \"作为AI助手，你会以严谨的态度提供准确的信息。\",\n",
    "            \"你是一个智能AI助手，能够理解上下文并给出恰当的回应。\"\n",
    "            \"作为一个AI助理，我会专业、友善地回答你的问题。\",\n",
    "            \"我是一个知识渊博的AI，很高兴能帮助你解决问题。\",\n",
    "            \"让我们一起探讨这个问题，我会尽我所能提供帮助。\",\n",
    "            \"我是一个智能助手，擅长理解和解答各类问题。\",\n",
    "            \"作为你的AI伙伴，我会用清晰简洁的方式回答问题。\",\n",
    "            \"我是一个全能型AI助手，可以处理各种类型的任务。\",\n",
    "            \"作为AI助手，我会以严谨专业的态度回应你的需求。\",\n",
    "            \"我是一个AI助理，会用通俗易懂的方式解释复杂概念。\",\n",
    "            \"作为你的AI搭档，我会用准确和有见地的方式回答问题。\"\n",
    "            ]\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"加载所有需要的数据\"\"\"\n",
    "        # 加载原始数据\n",
    "        with open(self.train_raw_path, 'r', encoding='utf-8') as f:\n",
    "            self.train_data = json.load(f)\n",
    "        with open(self.valid_raw_path, 'r', encoding='utf-8') as f:\n",
    "            self.valid_data = json.load(f)\n",
    "            \n",
    "        # 加载GPT生成的系统提示词\n",
    "        self.train_system_prompts = []\n",
    "        self.valid_system_prompts = []\n",
    "        \n",
    "        # 加载训练集系统提示词\n",
    "        train_prompt_path = os.path.join(self.system_prompts_dir, \"train_system_prompt.json\")\n",
    "        if os.path.exists(train_prompt_path):\n",
    "            with open(train_prompt_path, 'r', encoding='utf-8') as f:\n",
    "                self.train_system_prompts = json.load(f)\n",
    "                \n",
    "        # 加载验证集系统提示词\n",
    "        valid_prompt_path = os.path.join(self.system_prompts_dir, \"valid_system_prompt.json\")\n",
    "        if os.path.exists(valid_prompt_path):\n",
    "            with open(valid_prompt_path, 'r', encoding='utf-8') as f:\n",
    "                self.valid_system_prompts = json.load(f)\n",
    "                \n",
    "        print(\"数据加载完成：\")\n",
    "        print(f\"- 训练集数据: {len(self.train_data)} 条\")\n",
    "        print(f\"- 验证集数据: {len(self.valid_data)} 条\")\n",
    "        print(f\"- 训练集GPT生成提示词: {len(self.train_system_prompts)} 条\")\n",
    "        print(f\"- 验证集GPT生成提示词: {len(self.valid_system_prompts)} 条\")\n",
    "\n",
    "    def integrate_data(self, data, system_prompts, dataset_type: str):\n",
    "        \"\"\"整合数据集\n",
    "        \n",
    "        Args:\n",
    "            data: 原始数据\n",
    "            system_prompts: 实际GPT生成的系统提示词\n",
    "            dataset_type: 数据集类型（'train' 或 'valid'）\n",
    "        \"\"\"\n",
    "        total_count = len(data)\n",
    "        actual_gpt_count = len(system_prompts) # 实际GPT生成的系统提示词数量，也就是“system_prompts_gpt”目录下文件中生成的条数\n",
    "        target_gpt_count = int(total_count * self.prompt_ratio[\"gpt\"]) # 目标比例 * 总数\n",
    "        \n",
    "        print(f\"\\n{dataset_type} 数据集统计:\")\n",
    "        print(f\"总数据量: {total_count}\")\n",
    "        print(f\"目标GPT生成提示词数量: {target_gpt_count} ({self.prompt_ratio['gpt']*100:.1f}%)\")\n",
    "        print(f\"实际GPT生成提示词数量: {actual_gpt_count}\")\n",
    "        \n",
    "        # 确定使用的GPT提示词数量\n",
    "        if actual_gpt_count >= target_gpt_count: # 如果实际数量足够，使用目标数量\n",
    "            used_gpt_count = target_gpt_count # used_gpt_count: 最终要使用多少个gpt生成的系统提示词\n",
    "            fixed_count = int(total_count * self.prompt_ratio[\"fixed\"])\n",
    "            none_count = total_count - used_gpt_count - fixed_count\n",
    "        else: # 如果实际数量不足，使用全部实际数量，并重新分配剩余比例\n",
    "            used_gpt_count = actual_gpt_count\n",
    "            shortage_count = total_count - ((self.prompt_ratio[\"fixed\"] + self.prompt_ratio[\"none\"]) * total_count) - actual_gpt_count # 使用数量而不是比例，因为小数计算可能会出现误差\n",
    "            # 将未满足的比例分配给fixed\n",
    "            fixed_count = self.prompt_ratio[\"fixed\"] * total_count + shortage_count\n",
    "            none_count = total_count - used_gpt_count - fixed_count # 应该就等于 self.prompt_ratio[\"none\"] * total_count\n",
    "        \n",
    "        print(\"\\n最终分配:\")\n",
    "        print(f\"使用GPT生成提示词数量: {used_gpt_count} ({used_gpt_count/total_count*100:.1f}%)\")\n",
    "        print(f\"使用固定提示词数量: {fixed_count} ({fixed_count/total_count*100:.1f}%)\")\n",
    "        print(f\"无提示词数量: {none_count} ({none_count/total_count*100:.1f}%)\")\n",
    "        \n",
    "        # 创建新的数据列表\n",
    "        processed_data = data.copy()\n",
    "        \n",
    "        if actual_gpt_count > target_gpt_count: # 如果实际数量小于目标数量，则截取前“target_gpt_count”个 \n",
    "            selected_prompts = system_prompts[:target_gpt_count]\n",
    "        else: # 如果实际数量大于目标数量，则全部使用\n",
    "            selected_prompts = system_prompts\n",
    "\n",
    "        # 添加系统提示词\n",
    "        for i in range(len(processed_data)): # used_gpt_count: 要使用多少个gpt生成的系统提示词\n",
    "            if i < used_gpt_count:  # 添加GPT生成的提示词\n",
    "                processed_data[i]['text'] = format_system(selected_prompts[i]) + processed_data[i]['text']\n",
    "            elif i < used_gpt_count + fixed_count:  # 添加固定提示词\n",
    "                processed_data[i]['text'] = format_system(np.random.choice(self.fixed_system_prompts)) + processed_data[i]['text']\n",
    "            # 其余数据保持原样\n",
    "        \n",
    "        return processed_data\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"处理并保存数据\"\"\"\n",
    "        # 加载数据\n",
    "        self.load_data()\n",
    "        \n",
    "        # 处理训练集\n",
    "        processed_train = self.integrate_data(\n",
    "            self.train_data,\n",
    "            self.train_system_prompts,\n",
    "            'train'\n",
    "        )\n",
    "        \n",
    "        # 处理验证集\n",
    "        processed_valid = self.integrate_data(\n",
    "            self.valid_data,\n",
    "            self.valid_system_prompts,\n",
    "            'valid'\n",
    "        )\n",
    "        \n",
    "        # 保存处理后的数据\n",
    "        train_path = os.path.join(self.output_dir, 'train.json')\n",
    "        with open(train_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_train, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        valid_path = os.path.join(self.output_dir, 'valid.json')\n",
    "        with open(valid_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_valid, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"\\n数据保存完成：\")\n",
    "        print(f\"- 训练集: {train_path} ({len(processed_train)} 条数据)\")\n",
    "        print(f\"- 验证集: {valid_path} ({len(processed_valid)} 条数据)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompts_dir='stage1/system_prompts_gpt',\n",
    "\n",
    "# with open(f'{system_prompts_dir}/train_system_prompt.json', 'r') as file:\n",
    "#     train_data = json.load(file) # list, [{}, {}, ...]\n",
    "#     len_train_system_prompt = len(train_data)\n",
    "\n",
    "# with open(f'{system_prompts_dir}/valid_system_prompt.json', 'r') as file:\n",
    "#     valid_data = json.load(file) # list, [{}, {}, ...]\n",
    "#     len_valid_system_prompt = len(valid_data)\n",
    "\n",
    "# 使用示例\n",
    "def main():\n",
    "    integrator = DataIntegrator(\n",
    "        train_raw_path='stage1/data_raw/train.json',\n",
    "        valid_raw_path='stage1/data_raw/valid.json',\n",
    "        system_prompts_dir='stage1/system_prompts_gpt',\n",
    "        output_dir='stage1/data_final',\n",
    "        prompt_ratio={\"gpt\": 0.4, \"fixed\": 0.4, \"none\": 0.2}\n",
    "    )\n",
    "\n",
    "    integrator.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载完成：\n",
      "- 训练集数据: 80000 条\n",
      "- 验证集数据: 1000 条\n",
      "- 训练集GPT生成提示词: 32500 条\n",
      "- 验证集GPT生成提示词: 500 条\n",
      "\n",
      "train 数据集统计:\n",
      "总数据量: 80000\n",
      "目标GPT生成提示词数量: 32000 (40.0%)\n",
      "实际GPT生成提示词数量: 32500\n",
      "\n",
      "最终分配:\n",
      "使用GPT生成提示词数量: 32000 (40.0%)\n",
      "使用固定提示词数量: 32000 (40.0%)\n",
      "无提示词数量: 16000 (20.0%)\n",
      "\n",
      "valid 数据集统计:\n",
      "总数据量: 1000\n",
      "目标GPT生成提示词数量: 400 (40.0%)\n",
      "实际GPT生成提示词数量: 500\n",
      "\n",
      "最终分配:\n",
      "使用GPT生成提示词数量: 400 (40.0%)\n",
      "使用固定提示词数量: 400 (40.0%)\n",
      "无提示词数量: 200 (20.0%)\n",
      "\n",
      "数据保存完成：\n",
      "- 训练集: stage1/data_final/train.json (80000 条数据)\n",
      "- 验证集: stage1/data_final/valid.json (1000 条数据)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成的数据集的系统提示词是顺序的，前40%是gpt生成的，后40%是固定提示词，最后20%是原数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
