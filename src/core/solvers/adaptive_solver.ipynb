{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # type: ignore\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM # type: ignore\n",
    "from transformers import BitsAndBytesConfig # type: ignore\n",
    "from peft import PeftModel # type: ignore\n",
    "\n",
    "from src.core.model.model_initializer import initialize_model_and_tokenizer\n",
    "from src.core.utils.model_utils import generate_response, apply_chat_template\n",
    "from src.core.solvers.l2m import L2MSolver\n",
    "from src.core.solvers.self_verification import SelfVerifier\n",
    "from src.core.solvers.simplified_enhanced_solver import SimplifiedEnhancedSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveSolver:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        \"\"\"\n",
    "        初始化自适应求解器\n",
    "        Args:\n",
    "            model: 已初始化的模型实例\n",
    "            tokenizer: 已初始化的tokenizer实例\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # 初始化各个求解器\n",
    "        self.l2m_solver = L2MSolver(model, tokenizer)\n",
    "        self.verifier = SelfVerifier(model, tokenizer)\n",
    "        self.enhanced_solver = SimplifiedEnhancedSolver(self.l2m_solver, self.verifier)\n",
    "        \n",
    "        # 添加评估示例\n",
    "        self.complexity_examples = \"\"\"\n",
    "        示例问题及其复杂度评分：\n",
    "\n",
    "        问题1：今天的天气怎么样？\n",
    "        复杂度：0.1\n",
    "        原因：简单的事实性问题，可直接回答。\n",
    "\n",
    "        问题2：请解释DNA的双螺旋结构。\n",
    "        复杂度：0.5\n",
    "        原因：需要分步骤解释，包含多个知识点。\n",
    "\n",
    "        问题3：分析人工智能对未来社会的影响。\n",
    "        复杂度：0.9\n",
    "        原因：需要多角度分析，涉及复杂因果关系。\n",
    "        \"\"\"\n",
    "        \n",
    "    def evaluate_complexity(self, question):\n",
    "        \"\"\"评估问题复杂度\"\"\"\n",
    "        dialogue = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"你是一个专业的问题复杂度评估专家。请根据示例评估问题的复杂度。\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"{self.complexity_examples}\n",
    "\n",
    "请评估这个问题的复杂度：{question}\n",
    "\n",
    "评分标准：\n",
    "0.0-0.3：简单问题，可直接回答\n",
    "0.3-0.7：中等复杂度，需要分步骤解答\n",
    "0.7-1.0：高度复杂，需要详细分析\n",
    "\n",
    "请只输出一个0-1之间的数字分数。\"\"\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        prompt = apply_chat_template(dialogue)\n",
    "        response = generate_response(\n",
    "            self.model, \n",
    "            self.tokenizer, \n",
    "            prompt,\n",
    "            max_new_tokens=8,\n",
    "            temperature=0.1\n",
    "        ).strip()\n",
    "        \n",
    "        try:\n",
    "            score = float(response)\n",
    "            return min(max(score, 0.0), 1.0)\n",
    "        except ValueError:\n",
    "            return 0.5\n",
    "\n",
    "    def solve(self, question):\n",
    "        \"\"\"根据问题复杂度选择合适的求解方法\"\"\"\n",
    "        complexity = self.evaluate_complexity(question)\n",
    "        print(f\"问题复杂度评分: {complexity}\")\n",
    "        \n",
    "        if complexity < 0.3:\n",
    "            print(\"使用直接回答...\")\n",
    "            dialogue = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"你是一个专业的问答助手。请直接、简洁地回答问题。\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question\n",
    "                }\n",
    "            ]\n",
    "            prompt = apply_chat_template(dialogue)\n",
    "            answer = generate_response(\n",
    "                self.model,\n",
    "                self.tokenizer,\n",
    "                prompt,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"method\": \"direct\",\n",
    "                \"complexity\": complexity,\n",
    "                \"answer\": answer\n",
    "            }\n",
    "            \n",
    "        elif complexity < 0.7:\n",
    "            print(\"使用L2M方法...\")\n",
    "            result = self.l2m_solver.solve(question)\n",
    "            return {\n",
    "                \"method\": \"l2m\",\n",
    "                \"complexity\": complexity,\n",
    "                \"answer\": result[\"final_answer\"],\n",
    "                \"sub_solutions\": result[\"sub_solutions\"]\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            print(\"使用增强求解方法...\")\n",
    "            result = self.enhanced_solver.solve_complex_question(question)\n",
    "            return {\n",
    "                \"method\": \"enhanced\",\n",
    "                \"complexity\": complexity,\n",
    "                \"answer\": result[\"final_answer\"],\n",
    "                \"sub_solutions\": result[\"sub_solutions\"],\n",
    "                \"verification\": result[\"verification\"]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化模型和tokenizer\n",
    "    model_path = \"google/gemma-2-9b\"\n",
    "    cache_dir = \"/root/autodl-tmp/gemma\"\n",
    "    # lora_path = \"/root/autodl-tmp/models/stage1/gemma-base-zh-final\"\n",
    "    lora_path = None\n",
    "    \n",
    "    # 初始化基础组件\n",
    "    model, tokenizer = initialize_model_and_tokenizer(\n",
    "        model_path=model_path,\n",
    "        cache_dir=cache_dir,\n",
    "        lora_path=lora_path,\n",
    "        use_quantization=True\n",
    "    )\n",
    "    \n",
    "    # 初始化自适应求解器\n",
    "    solver = AdaptiveSolver(model, tokenizer)\n",
    "    \n",
    "    # 测试不同复杂度的问题\n",
    "    questions = [\n",
    "        \"今天星期几？\",  # 简单\n",
    "        \"请解释光合作用的过程。\",  # 中等\n",
    "        \"分析人工智能对未来社会的影响。\"  # 复杂\n",
    "    ]\n",
    "    \n",
    "    for q in questions:\n",
    "        print(f\"\\n处理问题: {q}\")\n",
    "        result = solver.solve(q)\n",
    "        print(f\"使用方法: {result['method']}\")\n",
    "        print(f\"复杂度评分: {result['complexity']}\")\n",
    "        print(\"答案:\", result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
