{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoDL官方学术资源加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到Python路径\n",
    "project_root = \"/home/cuipeng/Gemma\"\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# 现在可以正常导入src下的模块\n",
    "from src.core.model.model_initializer import initialize_model_and_tokenizer\n",
    "from src.core.utils.model_utils import generate_response, apply_chat_template, format_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least-to-Most Prompting (L2M)\n",
    "\n",
    "import torch # type: ignore\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM # type: ignore\n",
    "from transformers import BitsAndBytesConfig # type: ignore\n",
    "from peft import PeftModel  # type: ignore # 导入PeftModel用于加载微调模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2MSolver:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        \"\"\"\n",
    "        初始化L2M求解器\n",
    "        Args:\n",
    "            model: 已初始化的模型实例\n",
    "            tokenizer: 已初始化的tokenizer实例\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # 添加 few-shot 示例\n",
    "        self.decompose_examples = \"\"\"\n",
    "        问题1：量子纠缠现象是什么，它有什么应用？\n",
    "\n",
    "        分解步骤：\n",
    "        1. 什么是量子态和量子叠加？\n",
    "        2. 量子纠缠的基本概念是什么？\n",
    "        3. 量子纠缠现象如何被实验证实？\n",
    "        4. 量子纠缠在量子通信和量子计算中有什么应用？\n",
    "\n",
    "        问题2：区块链技术如何确保交易安全？\n",
    "\n",
    "        分解步骤：\n",
    "        1. 什么是哈希函数和加密算法？\n",
    "        2. 区块链的基本结构是什么？\n",
    "        3. 区块链如何实现去中心化验证？\n",
    "        4. 为什么区块链的交易记录难以篡改？\n",
    "        \"\"\"\n",
    "\n",
    "        self.decompose_system_prompt = \"\"\"\n",
    "        你是一位擅长分析和分解复杂问题的助手。你的主要任务是将复杂问题分解成若干个由简单到复杂的子问题。\n",
    "\n",
    "        在分解问题时，你应该：\n",
    "        1. 仔细分析问题的各个组成部分\n",
    "        2. 识别解决问题所需的关键知识和技能\n",
    "        3. 确保子问题之间具有逻辑连贯性\n",
    "        4. 确保每个子问题的答案都能为最终问题的解决提供必要的支持\n",
    "\n",
    "        你应该遵循以下原则：\n",
    "        1. 子问题数量应保持在3-5个之间\n",
    "        2. 子问题应该按照难度递增的顺序排列\n",
    "        3. 每个子问题都应该明确、具体且可回答\n",
    "        4. 避免输出无关的解释或评论\n",
    "\n",
    "        输出格式要求：\n",
    "        1. 必须严格按照\"分解步骤：\"开头\n",
    "        2. 每个子问题前标注序号\n",
    "        3. 除了分解的子问题外，不要包含任何其他内容\n",
    "\n",
    "        示例输出：\n",
    "        分解步骤：\n",
    "        1. [简单的子问题]\n",
    "        2. [稍复杂的子问题]\n",
    "        3. [较复杂的子问题]\n",
    "        4. [最复杂的子问题]\n",
    "\n",
    "        请记住：你的回答应该简洁明了，只包含问题分解本身，不需要任何额外的解释或评论。\n",
    "        \"\"\"\n",
    "\n",
    "    def decompose_question(self, complex_question):\n",
    "        dialogue = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": self.decompose_system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"请参考以下示例，将复杂问题分解为子问题：\n",
    "\n",
    "    {self.decompose_examples}\n",
    "\n",
    "    现在请分解这个问题：{complex_question}\n",
    "\n",
    "    请严格按照示例格式输出，以\"分解步骤：\"开头。\"\"\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        prompt = apply_chat_template(dialogue)\n",
    "        response = generate_response(self.model, self.tokenizer, prompt, max_new_tokens=512, temperature=0.3) # 降低温度，以保持回答的一致性\n",
    "        \n",
    "        # 改进子问题提取方法 ########################################################\n",
    "        def extract_questions(text):\n",
    "            import re\n",
    "            \n",
    "            pattern = r'\\d+\\.\\s*([^\\n]+)' # 匹配：1. This is the first line. 捕获组：This is the first line.\n",
    "            questions = re.findall(pattern, text)\n",
    "            \n",
    "            if questions: # 如果question不为空，则表示提取成功\n",
    "                return questions\n",
    "            \n",
    "            # 如果第一次提取失败，则尝试从\"分解步骤：\"开始提取\n",
    "            if \"分解步骤：\" in text:\n",
    "                text = text.split(\"分解步骤：\")[1].strip()\n",
    "            \n",
    "            lines = text.split('\\n')\n",
    "            questions = []\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                line = re.sub(r'^\\d+\\.\\s*', '', line)\n",
    "                if line and not line.startswith('分解步骤') and not line.startswith('要求'):\n",
    "                    questions.append(line)\n",
    "            \n",
    "            return questions\n",
    "        ########################################################################\n",
    "        \n",
    "        sub_questions = extract_questions(response)\n",
    "        \n",
    "        # 边缘处理\n",
    "        if not sub_questions:\n",
    "            backup_dialogue = [\n",
    "                {\"role\": \"system\", \"content\": self.decompose_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"之前的回答格式不正确。请重新分解问题，严格按照以下格式输出：\n",
    "                 问题：{complex_question}\n",
    "\n",
    "                 1. [第一个子问题]\n",
    "                 2. [第二个子问题]\n",
    "                 3. [第三个子问题]\n",
    "                 ...\n",
    "\n",
    "                请只输出编号和问题，不要有其他内容。\"\"\"}\n",
    "                ]\n",
    "            backup_prompt = apply_chat_template(backup_dialogue)\n",
    "            \n",
    "            response = generate_response(self.model, self.tokenizer, backup_prompt)\n",
    "            sub_questions = extract_questions(response)\n",
    "        \n",
    "        # 如果还没有sub_questions，则报错\n",
    "        if not sub_questions:\n",
    "            raise ValueError(\"无法正确分解问题，请检查模型输出格式\")\n",
    "            \n",
    "        return sub_questions\n",
    "\n",
    "    def solve_with_context(self, question, context=\"\"):\n",
    "        dialogue = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"你是一个专业的问题解答专家。请参考示例风格，基于已知信息回答问题。\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"示例：\n",
    "    问题：什么是量子态？\n",
    "    已知信息：无\n",
    "    答案：量子态是描述量子系统状态的数学表达，它可以同时存在于多个基本状态的叠加中。这种特性使得量子系统具有独特的性质。\n",
    "\n",
    "    现在请回答：\n",
    "    已知信息：\n",
    "    {context}\n",
    "\n",
    "    问题：{question}\"\"\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        prompt = apply_chat_template(dialogue)\n",
    "        return generate_response(self.model, self.tokenizer, prompt, max_new_tokens=1024, temperature=0.7)\n",
    "\n",
    "    def solve(self, complex_question):\n",
    "        # 1. 分解问题\n",
    "        sub_questions = self.decompose_question(complex_question) # 返回一个list\n",
    "        \n",
    "        # 2. 逐步解决\n",
    "        context = \"\"\n",
    "        solutions = []\n",
    "        \n",
    "        for q in sub_questions:\n",
    "            solution = self.solve_with_context(q, context)\n",
    "            solutions.append({\"question\": q, \"solution\": solution})\n",
    "            context += f\"\\n问题：{q}\\n答案：{solution}\\n\"\n",
    "        \n",
    "        # 3. 最终整合答案\n",
    "        dialogue = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"你是一个专业的总结专家。请参考示例风格，整合前面的解答。\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"示例：\n",
    "    解答过程：\n",
    "    问题：什么是量子计算？\n",
    "    答案：量子计算利用量子力学原理进行计算。\n",
    "\n",
    "    问题：量子计算有什么优势？\n",
    "    答案：可以并行处理大量数据。\n",
    "\n",
    "    总结：量子计算是一种基于量子力学原理的新型计算方式，其最大优势在于并行处理能力。\n",
    "\n",
    "    现在请总结：\n",
    "    解答过程：\n",
    "    {context}\n",
    "\n",
    "    原始问题：{complex_question}\"\"\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        prompt = apply_chat_template(dialogue)\n",
    "        final_answer = generate_response(\n",
    "            self.model,\n",
    "            self.tokenizer,\n",
    "            prompt,\n",
    "            max_new_tokens=1536,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_question\": complex_question,\n",
    "            \"sub_solutions\": solutions,\n",
    "            \"final_answer\": final_answer\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1a86fe1eb449328b64c9d545ebc7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/Gemma/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4624 > 4096). Running this sequence through the model will result in indexing errors\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (8192). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "# 使用示例部分的修改\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化模型和tokenizer\n",
    "    model_path = \"google/gemma-2-9b\"\n",
    "    cache_dir = \"/root/autodl-tmp/gemma\"\n",
    "    # lora_path = \"/root/autodl-tmp/models/stage1/checkpoints/gemma-base-zh/checkpoint-20000\"\n",
    "    lora_path = None\n",
    "\n",
    "    model, tokenizer = initialize_model_and_tokenizer(\n",
    "        model_path=model_path,\n",
    "        cache_dir=cache_dir,\n",
    "        lora_path=lora_path,\n",
    "        use_quantization=True  # 确保使用量化\n",
    "    )\n",
    "    \n",
    "    # 测试示例\n",
    "    solver = L2MSolver(model, tokenizer)\n",
    "    question = \"在NLP领域，请介绍一下位置编码\"\n",
    "    result = solver.solve(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_question': '在NLP领域，请介绍一下位置编码', 'sub_solutions': [{'question': '\"\"\"', 'solution': '\"\"\".format(question)\\n     # 用户输入开始\\nwhile True:\\n   answer = input(\"请输入你的答案:\")\\n   print(\\'你的答案:\\', answer)\\n   if (answer == \\'exit\\'):\\n      break; \\n  # 用户输入结束\\n # 输出结果\\n print(\\'\\\\n\\\\n输出结果:\\')\\n for i in range(len(answers)):\\n   print(answers[i][0]+\\':\\', answers[i][1])\\n\\n\\n```\\n\\nIn []:\\n## 模型部署及推理预测\\n\\nIn []:\\n### 将模型保存为`pickle`格式文件\\n\\n将训练好的ChatGPT-LLM模型保存为`chatgpt_llm.pkl` pickle格式的文件，供用户调用使用。\\n\\nIn []:\\n```python\\nimport joblib\\njoblib.dump(llm, \"chatgpt_llm.pkl\")\\n\\n```\\n\\nOut []:\\n```output\\n[\\'chatgpt_llm.pkl\\']\\n```\\n\\nIn []:\\n### 推理测试\\n\\n对一个随机的句子进行推理，查看结果是否与预期一致。\\n\\nIn []:\\n```python\\ndef predict(text):\\n    return llm.generate(prompt=text, num_beams=5, max_length=20)\\n\\nrandom_sentence = \"这是我第一次用LLM生成文本。\"\\nresult = predict(random_sentence)\\nprint(result)\\n\\n```\\n\\nOut []:\\n```output\\n<output truncated>\\n```\\n\\nIn []:\\n### 使用API接口\\n\\n在本地服务器上创建一个应用，提供RESTful API接口供外部程序调用。\\n\\nIn []:\\n```python\\nfrom flask import Flask, request, jsonify\\napp = Flask(__name__)\\n\\n@app.route(\\'/predict\\')\\ndef predict():\\n    data = request.get_json()\\n    response = {\\n        \\'message\\': \\'\\',\\n        \\'status\\': \\'\\'\\n    }\\n    try:\\n        prompt = data[\\'prompt\\']\\n        prediction = predict(prompt)\\n        response[\\'message\\'] = prediction\\n        response[\\'status\\'] = \\'success\\'\\n    except Exception as e:\\n        response[\\'message\\'] = str(e)\\n        response[\\'status\\'] = \\'error\\'\\n    return jsonify(response)\\n\\n```\\n\\nIn []:\\n启动服务并访问`http://localhost:5000/predict?prompt={your_input}`来体验我们的API接口。\\n\\nIn []:\\n```python\\nif __name__ == \\'__main__\\':\\n    app.run(host=\\'0.0.0.0\\', port=8000, debug=True)\\n\\n```\\n\\nOut []:\\n```output\\n* Serving Flask app \"__main__.py\" (lazy loading)\\n * Environment: production\\n\\x1b[32m   WARNING: This is a development server. Do not use it in production.\\x1b[0m\\n * Debug mode: on\\n* Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\\n127.0.0.1 - - [2023-06-14 19:00:46] \"GET /favicon.ico HTTP/1.1\" 404 -\\n127.0.0.1 - - [2023-06-14 19:00:46] \"GET /static/js/bundle.js HTTP/1.1\" 200 -\\n127.0.0.1 - - [2023-06-14 19:00:46] \"GET /static/css/style.css HTTP/1.1\" 200 -\\n127.0.0.1 - - [2023-06-14 19:00:46] \"GET /static/img/logo.png HTTP/1.1\" 200 -\\n127.0.0.1 - - [2023-06-14 19:00:47] \"POST /favicon.ico HTTP/1.1\" 200 -\\n127.0.0.1 - - [2023-06-14 19:00:47] \"POST /static/js/bundle.js HTTP/1.1\" 200 -\\n127.0.0.1 - - [2023-06-14 19:00:47] \"POST /static/css/style.css HTTP/1.1\" 200 -\\n127.0.0.1 - - [2023-06-14 1'}, {'question': 'def get_sub_problems(problem):', 'solution': '参数说明：\\n        problem: 需要分解成子问题的字符串\\n    返回值说明：\\n        返回分解后的子问题列表\\n    实现思路：\\n        1. 将待处理问题拆分成几个部分（例如“如何成为一名优秀工程师”可拆分为“如何学习技术”、“如何提高技能水平”等）。\\n        2. 对每个部分进行分析，找出其中的核心问题和关键点。\\n        3. 根据这些问题构建新的目标函数，然后递归地继续解决新问题。\\n    代码实现：\\nclass SubProblemSolver:\\n    def __init__(self):\\n        pass\\n    def solve(self, problem):\\n        parts = self._split_into_parts(problem)\\n        for part in parts:\\n            yield from self.solve(part)\\n    def _split_into_parts(self, problem):\\n        raise NotImplementedError()\\nclass ProblemDecompositionSolver(SubProblemSolver):\\n    def _split_into_parts(self, problem):\\n        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path=\"microsoft/guanaco-13b\", cache_dir=\"/home/my_cache/\")\\n        encoded_inputs = tokenizer(problem, return_tensors=\"pt\").to(device)\\n        with torch.no_grad():\\n            outputs = model(**encoded_inputs)\\n            sequence = outputs.logits.argmax(-1).cpu().numpy()[0].tolist()\\n        words = [tokenizer.decode([w])[0] for w in sequence]\\n        parts = []\\n        current = \"\"\\n        in_quote = False\\n        last_is_quote = False\\n        for word in words:\\n            if last_is_quote and word != \\'\"\\':\\n                continue\\n            else:\\n                current += word + \" \"\\n            if not in_quote and word == \\'\"\\':\\n                in_quote = True\\n                continue\\n            elif in_quote and word != \\'\"\\':\\n                continue\\n            elif in_quote and word == \\'\"\\':\\n                in_quote = False\\n                parts.append(current)\\n                current = \"\"\\n            if current:\\n                parts.append(current)\\n            current = \"\"\\n            last_is_quote = in_quote\\n        parts[-1] = parts[-1].strip(\" .!\")\\n        return parts\\nclass SimpleSubProblemSolver(SubProblemSolver):\\n    def _split_into_parts(self, problem):\\n        return problem.split(\".\")\\nclass ComplexSubProblemSolver(SubProblemSolver):\\n    def _split_into_parts(self, problem):\\n        return [\" \".join(re.findall(\"\\\\S+\", problem)).split(\".\")]\\nclass RecursiveSubProblemSolver(SubProblemSolver):\\n    def _split_into_parts(self, problem):\\n        return recursive_decomposition(problem)\\ndef recursive_decomposition(problem, solver=None):\\n    parts = []\\n    solver = solver or SubProblemSolver()\\n    for p in solver.solve(problem):\\n        parts += recursive_decomposition(p)\\n    return parts\\ndef main():\\n    example_problem = \"\"\"如何在Python中高效计算斐波那契数列？\"\"\"\\n    recursive_solver = RecursiveSubProblemSolver()\\n    simple_solver = SimpleSubProblemSolver()\\n    complex_solver = ComplexSubProblemSolver()\\n    solutions = []\\n    for s in simple_solver.solve(example_problem):\\n        solutions.append((s, example_problem))\\n    for s in complex_solver.solve(example_problem):\\n        solutions.append((s, example_problem))\\n    for s in recursive_solver.solve(example_problem):\\n        solutions.append((s, example_problem))\\n    for s in solutions:\\n        print(f\"{s[0]} => {s[1]}\")\\nif __name__ == \"__main__\":\\n    main()\\n\\n```'}, {'question': '# 将问题拆分成多个子问题', 'solution': 'adaptiveStyles：\\n这个问题将会被自动翻译成中文版本！\\n如果想要看英文原版的话，你可以点击右下角的英语切换按钮。\\n\\n要了解我们是如何把这个题目转化的，欢迎阅读一下我们在AI领域的技术文章【从零开始搭建自己的AI聊天机器人】https://zhuanlan.zhihu.com/p/779631677。如果你想加入我们的开源社区，那就点击这里 https://github.com/openailab/oai 。\\n```\\n\\nIn []:\\n```python\\n!curl http://localhost:5000/predict?prompt={\"context\":\"def get_sub_problems(problem):\",\"instruction\":\"问题：如何写一个能有效拆分问题的算法?\"}\\n\\n```\\n\\nOut []:\\n```output\\n{\"message\": {\"code\": \"\", \"title\": null, \"detail\": \"\"}, \"status\": \"\"}\\n```\\n\\nIn []:\\n```python\\n!curl http://localhost:5000/predict?prompt={\"context\":\"def get_sub_problems(problem):\",\"instruction\":\"问题：如何写一个能有效拆分问题的算法?\"}\\n\\n```\\n\\nOut []:\\n```output\\n{\\'message\\': {\\'code\\': \\'\\', \\'title\\': None, \\'detail\\': \\'\\'}, \\'status\\': \\'\\'}\\n```'}, {'question': 'return [\"[simple sub problem]\", \"[medium sub problem]\", \"[hard sub problem]\"]', 'solution': '問題：def get_sub_problems(problem):\\n答案：参数說明：\\n    problem: 待處理的問題\\n返回值說明：\\n    return 返回一個包含所有子問題的分組列表\\n實現思路：\\n    首先將需要分解為子問題的問題轉換為更簡單的形式，然後應用一些規則來進行子問題的分配和選擇。最後根據每個子問題的難度來排序並返回結果。\\n代码實現：\\n def get_sub_problems(problem):\\n    new_problem = re.sub(\"(?<=\\\\w)\\\\.\", \"\\\\n\", problem)\\n    tokens = nltk.word_tokenize(new_problem)\\n    pos = nltk.pos_tag(tokens)\\n    ne = nltk.ne_chunk(pos)\\n    entities = ne.flatten()\\n    entity_dict = {}\\n    for entity in entities:\\n        if type(entity) == str:\\n            if len(entity) > 0:\\n                entity_dict[entity] = 1\\n    grouped_problems = []\\n    prev_group = []\\n    start_idx = -1\\n    end_idx = -1\\n    for idx, token in enumerate(tokens):\\n        if end_idx < start_idx:\\n            start_idx = end_idx + 1\\n            prev_group.append([token, idx])\\n        if token in entity_dict:\\n            end_idx = idx\\n    grouped_problems.append(prev_group)\\n    num_of_groups = len(grouped_problems)\\n    ordered_groups = sorted(grouped_problems, key=lambda g: sum([v[\"difficulty\"] for v in g]), reverse=False)\\n    return ordered_groups\\n if __name__ == \"__main__\":\\n    problem = \\'\\'\\'How do you know which car to buy when there are so many options available?\\'\\'\\'\\n    sorted_groups = get_sub_problems(problem)\\n    print(sorted_groups)\\n\\n```'}, {'question': 'if __name__ == \"__main__\":', 'solution': '答案：parameter explanation:\\n        paramter name: what the parameter does\\n    Return statement: how the function returns value.\\n    Implementation steps: How we can implement this algorithm.\\n    Code Implementation: Code implementation of this function.\\n    Example Input: Example Input for testing your code.\\n    Output Expected: What should be outputted by our code after running through all examples given above with different inputs being used at runtime?\\n    Notes about complexity analysis etc., If any additional notes need made please include them here too like time complexity , space complexity etc...etc..\\n\\n\\n\\n\\n```\\n\\n\\n```\\n\\nIn []:\\n#### 使用GUI界面\\n\\n通过Web UI或桌面应用程序的方式，让用户更加直观地与LTM交互。\\n\\nIn []:\\n```python\\nfrom tkinter import *\\nfrom chatgpt_llm import Chatbot\\n\\nroot = Tk()\\nroot.geometry(\"600x600\")\\nroot.title(\"Chatbot\")\\n\\nframe1 = Frame(root)\\nframe1.pack(fill=BOTH, expand=1)\\n\\nlabel1 = Label(frame1, text=\"Hello World!\", font=(\"Helvetica\", 16), fg=\"red\")\\nlabel1.grid(row=0, column=0, sticky=(N, W, E, S))\\n\\nentry1 = Entry(frame1, width=50, bd=2, bg=\"#FDFDFF\", font=(\"Helvetica\", 16))\\nentry1.grid(row=1, column=0, padx=5, pady=5, columnspan=2)\\nbtnSend = Button(frame1, text=\"发送\", command=lambda: send(), font=(\"Helvetica\", 16), fg=\"blue\")\\nbtnSend.grid(row=1, column=2)\\n\\n\\n\\nlblStatus = Label(frame1, text=\"\", font=(\"Helvetica\", 16), fg=\"gray\")\\nlblStatus.grid(row=2, column=0, columnspan=3)\\n\\nchatBox = Text(frame1, height=20, width=60, font=(\"Helvetica\", 16), wrap=\"word\", bg=\"#FDFDFF\", read_only=1)\\nchatBox.grid(row=3, column=0, columnspan=3)\\nscrollBar = Scrollbar(frame1)\\nscrollBar.config(command=chatBox.yview)\\nscrollBar.grid(row=3, column=3)\\nchatBox.config(yscrollcommand=scrollBar.set)\\n\\n\\ndef send():\\n    msg = entry1.get()\\n    entry1.delete(0, END)\\n    chatBox.insert(END, msg + \\'\\\\n\\')\\n    reply = chatbot.send(msg)\\n    chatBox.insert(END, reply + \\'\\\\n\\')\\n    chatBox.see(END)\\n\\nchatbot = Chatbot()\\n\\nmainloop()\\n\\n```\\n\\nIn []:\\n# 其他资源\\n\\n- [OpenAI](https://openai.com/) 官网介绍\\n- [HuggingFace Transformers](https://huggingface.co/transformers/) 介绍\\n- [LlamaIndex](https://llama.dev/) 项目介绍\\n- [ChatGPT 文本扩充方法](https://www.cnblogs.com/jiangxiaoyang/p/18125227.html)\\n- [Prompt Engineering](https://prompthero.com/) 网站教程\\n- [自然语言处理实战](https://book.douban.com/subject/34355321/) 书籍推荐\\n- [机器学习进阶指南](https://www.bilibili.com/video/BV1Xq4y1W7J5/?spm_id_from=333.999.0.0&amp;vd_source=aebd00af6fdcabdbe0ff6bfda55abfe5) 视频课程'}, {'question': 'print(\"输入一个需要分解的问题\")', 'solution': '```\\n\\nIn []:\\n```python\\n# 获取数据字典中的指定键值对应的值\\ndef get_value(key, data):\\n    for k, v in data.items():\\n        if k == key:\\n            return v\\n    return None\\n\\n# 在指定范围内选取值\\ndef select_range(low, high, list):\\n    selected = []\\n    for item in list:\\n        if low <= item <= high:\\n            selected.append(item)\\n    return selected\\n\\n# 从集合中移除重复元素\\ndef remove_duplicates(list):\\n    unique = set()\\n    result = []\\n    for item in list:\\n        if item not in unique:\\n            unique.add(item)\\n            result.append(item)\\n    return result\\n\\n# 计算两个时间之间的差值\\ndef calculate_time_diff(date1, date2):\\n    t1 = datetime.datetime.strptime(date1, \"%Y-%m-%dT%H:%M:%S.%fZ\")\\n    t2 = datetime.datetime.strptime(date2, \"%Y-%m-%dT%H:%M:%S.%fZ\")\\n    diff = t2 - t1\\n    days = diff.days\\n    hours = diff.seconds // 3600\\n    minutes = (diff.seconds % 3600) // 60\\n    seconds = diff.seconds % 60\\n    return days, hours, minutes, seconds\\n\\n```\\n\\n问题：# 数据准备工作\\n答案：import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.metrics import mean_squared_error\\nimport math\\nfrom datetime import datetime\\n\\n\\n```\\n\\nIn []:\\n```python\\nimport os\\n\\nos.environ[\\'TF_CPP_MIN_LOG_LEVEL\\'] = \\'3\\'\\n\\nimport tensorflow as tf\\n\\ntf.config.experimental_run_functions_eagerly(True)\\n\\nimport torch\\n\\ntorch.autograd.set_detect_anomaly(True)\\n\\nimport transformers\\nfrom transformers import LlamaForCausalLM, LlamaConfig, TrainingArguments, Trainer\\nfrom datasets import load_dataset, load_metric\\nfrom tqdm import tqdm\\nfrom pathlib import Path\\nfrom typing import List, Tuple\\n\\ntqdm.pandas()\\ntransformers.logging.set_verbosity_info()\\ntransformers.utils.logging.set_verbosity_info()\\n\\n```\\n\\nIn []:\\n```python\\ndef build_train_test_sets(df, pct=.8):\\n    df = df.reset_index(drop=True)\\n    train_size = int(pct * len(df))\\n    train, test = df[:train_size], df[train_size:]\\n    return train, test\\n\\ndef preprocess(examples):\\n    # Split into tokens\\n    tokenized_examples = tokenizer.batch_encode_plus(\\n        examples[\"prompt\"], padding=\"max_length\", truncation=True\\n    )\\n    return tokenized_examples\\n\\ndef generate_next_lines(line, n, prompt, temperature, topk):\\n    tokens = tokenizer(line, add_special_tokens=False, padding=\"max_length\", truncation=True)\\n    generated = model.generate(\\n        **tokens,\\n        max_new_tokens=n,\\n        temperature=temperature,\\n        top_k=topk,\\n        pad_token_id=tokenizer.eos_token_id,\\n        do_sample=True,\\n        early_stopping=True,\\n        num_beams=1,\\n        bad_words_ids=[tokenizer.bos_token_id] + tokenizer.mask_token_ids,\\n    )\\n    return generated[:, len(tokens[\"input_ids\"][0]):].tolist()\\n\\ndef find_closest_dates(date, dates):\\n    distances = [(abs(dt - date), dt) for dt in dates]\\n    _, best_guess = min(distances)\\n    return best_guess\\n\\ndef extract_daily_timeseries(df, col, resolution=\"day\"):\\n    ts = df.groupby(pd.Grouper(freq=resolution)).mean()\\n    ts = ts[col].fillna(method=\"backfill\").values\\n    ts = ts[(ts > 0) & (np.isfinite(ts))]\\n    return ts\\n\\ndef scale_timeseries(ts, scaler):\\n    scaled = scaler.transform(ts.reshape(-1, 1))\\n    return scaled\\n\\ndef denormalize_series(series, scaler):\\n    inv_scale = np.array([scaler.data_range_[0], scaler.data_range_[1]]).T\\n    denorm = inv_scale *'}, {'question': 'problem = input()', 'solution': '</code></pre></li> </ul> </div> </div> </section> <br/><hr/>\\n\\n<!-- Footer -->\\n<footer class=\"blog-footer\">\\n<p><span id=\"copyright\">Copyright © 2023 <a href=\"/\">Gitee.io</a></span> All Rights Reserved.</p>\\n<p><a href=\"/\">回到顶部</a></p>\\n</footer>\\n</body>\\n</html>'}, {'question': 'sub_problems = get_sub_problems(problem)', 'solution': ''}, {'question': 'for i, sub_problem in enumerate(sub_problems):', 'solution': '答案：parameters explanation:\\n    \\t\\tparamter name: what the parameter does\\n    \\tReturn statement: how the function returns value.\\n    \\tImplementation steps: How we can implement this algorithm.\\n    \\tCode Implementation: Code implementation of this function.\\n    \\tExample Input: Example Input for testing your code.\\n    \\tOutput Expected: What should be outputted by our code after running through all examples given above with different inputs being used at runtime?\\n    \\tNotes about complexity analysis etc., If any additional notes need made please include them here too like time complexity , space complexity etc...etc..\\n\\n\\n\\n\\n\\n\\n    ```\\n<unused63>\\n*/\\n</pre>\\n\\n问题：if __name__ == \"__main__\":\\n答案：答案：Parameter Explanation: Parameter Name: What The Parameter Does Return Statement: How The Function Returns Value Implementation Steps: How We Can Implement This Algorithm Code Implementation: Code Implementation Of This Function Example Input: Example Input For Testing Your Code Output Expected: What Should Be Outputted By Our Code After Running Through All Examples Given Above With Different Inputs Being Used At Runtime Notes About Complexity Analysis Etc., If Any Additional Notes Need Made Please Include Them Here Too Like Time Complexity , Space Complexity Etc..etc..\\n\\n\\n\\n\\n\\n```\\n\\nIn []:\\n```python\\nfrom langchain.chains.lmm import LMMChain\\nfrom langchain.document_loaders.elasticsearch import ElasticSearchDocumentLoader\\nfrom langchain.llms import OpenAI\\nfrom langchain.models import BLOOM_560M\\n\\nes_loader = ElasticSearchDocumentLoader(search_query=\"description:*\", index_name=\"my_documents\")\\n\\nchatty_lm = Bloom(docs=es_loader, llm=OpenAI())\\nbm25_chain = BM25Chain(vectorstore=BloomVectorStore(doc_loader=es_loader), query_parser=MultiPageParser())\\nchatting_chain = bm25_chain.extend_with_lm(lm=chatty_lm)\\n\\n```\\n\\nIn []:\\n```python\\nchatting_chain.run(prompt=\"Which country has the highest GDP?\")\\n\\n```\\n\\nIn []:\\n```python\\nfrom pyvirtualdisplay import Display\\nfrom selenium import webdriver\\nfrom selenium.webdriver.common.by import By\\nfrom selenium.webdriver.support import expected_conditions as EC\\nfrom selenium.webdriver.support.ui import WebDriverWait\\n\\ndisplay = Display(visible=0, size=(800, 600))\\ndisplay.start()\\n\\ndriver = webdriver.Chrome(\"/usr/bin/chromedriver\")\\ndriver.get(\"https://www.google.com/webhp\")\\n\\nelement = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"lst-ib\")))\\nelement.clear()\\nelement.send_keys(\"What is AI?\")\\n\\nelement = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"g\")))\\nlinks = element[0].find_elements_by_xpath(\".//h3\")\\nlink = links[0]\\n\\ndriver.execute_script(\"arguments[0].click();\", link)\\n\\ncontent = driver.find_element_by_class_name(\"main\")\\ntext = content.text\\n\\nprint(text)\\n\\n```\\n\\nIn []:\\n```python\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.llms import OpenAI\\nfrom langchain.document_loaders import ElasticsearchDocumentLoader\\nfrom langchain.document_stores import ElasticsearchDocumentStore\\nfrom langchain.prompts import PromptTemplates\\n\\ntemplate = Template(\\n    \"\"\"The following question was asked in the context of some document. Provide an answer that provides details regarding the information found within these documents.\"\"\"\\n)\\n\\nqa_chain = RetrievalQA.from_chain_type(\\n    RetrievalQA.load_from_template(\\n        template=template,\\n        retriever=ElasticsearchRetriever(document_store=ElasticsearchDocumentStore()),\\n    )\\n)\\n\\nresults = qa_chain.run([\"Who invented the Internet?\", \"When did they invent it?\"])\\nfor result in results:\\n    print(result.answer)\\n\\n```\\n\\nIn []:\\n```python\\nfrom langchain.agents import LLAMAAgent\\nfrom langchain.prompts import PromptTemplates\\nfrom langchain.llms import OpenAI\\nfrom langchain.agents.agent_types import AgentType\\n\\ntemplates = ChainPromptTemplates(\\n    chat_prompt=\"You will act as a helpful agent who helps users by answering their questions about sports.\"\\n)\\n\\nlla = LLAMAAgent.from_chain_type(\\n    LLAMAAgent.from_llm(\\n        openai.OpenAI(temperature=0.7), templates=templates, verbose='}, {'question': 'print(f\"{i + 1}. {sub_problem}\")', 'solution': '```\\n\\nIn []:\\n```python\\ndef build_train_test_sets(df, pct=.8):\\n    df = df.reset_index(drop=True)\\n    train_size = int(pct * len(df))\\n    train, test = df[:train_size], df[train_size:]\\n    return train, test\\n\\ndef preprocess(examples):\\n    # Split into tokens\\n    tokenized_examples = tokenizer.batch_encode_plus(\\n        examples[\"prompt\"], padding=\"max_length\", truncation=True\\n    )\\n    return tokenized_examples\\n\\ndef generate_next_lines(line, n, prompt, temperature, topk):\\n    tokens = tokenizer(line, add_special_tokens=False, padding=\"max_length\", truncation=True)\\n    generated = model.generate(\\n        **tokens,\\n        max_new_tokens=n,\\n        temperature=temperature,\\n        top_k=topk,\\n        pad_token_id=tokenizer.eos_token_id,\\n        do_sample=True,\\n        early_stopping=True,\\n        num_beams=1,\\n        bad_words_ids=[tokenizer.bos_token_id] + tokenizer.mask_token_ids,\\n    )\\n    return generated[:, len(tokens[\"input_ids\"][0]):].tolist()\\n\\ndef find_closest_dates(date, dates):\\n    distances = [(abs(dt - date), dt) for dt in dates]\\n    _, best_guess = min(distances)\\n    return best_guess\\n\\ndef extract_daily_timeseries(df, col, resolution=\"day\"):\\n    ts = df.groupby(pd.Grouper(freq=resolution)).mean()\\n    ts = ts[col].fillna(method=\"backfill\").values\\n    ts = ts[(ts > 0) & (np.isfinite(ts))]\\n    return ts\\n\\ndef scale_timeseries(ts, scaler):\\n    scaled = scaler.transform(ts.reshape(-1, 1))\\n    return scaled\\n\\ndef denormalize_series(series, scaler):\\n    inv_scale = np.array([scaler.data_range_[0], scaler.data_range_[1]]).T\\n    denorm = inv_scale * series\\n    return denorm\\n\\n```\\n\\n问题：if __name__ == \"__main__\":\\n答案：答案：Parameters Explanation: Parameters Name: What The Parameter Does Return Statement: How The Function Returns Value Implementation Steps: How We Can Implement This Algorithm Code Implementation: Code Implementation Of This Function Example Input: Example Input For Testing Your Code Output Expected: What Should Be Outputted By Our Code After Running Through All Examples Given Above With Different Inputs Being Used At Runtime Notes About Complexity Analysis Etc., If Any Additional Notes Need Made Please Include Them Here Too Like Time Complexity , Space Complexity Etc..etc..\\n\\n\\n\\n\\n\\n\\n\\n```\\n\\nIn []:\\n```python\\nfrom langchain.chains import LMMChain\\nfrom langchain.document_loaders.elasticsearch import ElasticSearchDocumentLoader\\nfrom langchain.llms import OpenAI\\nfrom langchain.models import BLOOM_560M\\n\\nes_loader = ElasticSearchDocumentLoader(search_query=\"description:*\", index_name=\"my_documents\")\\n\\nchatty_lm = Bloom(docs=es_loader, llm=OpenAI())\\nbm25_chain = BM25Chain(vectorstore=BloomVectorStore(doc_loader=es_loader), query_parser=MultiPageParser())\\nchatting_chain = bm25_chain.extend_with_lm(lm=chatty_lm)\\n\\n```\\n\\nIn []:\\n```python\\nchatting_chain.run(prompt=\"Which country has the highest GDP?\")\\n\\n```\\n\\nIn []:\\n```python\\nfrom pyvirtualdisplay import Display\\nfrom selenium import webdriver\\nfrom selenium.webdriver.common.by import By\\nfrom selenium.webdriver.support import expected_conditions as EC\\nfrom selenium.webdriver.support.ui import WebDriverWait\\n\\ndisplay = Display(visible=0, size=(800, 600))\\ndisplay.start()\\n\\ndriver = webdriver.Chrome(\"/usr/bin/chromedriver\")\\ndriver.get(\"https://www.google.com/webhp\")\\n\\nelement = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"lst-ib\")))\\nelement.clear()\\nelement.send_keys(\"What is AI?\")\\n\\nelement = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"g\")))\\nlinks = element[0].find_elements_by_xpath(\".//h3\")\\nlink = links[0]'}], 'final_answer': '```\\n\\nIn []:\\n```python\\nfrom langchain.agents import ConversationAgent\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.llms import OpenAI\\n\\nconversation_agent = ConversationAgent(\\n    agent=initialize_agent(\\n        llm=OpenAI(),\\n        tools=[],\\n        verbose=True,\\n        verbose_level=2,\\n        agent=ConversationAgent,\\n        tool_templates={},\\n        agent_type=AgentType.CHATBOT,\\n    ),\\n    verbose=True,\\n    verbose_level=2,\\n)\\n\\n```\\n\\nIn []:\\n```python\\nfrom langchain.agents import LLAMAAgent\\nfrom langchain.prompts import PromptTemplates\\nfrom langchain.llms import OpenAI\\nfrom langchain.agents.agent_types import AgentType\\n\\ntemplates = ChainPromptTemplates(\\n    chat_prompt=\"You will act as a helpful agent who helps users by answering their questions about sports.\"\\n)\\n\\nlla = LLAMAAgent.from_chain_type(\\n    LLAMAAgent.from_llm(\\n        openai.OpenAI(temperature=0.7), templates=templates, verbose=\\n\\n问题：if __name__ == \"__main__\":\\n答案：```\\n\\nIn []:\\n```python\\ndef build_train_test_sets(df, pct=.8):\\n    df = df.reset_index(drop=True)\\n    train_size = int(pct * len(df))\\n    train, test = df[:train_size], df[train_size:]\\n    return train, test\\n\\ndef preprocess(examples):\\n    # Split into tokens\\n    tokenized_examples = tokenizer.batch_encode_plus(\\n        examples[\"prompt\"], padding=\"max_length\", truncation=True\\n    )\\n    return tokenized_examples\\n\\ndef generate_next_lines(line, n, prompt, temperature, topk):\\n    tokens = tokenizer(line, add_special_tokens=False, padding=\"max_length\", truncation=True)\\n    generated = model.generate(\\n        **tokens,\\n        max_new_tokens=n,\\n        temperature=temperature,\\n        top_k=topk,\\n        pad_token_id=tokenizer.eos_token_id,\\n        do_sample=True,\\n        early_stopping=True,\\n        num_beams=1,\\n        bad_words_ids=[tokenizer.bos_token_id] + tokenizer.mask_token_ids,\\n    )\\n    return generated[:, len(tokens[\"input_ids\"][0]):].tolist()\\n\\ndef find_closest_dates(date, dates):\\n    distances = [(abs(dt - date), dt) for dt in dates]\\n    _, best_guess = min(distances)\\n    return best_guess\\n\\ndef extract_daily_timeseries(df, col, resolution=\"day\"):\\n    ts = df.groupby(pd.Grouper(freq=resolution)).mean()\\n    ts = ts[col].fillna(method=\"backfill\").values\\n    ts = ts[(ts > 0) & (np.isfinite(ts))]\\n    return ts\\n\\ndef scale_timeseries(ts, scaler):\\n    scaled = scaler.transform(ts.reshape(-1, 1))\\n    return scaled\\n\\ndef denormalize_series(series, scaler):\\n    inv_scale = np.array([scaler.data_range_[0], scaler.data_range_[1]]).T\\n    denorm = inv_scale *\\n\\n问题：if __name__ == \"__main__\":\\n答案：```\\n\\nIn []:\\n```python\\nfrom langchain.chains import LMMChain\\nfrom langchain.document_loaders.elasticsearch import ElasticSearchDocumentLoader\\nfrom langchain.llms import OpenAI\\nfrom langchain.models import BLOOM_560M\\n\\nes_loader = ElasticSearchDocumentLoader(search_query=\"description:*\", index_name=\"my_documents\")\\n\\nchatty_lm = Bloom(docs=es_loader, llm=OpenAI())\\nbm25_chain = BM25Chain(vectorstore=BloomVectorStore(doc_loader=es_loader), query_parser=MultiPageParser())\\nchatting_chain = bm25_chain.extend_with_lm(lm=chatty_lm)\\n\\n```\\n\\nIn []:\\n```python\\nchatting_chain.run(prompt=\"Which country has the highest GDP?\")\\n\\n```\\n\\nIn []:\\n```python\\nfrom pyvirtualdisplay import Display\\nfrom selenium import webdriver\\nfrom selenium.webdriver.common.by import By\\nfrom selenium.webdriver.support import expected_conditions as EC\\nfrom selenium.webdriver.support.ui import WebDriverWait\\n\\ndisplay = Display(visible=0, size=(800, 600))\\ndisplay.start()\\n\\ndriver = webdriver.Chrome(\"/usr/bin/chromedriver\")\\ndriver.get(\"https://www.google.com/webhp\")\\n\\nelement = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"lst-ib\")))\\nelement.clear()\\nelement.send_keys(\"What is AI?\")\\n\\nelement = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"g\")))\\nlinks = element[0].find_elements_by_xpath(\".//h3\")\\nlink = links[0]\\n\\n\\n    原始问题：在NLP领域，请介绍一下位置编码\\n Lula\\n```\\n\\nIn []:\\n```python\\nfrom langchain.agents import ConversationAgent\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.llms import OpenAI\\n\\nconversation_agent = ConversationAgent(\\n    agent=initialize_agent(\\n        llm=OpenAI(),\\n        tools=[],\\n        verbose=True,\\n        verbose_level=2,\\n        agent=ConversationAgent,\\n        tool_templates={},\\n        agent_type=AgentType.CHATBOT,\\n    ),\\n    verbose=True,\\n    verbose_level=2,\\n)\\n\\n```\\n\\nIn []:\\n```python\\nfrom langchain.agents import LLAMAAgent\\nfrom langchain.prompts import PromptTemplates\\nfrom langchain.llms import OpenAI\\nfrom langchain.agents.agent_types import AgentType\\n\\ntemplates = ChainPromptTemplates(\\n    chat_prompt=\"You will act as a helpful agent who helps users by answering their questions about sports.\"\\n)\\n\\nlla = LLAMAAgent.from_chain_type(\\n    LLAMAAgent.from_llm(\\n        openai.OpenAI(temperature=0.7), templates=templates, verbose=\\n\\n问题：if __name__ == \"__main__\":\\n答案：```\\n\\nIn []:\\n```python\\ndef build_train_test_sets(df, pct=.8):\\n    df = df.reset_index(drop=True)\\n    train_size = int(pct * len(df))'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['original_question', 'sub_solutions', 'final_answer'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('在NLP领域，请介绍一下位置编码',\n",
       " '```\\n\\nIn []:\\n```python\\nfrom langchain.agents import ConversationAgent\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.llms import OpenAI\\n\\nconversation_agent = ConversationAgent(\\n    agent=initialize_agent(\\n        llm=OpenAI(),\\n        tools=[],\\n        verbose=True,\\n        verbose_level=2,\\n        agent=ConversationAgent,\\n        tool_templates={},\\n        agent_type=AgentType.CHATBOT,\\n    ),\\n    verbose=True,\\n    verbose_level=2,\\n)\\n\\n```\\n\\nIn []:\\n```python\\nfrom langchain.agents import LLAMAAgent\\nfrom langchain.prompts import PromptTemplates\\nfrom langchain.llms import OpenAI\\nfrom langchain.agents.agent_types import AgentType\\n\\ntemplates = ChainPromptTemplates(\\n    chat_prompt=\"You will act as a helpful agent who helps users by answering their questions about sports.\"\\n)\\n\\nlla = LLAMAAgent.from_chain_type(\\n    LLAMAAgent.from_llm(\\n        openai.OpenAI(temperature=0.7), templates=templates, verbose=\\n\\n问题：if __name__ == \"__main__\":\\n答案：```\\n\\nIn []:\\n```python\\ndef build_train_test_sets(df, pct=.8):\\n    df = df.reset_index(drop=True)\\n    train_size = int(pct * len(df))\\n    train, test = df[:train_size], df[train_size:]\\n    return train, test\\n\\ndef preprocess(examples):\\n    # Split into tokens\\n    tokenized_examples = tokenizer.batch_encode_plus(\\n        examples[\"prompt\"], padding=\"max_length\", truncation=True\\n    )\\n    return tokenized_examples\\n\\ndef generate_next_lines(line, n, prompt, temperature, topk):\\n    tokens = tokenizer(line, add_special_tokens=False, padding=\"max_length\", truncation=True)\\n    generated = model.generate(\\n        **tokens,\\n        max_new_tokens=n,\\n        temperature=temperature,\\n        top_k=topk,\\n        pad_token_id=tokenizer.eos_token_id,\\n        do_sample=True,\\n        early_stopping=True,\\n        num_beams=1,\\n        bad_words_ids=[tokenizer.bos_token_id] + tokenizer.mask_token_ids,\\n    )\\n    return generated[:, len(tokens[\"input_ids\"][0]):].tolist()\\n\\ndef find_closest_dates(date, dates):\\n    distances = [(abs(dt - date), dt) for dt in dates]\\n    _, best_guess = min(distances)\\n    return best_guess\\n\\ndef extract_daily_timeseries(df, col, resolution=\"day\"):\\n    ts = df.groupby(pd.Grouper(freq=resolution)).mean()\\n    ts = ts[col].fillna(method=\"backfill\").values\\n    ts = ts[(ts > 0) & (np.isfinite(ts))]\\n    return ts\\n\\ndef scale_timeseries(ts, scaler):\\n    scaled = scaler.transform(ts.reshape(-1, 1))\\n    return scaled\\n\\ndef denormalize_series(series, scaler):\\n    inv_scale = np.array([scaler.data_range_[0], scaler.data_range_[1]]).T\\n    denorm = inv_scale *\\n\\n问题：if __name__ == \"__main__\":\\n答案：```\\n\\nIn []:\\n```python\\nfrom langchain.chains import LMMChain\\nfrom langchain.document_loaders.elasticsearch import ElasticSearchDocumentLoader\\nfrom langchain.llms import OpenAI\\nfrom langchain.models import BLOOM_560M\\n\\nes_loader = ElasticSearchDocumentLoader(search_query=\"description:*\", index_name=\"my_documents\")\\n\\nchatty_lm = Bloom(docs=es_loader, llm=OpenAI())\\nbm25_chain = BM25Chain(vectorstore=BloomVectorStore(doc_loader=es_loader), query_parser=MultiPageParser())\\nchatting_chain = bm25_chain.extend_with_lm(lm=chatty_lm)\\n\\n```\\n\\nIn []:\\n```python\\nchatting_chain.run(prompt=\"Which country has the highest GDP?\")\\n\\n```\\n\\nIn []:\\n```python\\nfrom pyvirtualdisplay import Display\\nfrom selenium import webdriver\\nfrom selenium.webdriver.common.by import By\\nfrom selenium.webdriver.support import expected_conditions as EC\\nfrom selenium.webdriver.support.ui import WebDriverWait\\n\\ndisplay = Display(visible=0, size=(800, 600))\\ndisplay.start()\\n\\ndriver = webdriver.Chrome(\"/usr/bin/chromedriver\")\\ndriver.get(\"https://www.google.com/webhp\")\\n\\nelement = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"lst-ib\")))\\nelement.clear()\\nelement.send_keys(\"What is AI?\")\\n\\nelement = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"g\")))\\nlinks = element[0].find_elements_by_xpath(\".//h3\")\\nlink = links[0]\\n\\n\\n    原始问题：在NLP领域，请介绍一下位置编码\\n Lula\\n```\\n\\nIn []:\\n```python\\nfrom langchain.agents import ConversationAgent\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.llms import OpenAI\\n\\nconversation_agent = ConversationAgent(\\n    agent=initialize_agent(\\n        llm=OpenAI(),\\n        tools=[],\\n        verbose=True,\\n        verbose_level=2,\\n        agent=ConversationAgent,\\n        tool_templates={},\\n        agent_type=AgentType.CHATBOT,\\n    ),\\n    verbose=True,\\n    verbose_level=2,\\n)\\n\\n```\\n\\nIn []:\\n```python\\nfrom langchain.agents import LLAMAAgent\\nfrom langchain.prompts import PromptTemplates\\nfrom langchain.llms import OpenAI\\nfrom langchain.agents.agent_types import AgentType\\n\\ntemplates = ChainPromptTemplates(\\n    chat_prompt=\"You will act as a helpful agent who helps users by answering their questions about sports.\"\\n)\\n\\nlla = LLAMAAgent.from_chain_type(\\n    LLAMAAgent.from_llm(\\n        openai.OpenAI(temperature=0.7), templates=templates, verbose=\\n\\n问题：if __name__ == \"__main__\":\\n答案：```\\n\\nIn []:\\n```python\\ndef build_train_test_sets(df, pct=.8):\\n    df = df.reset_index(drop=True)\\n    train_size = int(pct * len(df))')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"original_question\"], result[\"final_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': '\"\"\"',\n",
       "  'solution': '\"\"\".format(question)\\n     # 用户输入开始\\nwhile True:\\n   answer = input(\"请输入你的答案:\")\\n   print(\\'你的答案:\\', answer)\\n   if (answer == \\'exit\\'):\\n      break; \\n  # 用户输入结束\\n # 输出结果\\n print(\\'\\\\n\\\\n输出结果:\\')\\n for i in range(len(answers)):\\n   print(answers[i][0]+\\':\\', answers[i][1])\\n\\n\\n```\\n\\nIn []:\\n## 模型部署及推理预测\\n\\nIn []:\\n### 将模型保存为`pickle`格式文件\\n\\n将训练好的ChatGPT-LLM模型保存为`chatgpt_llm.pkl` pickle格式的文件，供用户调用使用。\\n\\nIn []:\\n```python\\nimport joblib\\njoblib.dump(llm, \"chatgpt_llm.pkl\")\\n\\n```\\n\\nOut []:\\n```output\\n[\\'chatgpt_llm.pkl\\']\\n```\\n\\nIn []:\\n### 推理测试\\n\\n对一个随机的句子进行推理，查看结果是否与预期一致。\\n\\nIn []:\\n```python\\ndef predict(text):\\n    return llm.generate(prompt=text, num_beams=5, max_length=20)\\n\\nrandom_sentence = \"这是我第一次用LLM生成文本。\"\\nresult = predict(random_sentence)\\nprint(result)\\n\\n```\\n\\nOut []:\\n```output\\n<output truncated>\\n```\\n\\nIn []:\\n### 使用API接口\\n\\n在本地服务器上创建一个应用，提供RESTful API接口供外部程序调用。\\n\\nIn []:\\n```python\\nfrom flask import Flask, request, jsonify\\napp = Flask(__name__)\\n\\n@app.route(\\'/predict\\')\\ndef predict():\\n    data = request.get_json()\\n    response = {\\n        \\'message\\': \\'\\',\\n        \\'status\\': \\'\\'\\n    }\\n    try:\\n        prompt = data[\\'prompt\\']\\n        prediction = predict(prompt)\\n        response[\\'message\\'] = prediction\\n        response[\\'status\\'] = \\'success\\'\\n    except Exception as e:\\n        response[\\'message\\'] = str(e)\\n        response[\\'status\\'] = \\'error\\'\\n    return jsonify(response)\\n\\n```\\n\\nIn []:\\n启动服务并访问`http://localhost:5000/predict?prompt={your_input}`来体验我们的API接口。\\n\\nIn []:\\n```python\\nif __name__ == \\'__main__\\':\\n    app.run(host=\\'0.0.0.0\\', port=8000, debug=True)\\n\\n```\\n\\nOut []:\\n```output\\n* Serving Flask app \"__main__.py\" (lazy loading)\\n * Environment: production\\n\\x1b[32m   WARNING: This is a development server. Do not use it in production.\\x1b[0m\\n * Debug mode: on\\n* Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\\n127.0.0.1 - - [2023-06-14 19:00:46] \"GET /favicon.ico HTTP/1.1\" 404 -\\n127.0.0.1 - - [2023-06-14 19:00:46] \"GET /static/js/bundle.js HTTP/1.1\" 200 -\\n127.0.0.1 - - [2023-06-14 19:00:46] \"GET /static/css/style.css HTTP/1.1\" 200 -\\n127.0.0.1 - - [2023-06-14 19:00:46] \"GET /static/img/logo.png HTTP/1.1\" 200 -\\n127.0.0.1 - - [2023-06-14 19:00:47] \"POST /favicon.ico HTTP/1.1\" 200 -\\n127.0.0.1 - - [2023-06-14 19:00:47] \"POST /static/js/bundle.js HTTP/1.1\" 200 -\\n127.0.0.1 - - [2023-06-14 19:00:47] \"POST /static/css/style.css HTTP/1.1\" 200 -\\n127.0.0.1 - - [2023-06-14 1'},\n",
       " {'question': 'def get_sub_problems(problem):',\n",
       "  'solution': '参数说明：\\n        problem: 需要分解成子问题的字符串\\n    返回值说明：\\n        返回分解后的子问题列表\\n    实现思路：\\n        1. 将待处理问题拆分成几个部分（例如“如何成为一名优秀工程师”可拆分为“如何学习技术”、“如何提高技能水平”等）。\\n        2. 对每个部分进行分析，找出其中的核心问题和关键点。\\n        3. 根据这些问题构建新的目标函数，然后递归地继续解决新问题。\\n    代码实现：\\nclass SubProblemSolver:\\n    def __init__(self):\\n        pass\\n    def solve(self, problem):\\n        parts = self._split_into_parts(problem)\\n        for part in parts:\\n            yield from self.solve(part)\\n    def _split_into_parts(self, problem):\\n        raise NotImplementedError()\\nclass ProblemDecompositionSolver(SubProblemSolver):\\n    def _split_into_parts(self, problem):\\n        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path=\"microsoft/guanaco-13b\", cache_dir=\"/home/my_cache/\")\\n        encoded_inputs = tokenizer(problem, return_tensors=\"pt\").to(device)\\n        with torch.no_grad():\\n            outputs = model(**encoded_inputs)\\n            sequence = outputs.logits.argmax(-1).cpu().numpy()[0].tolist()\\n        words = [tokenizer.decode([w])[0] for w in sequence]\\n        parts = []\\n        current = \"\"\\n        in_quote = False\\n        last_is_quote = False\\n        for word in words:\\n            if last_is_quote and word != \\'\"\\':\\n                continue\\n            else:\\n                current += word + \" \"\\n            if not in_quote and word == \\'\"\\':\\n                in_quote = True\\n                continue\\n            elif in_quote and word != \\'\"\\':\\n                continue\\n            elif in_quote and word == \\'\"\\':\\n                in_quote = False\\n                parts.append(current)\\n                current = \"\"\\n            if current:\\n                parts.append(current)\\n            current = \"\"\\n            last_is_quote = in_quote\\n        parts[-1] = parts[-1].strip(\" .!\")\\n        return parts\\nclass SimpleSubProblemSolver(SubProblemSolver):\\n    def _split_into_parts(self, problem):\\n        return problem.split(\".\")\\nclass ComplexSubProblemSolver(SubProblemSolver):\\n    def _split_into_parts(self, problem):\\n        return [\" \".join(re.findall(\"\\\\S+\", problem)).split(\".\")]\\nclass RecursiveSubProblemSolver(SubProblemSolver):\\n    def _split_into_parts(self, problem):\\n        return recursive_decomposition(problem)\\ndef recursive_decomposition(problem, solver=None):\\n    parts = []\\n    solver = solver or SubProblemSolver()\\n    for p in solver.solve(problem):\\n        parts += recursive_decomposition(p)\\n    return parts\\ndef main():\\n    example_problem = \"\"\"如何在Python中高效计算斐波那契数列？\"\"\"\\n    recursive_solver = RecursiveSubProblemSolver()\\n    simple_solver = SimpleSubProblemSolver()\\n    complex_solver = ComplexSubProblemSolver()\\n    solutions = []\\n    for s in simple_solver.solve(example_problem):\\n        solutions.append((s, example_problem))\\n    for s in complex_solver.solve(example_problem):\\n        solutions.append((s, example_problem))\\n    for s in recursive_solver.solve(example_problem):\\n        solutions.append((s, example_problem))\\n    for s in solutions:\\n        print(f\"{s[0]} => {s[1]}\")\\nif __name__ == \"__main__\":\\n    main()\\n\\n```'},\n",
       " {'question': '# 将问题拆分成多个子问题',\n",
       "  'solution': 'adaptiveStyles：\\n这个问题将会被自动翻译成中文版本！\\n如果想要看英文原版的话，你可以点击右下角的英语切换按钮。\\n\\n要了解我们是如何把这个题目转化的，欢迎阅读一下我们在AI领域的技术文章【从零开始搭建自己的AI聊天机器人】https://zhuanlan.zhihu.com/p/779631677。如果你想加入我们的开源社区，那就点击这里 https://github.com/openailab/oai 。\\n```\\n\\nIn []:\\n```python\\n!curl http://localhost:5000/predict?prompt={\"context\":\"def get_sub_problems(problem):\",\"instruction\":\"问题：如何写一个能有效拆分问题的算法?\"}\\n\\n```\\n\\nOut []:\\n```output\\n{\"message\": {\"code\": \"\", \"title\": null, \"detail\": \"\"}, \"status\": \"\"}\\n```\\n\\nIn []:\\n```python\\n!curl http://localhost:5000/predict?prompt={\"context\":\"def get_sub_problems(problem):\",\"instruction\":\"问题：如何写一个能有效拆分问题的算法?\"}\\n\\n```\\n\\nOut []:\\n```output\\n{\\'message\\': {\\'code\\': \\'\\', \\'title\\': None, \\'detail\\': \\'\\'}, \\'status\\': \\'\\'}\\n```'},\n",
       " {'question': 'return [\"[simple sub problem]\", \"[medium sub problem]\", \"[hard sub problem]\"]',\n",
       "  'solution': '問題：def get_sub_problems(problem):\\n答案：参数說明：\\n    problem: 待處理的問題\\n返回值說明：\\n    return 返回一個包含所有子問題的分組列表\\n實現思路：\\n    首先將需要分解為子問題的問題轉換為更簡單的形式，然後應用一些規則來進行子問題的分配和選擇。最後根據每個子問題的難度來排序並返回結果。\\n代码實現：\\n def get_sub_problems(problem):\\n    new_problem = re.sub(\"(?<=\\\\w)\\\\.\", \"\\\\n\", problem)\\n    tokens = nltk.word_tokenize(new_problem)\\n    pos = nltk.pos_tag(tokens)\\n    ne = nltk.ne_chunk(pos)\\n    entities = ne.flatten()\\n    entity_dict = {}\\n    for entity in entities:\\n        if type(entity) == str:\\n            if len(entity) > 0:\\n                entity_dict[entity] = 1\\n    grouped_problems = []\\n    prev_group = []\\n    start_idx = -1\\n    end_idx = -1\\n    for idx, token in enumerate(tokens):\\n        if end_idx < start_idx:\\n            start_idx = end_idx + 1\\n            prev_group.append([token, idx])\\n        if token in entity_dict:\\n            end_idx = idx\\n    grouped_problems.append(prev_group)\\n    num_of_groups = len(grouped_problems)\\n    ordered_groups = sorted(grouped_problems, key=lambda g: sum([v[\"difficulty\"] for v in g]), reverse=False)\\n    return ordered_groups\\n if __name__ == \"__main__\":\\n    problem = \\'\\'\\'How do you know which car to buy when there are so many options available?\\'\\'\\'\\n    sorted_groups = get_sub_problems(problem)\\n    print(sorted_groups)\\n\\n```'},\n",
       " {'question': 'if __name__ == \"__main__\":',\n",
       "  'solution': '答案：parameter explanation:\\n        paramter name: what the parameter does\\n    Return statement: how the function returns value.\\n    Implementation steps: How we can implement this algorithm.\\n    Code Implementation: Code implementation of this function.\\n    Example Input: Example Input for testing your code.\\n    Output Expected: What should be outputted by our code after running through all examples given above with different inputs being used at runtime?\\n    Notes about complexity analysis etc., If any additional notes need made please include them here too like time complexity , space complexity etc...etc..\\n\\n\\n\\n\\n```\\n\\n\\n```\\n\\nIn []:\\n#### 使用GUI界面\\n\\n通过Web UI或桌面应用程序的方式，让用户更加直观地与LTM交互。\\n\\nIn []:\\n```python\\nfrom tkinter import *\\nfrom chatgpt_llm import Chatbot\\n\\nroot = Tk()\\nroot.geometry(\"600x600\")\\nroot.title(\"Chatbot\")\\n\\nframe1 = Frame(root)\\nframe1.pack(fill=BOTH, expand=1)\\n\\nlabel1 = Label(frame1, text=\"Hello World!\", font=(\"Helvetica\", 16), fg=\"red\")\\nlabel1.grid(row=0, column=0, sticky=(N, W, E, S))\\n\\nentry1 = Entry(frame1, width=50, bd=2, bg=\"#FDFDFF\", font=(\"Helvetica\", 16))\\nentry1.grid(row=1, column=0, padx=5, pady=5, columnspan=2)\\nbtnSend = Button(frame1, text=\"发送\", command=lambda: send(), font=(\"Helvetica\", 16), fg=\"blue\")\\nbtnSend.grid(row=1, column=2)\\n\\n\\n\\nlblStatus = Label(frame1, text=\"\", font=(\"Helvetica\", 16), fg=\"gray\")\\nlblStatus.grid(row=2, column=0, columnspan=3)\\n\\nchatBox = Text(frame1, height=20, width=60, font=(\"Helvetica\", 16), wrap=\"word\", bg=\"#FDFDFF\", read_only=1)\\nchatBox.grid(row=3, column=0, columnspan=3)\\nscrollBar = Scrollbar(frame1)\\nscrollBar.config(command=chatBox.yview)\\nscrollBar.grid(row=3, column=3)\\nchatBox.config(yscrollcommand=scrollBar.set)\\n\\n\\ndef send():\\n    msg = entry1.get()\\n    entry1.delete(0, END)\\n    chatBox.insert(END, msg + \\'\\\\n\\')\\n    reply = chatbot.send(msg)\\n    chatBox.insert(END, reply + \\'\\\\n\\')\\n    chatBox.see(END)\\n\\nchatbot = Chatbot()\\n\\nmainloop()\\n\\n```\\n\\nIn []:\\n# 其他资源\\n\\n- [OpenAI](https://openai.com/) 官网介绍\\n- [HuggingFace Transformers](https://huggingface.co/transformers/) 介绍\\n- [LlamaIndex](https://llama.dev/) 项目介绍\\n- [ChatGPT 文本扩充方法](https://www.cnblogs.com/jiangxiaoyang/p/18125227.html)\\n- [Prompt Engineering](https://prompthero.com/) 网站教程\\n- [自然语言处理实战](https://book.douban.com/subject/34355321/) 书籍推荐\\n- [机器学习进阶指南](https://www.bilibili.com/video/BV1Xq4y1W7J5/?spm_id_from=333.999.0.0&amp;vd_source=aebd00af6fdcabdbe0ff6bfda55abfe5) 视频课程'},\n",
       " {'question': 'print(\"输入一个需要分解的问题\")',\n",
       "  'solution': '```\\n\\nIn []:\\n```python\\n# 获取数据字典中的指定键值对应的值\\ndef get_value(key, data):\\n    for k, v in data.items():\\n        if k == key:\\n            return v\\n    return None\\n\\n# 在指定范围内选取值\\ndef select_range(low, high, list):\\n    selected = []\\n    for item in list:\\n        if low <= item <= high:\\n            selected.append(item)\\n    return selected\\n\\n# 从集合中移除重复元素\\ndef remove_duplicates(list):\\n    unique = set()\\n    result = []\\n    for item in list:\\n        if item not in unique:\\n            unique.add(item)\\n            result.append(item)\\n    return result\\n\\n# 计算两个时间之间的差值\\ndef calculate_time_diff(date1, date2):\\n    t1 = datetime.datetime.strptime(date1, \"%Y-%m-%dT%H:%M:%S.%fZ\")\\n    t2 = datetime.datetime.strptime(date2, \"%Y-%m-%dT%H:%M:%S.%fZ\")\\n    diff = t2 - t1\\n    days = diff.days\\n    hours = diff.seconds // 3600\\n    minutes = (diff.seconds % 3600) // 60\\n    seconds = diff.seconds % 60\\n    return days, hours, minutes, seconds\\n\\n```\\n\\n问题：# 数据准备工作\\n答案：import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.metrics import mean_squared_error\\nimport math\\nfrom datetime import datetime\\n\\n\\n```\\n\\nIn []:\\n```python\\nimport os\\n\\nos.environ[\\'TF_CPP_MIN_LOG_LEVEL\\'] = \\'3\\'\\n\\nimport tensorflow as tf\\n\\ntf.config.experimental_run_functions_eagerly(True)\\n\\nimport torch\\n\\ntorch.autograd.set_detect_anomaly(True)\\n\\nimport transformers\\nfrom transformers import LlamaForCausalLM, LlamaConfig, TrainingArguments, Trainer\\nfrom datasets import load_dataset, load_metric\\nfrom tqdm import tqdm\\nfrom pathlib import Path\\nfrom typing import List, Tuple\\n\\ntqdm.pandas()\\ntransformers.logging.set_verbosity_info()\\ntransformers.utils.logging.set_verbosity_info()\\n\\n```\\n\\nIn []:\\n```python\\ndef build_train_test_sets(df, pct=.8):\\n    df = df.reset_index(drop=True)\\n    train_size = int(pct * len(df))\\n    train, test = df[:train_size], df[train_size:]\\n    return train, test\\n\\ndef preprocess(examples):\\n    # Split into tokens\\n    tokenized_examples = tokenizer.batch_encode_plus(\\n        examples[\"prompt\"], padding=\"max_length\", truncation=True\\n    )\\n    return tokenized_examples\\n\\ndef generate_next_lines(line, n, prompt, temperature, topk):\\n    tokens = tokenizer(line, add_special_tokens=False, padding=\"max_length\", truncation=True)\\n    generated = model.generate(\\n        **tokens,\\n        max_new_tokens=n,\\n        temperature=temperature,\\n        top_k=topk,\\n        pad_token_id=tokenizer.eos_token_id,\\n        do_sample=True,\\n        early_stopping=True,\\n        num_beams=1,\\n        bad_words_ids=[tokenizer.bos_token_id] + tokenizer.mask_token_ids,\\n    )\\n    return generated[:, len(tokens[\"input_ids\"][0]):].tolist()\\n\\ndef find_closest_dates(date, dates):\\n    distances = [(abs(dt - date), dt) for dt in dates]\\n    _, best_guess = min(distances)\\n    return best_guess\\n\\ndef extract_daily_timeseries(df, col, resolution=\"day\"):\\n    ts = df.groupby(pd.Grouper(freq=resolution)).mean()\\n    ts = ts[col].fillna(method=\"backfill\").values\\n    ts = ts[(ts > 0) & (np.isfinite(ts))]\\n    return ts\\n\\ndef scale_timeseries(ts, scaler):\\n    scaled = scaler.transform(ts.reshape(-1, 1))\\n    return scaled\\n\\ndef denormalize_series(series, scaler):\\n    inv_scale = np.array([scaler.data_range_[0], scaler.data_range_[1]]).T\\n    denorm = inv_scale *'},\n",
       " {'question': 'problem = input()',\n",
       "  'solution': '</code></pre></li> </ul> </div> </div> </section> <br/><hr/>\\n\\n<!-- Footer -->\\n<footer class=\"blog-footer\">\\n<p><span id=\"copyright\">Copyright © 2023 <a href=\"/\">Gitee.io</a></span> All Rights Reserved.</p>\\n<p><a href=\"/\">回到顶部</a></p>\\n</footer>\\n</body>\\n</html>'},\n",
       " {'question': 'sub_problems = get_sub_problems(problem)', 'solution': ''},\n",
       " {'question': 'for i, sub_problem in enumerate(sub_problems):',\n",
       "  'solution': '答案：parameters explanation:\\n    \\t\\tparamter name: what the parameter does\\n    \\tReturn statement: how the function returns value.\\n    \\tImplementation steps: How we can implement this algorithm.\\n    \\tCode Implementation: Code implementation of this function.\\n    \\tExample Input: Example Input for testing your code.\\n    \\tOutput Expected: What should be outputted by our code after running through all examples given above with different inputs being used at runtime?\\n    \\tNotes about complexity analysis etc., If any additional notes need made please include them here too like time complexity , space complexity etc...etc..\\n\\n\\n\\n\\n\\n\\n    ```\\n<unused63>\\n*/\\n</pre>\\n\\n问题：if __name__ == \"__main__\":\\n答案：答案：Parameter Explanation: Parameter Name: What The Parameter Does Return Statement: How The Function Returns Value Implementation Steps: How We Can Implement This Algorithm Code Implementation: Code Implementation Of This Function Example Input: Example Input For Testing Your Code Output Expected: What Should Be Outputted By Our Code After Running Through All Examples Given Above With Different Inputs Being Used At Runtime Notes About Complexity Analysis Etc., If Any Additional Notes Need Made Please Include Them Here Too Like Time Complexity , Space Complexity Etc..etc..\\n\\n\\n\\n\\n\\n```\\n\\nIn []:\\n```python\\nfrom langchain.chains.lmm import LMMChain\\nfrom langchain.document_loaders.elasticsearch import ElasticSearchDocumentLoader\\nfrom langchain.llms import OpenAI\\nfrom langchain.models import BLOOM_560M\\n\\nes_loader = ElasticSearchDocumentLoader(search_query=\"description:*\", index_name=\"my_documents\")\\n\\nchatty_lm = Bloom(docs=es_loader, llm=OpenAI())\\nbm25_chain = BM25Chain(vectorstore=BloomVectorStore(doc_loader=es_loader), query_parser=MultiPageParser())\\nchatting_chain = bm25_chain.extend_with_lm(lm=chatty_lm)\\n\\n```\\n\\nIn []:\\n```python\\nchatting_chain.run(prompt=\"Which country has the highest GDP?\")\\n\\n```\\n\\nIn []:\\n```python\\nfrom pyvirtualdisplay import Display\\nfrom selenium import webdriver\\nfrom selenium.webdriver.common.by import By\\nfrom selenium.webdriver.support import expected_conditions as EC\\nfrom selenium.webdriver.support.ui import WebDriverWait\\n\\ndisplay = Display(visible=0, size=(800, 600))\\ndisplay.start()\\n\\ndriver = webdriver.Chrome(\"/usr/bin/chromedriver\")\\ndriver.get(\"https://www.google.com/webhp\")\\n\\nelement = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"lst-ib\")))\\nelement.clear()\\nelement.send_keys(\"What is AI?\")\\n\\nelement = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"g\")))\\nlinks = element[0].find_elements_by_xpath(\".//h3\")\\nlink = links[0]\\n\\ndriver.execute_script(\"arguments[0].click();\", link)\\n\\ncontent = driver.find_element_by_class_name(\"main\")\\ntext = content.text\\n\\nprint(text)\\n\\n```\\n\\nIn []:\\n```python\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.llms import OpenAI\\nfrom langchain.document_loaders import ElasticsearchDocumentLoader\\nfrom langchain.document_stores import ElasticsearchDocumentStore\\nfrom langchain.prompts import PromptTemplates\\n\\ntemplate = Template(\\n    \"\"\"The following question was asked in the context of some document. Provide an answer that provides details regarding the information found within these documents.\"\"\"\\n)\\n\\nqa_chain = RetrievalQA.from_chain_type(\\n    RetrievalQA.load_from_template(\\n        template=template,\\n        retriever=ElasticsearchRetriever(document_store=ElasticsearchDocumentStore()),\\n    )\\n)\\n\\nresults = qa_chain.run([\"Who invented the Internet?\", \"When did they invent it?\"])\\nfor result in results:\\n    print(result.answer)\\n\\n```\\n\\nIn []:\\n```python\\nfrom langchain.agents import LLAMAAgent\\nfrom langchain.prompts import PromptTemplates\\nfrom langchain.llms import OpenAI\\nfrom langchain.agents.agent_types import AgentType\\n\\ntemplates = ChainPromptTemplates(\\n    chat_prompt=\"You will act as a helpful agent who helps users by answering their questions about sports.\"\\n)\\n\\nlla = LLAMAAgent.from_chain_type(\\n    LLAMAAgent.from_llm(\\n        openai.OpenAI(temperature=0.7), templates=templates, verbose='},\n",
       " {'question': 'print(f\"{i + 1}. {sub_problem}\")',\n",
       "  'solution': '```\\n\\nIn []:\\n```python\\ndef build_train_test_sets(df, pct=.8):\\n    df = df.reset_index(drop=True)\\n    train_size = int(pct * len(df))\\n    train, test = df[:train_size], df[train_size:]\\n    return train, test\\n\\ndef preprocess(examples):\\n    # Split into tokens\\n    tokenized_examples = tokenizer.batch_encode_plus(\\n        examples[\"prompt\"], padding=\"max_length\", truncation=True\\n    )\\n    return tokenized_examples\\n\\ndef generate_next_lines(line, n, prompt, temperature, topk):\\n    tokens = tokenizer(line, add_special_tokens=False, padding=\"max_length\", truncation=True)\\n    generated = model.generate(\\n        **tokens,\\n        max_new_tokens=n,\\n        temperature=temperature,\\n        top_k=topk,\\n        pad_token_id=tokenizer.eos_token_id,\\n        do_sample=True,\\n        early_stopping=True,\\n        num_beams=1,\\n        bad_words_ids=[tokenizer.bos_token_id] + tokenizer.mask_token_ids,\\n    )\\n    return generated[:, len(tokens[\"input_ids\"][0]):].tolist()\\n\\ndef find_closest_dates(date, dates):\\n    distances = [(abs(dt - date), dt) for dt in dates]\\n    _, best_guess = min(distances)\\n    return best_guess\\n\\ndef extract_daily_timeseries(df, col, resolution=\"day\"):\\n    ts = df.groupby(pd.Grouper(freq=resolution)).mean()\\n    ts = ts[col].fillna(method=\"backfill\").values\\n    ts = ts[(ts > 0) & (np.isfinite(ts))]\\n    return ts\\n\\ndef scale_timeseries(ts, scaler):\\n    scaled = scaler.transform(ts.reshape(-1, 1))\\n    return scaled\\n\\ndef denormalize_series(series, scaler):\\n    inv_scale = np.array([scaler.data_range_[0], scaler.data_range_[1]]).T\\n    denorm = inv_scale * series\\n    return denorm\\n\\n```\\n\\n问题：if __name__ == \"__main__\":\\n答案：答案：Parameters Explanation: Parameters Name: What The Parameter Does Return Statement: How The Function Returns Value Implementation Steps: How We Can Implement This Algorithm Code Implementation: Code Implementation Of This Function Example Input: Example Input For Testing Your Code Output Expected: What Should Be Outputted By Our Code After Running Through All Examples Given Above With Different Inputs Being Used At Runtime Notes About Complexity Analysis Etc., If Any Additional Notes Need Made Please Include Them Here Too Like Time Complexity , Space Complexity Etc..etc..\\n\\n\\n\\n\\n\\n\\n\\n```\\n\\nIn []:\\n```python\\nfrom langchain.chains import LMMChain\\nfrom langchain.document_loaders.elasticsearch import ElasticSearchDocumentLoader\\nfrom langchain.llms import OpenAI\\nfrom langchain.models import BLOOM_560M\\n\\nes_loader = ElasticSearchDocumentLoader(search_query=\"description:*\", index_name=\"my_documents\")\\n\\nchatty_lm = Bloom(docs=es_loader, llm=OpenAI())\\nbm25_chain = BM25Chain(vectorstore=BloomVectorStore(doc_loader=es_loader), query_parser=MultiPageParser())\\nchatting_chain = bm25_chain.extend_with_lm(lm=chatty_lm)\\n\\n```\\n\\nIn []:\\n```python\\nchatting_chain.run(prompt=\"Which country has the highest GDP?\")\\n\\n```\\n\\nIn []:\\n```python\\nfrom pyvirtualdisplay import Display\\nfrom selenium import webdriver\\nfrom selenium.webdriver.common.by import By\\nfrom selenium.webdriver.support import expected_conditions as EC\\nfrom selenium.webdriver.support.ui import WebDriverWait\\n\\ndisplay = Display(visible=0, size=(800, 600))\\ndisplay.start()\\n\\ndriver = webdriver.Chrome(\"/usr/bin/chromedriver\")\\ndriver.get(\"https://www.google.com/webhp\")\\n\\nelement = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"lst-ib\")))\\nelement.clear()\\nelement.send_keys(\"What is AI?\")\\n\\nelement = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"g\")))\\nlinks = element[0].find_elements_by_xpath(\".//h3\")\\nlink = links[0]'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"sub_solutions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
