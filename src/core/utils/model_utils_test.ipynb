{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到Python路径\n",
    "project_root = \"/home/cuipeng/Gemma\"\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# 现在可以正常导入src下的模块\n",
    "from src.core.model.model_initializer import initialize_model_and_tokenizer\n",
    "from src.core.utils.model_utils import generate_response, apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1191b4e7034988bef1ca856da3f00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 初始化模型和tokenizer\n",
    "model_path = \"google/gemma-2-9b\"\n",
    "cache_dir = \"/root/autodl-tmp/gemma\"\n",
    "# lora_path = \"/root/autodl-tmp/models/stage1/checkpoints/gemma-base-zh/checkpoint-20000\"\n",
    "lora_path = None\n",
    "\n",
    "model, tokenizer = initialize_model_and_tokenizer(\n",
    "    model_path=model_path,\n",
    "    cache_dir=cache_dir,\n",
    "    lora_path=lora_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "Hello!\n",
      "<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "Hello! How can I assist you today?\n",
      "<end_of_turn><eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dialogue = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "#     {\"role\": \"model\", \"content\": \"Hello! How can I assist you today?\"}\n",
    "# ]\n",
    "# dialogue_str =apply_chat_template(dialogue)\n",
    "# print(dialogue_str)\n",
    "\n",
    "dialogue = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"model\", \"content\": \"Hello! How can I assist you today?\"}\n",
    "]\n",
    "dialogue_str =apply_chat_template(dialogue)\n",
    "print(dialogue_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型回答：\n",
      " I am here to help you find the right answers and support for your questions or concerns.\n",
      "sendLogPayload\n",
      "Let's get started by telling me what I can do to assist you today.\n",
      "\n",
      "The system is not accepting any other options, no matter how many times i say \"i want to speak with someone\". Also it said that there was some issue in my account but when i asked if they could fix it, it told me something about being unable to process my request at this time.\n"
     ]
    }
   ],
   "source": [
    "dialogue = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "]\n",
    "\n",
    "dialogue_str =apply_chat_template(dialogue)\n",
    "# print(dialogue_str)\n",
    "\n",
    "response = generate_response(model, tokenizer, dialogue_str)\n",
    "print(\"模型回答：\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载tokenizer后\n",
    "print(tokenizer.chat_template)\n",
    "# 或者\n",
    "print(tokenizer.init_kwargs.get('chat_template'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设tokenizer已经加载\n",
    "config = tokenizer.init_kwargs\n",
    "print(config.keys(), \"\\n\\n\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成响应\n",
    "response = generate_response(model, tokenizer, \"你好，世界！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer([\"<start_of_turn>\", \"user\", \"model\", \"<end_of_turn>\", \"<eos>\", \"\\n\", \"system\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([1, 2, 106, 1645, 2516, 107, 108])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([106, 2516, 108])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inputs 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([2,  87139, 235365,   9979, 235482])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outputs 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([2, 87139, 235365, 9979, 235482, 108, 122079, 86446, 235366, 4521, 235269, 3855, 7567, 29799, 235362, 109, 41037, 8894, 29799, 235635, 235365, 107541, 24957, 104946, 37094, 18390, 12620, 235465, 108, 207, 4521, 235269, 3855, 235341, 216, 109, 18733, 27922, 14520, 5121, 56003, 235370, 68388, 29799, 235365, 35760, 40163, 235983, 236107, 103571, 25661, 136073, 48982, 235581, 19252, 151473, 235362, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([2, 87139, 235365, 9979, 235482, 108, 40290, 236393, 235745, 235362, 109, 15409, 62505, 143777, 235427, 50692, 235581, 98633, 235362, 235509, 59015, 235785, 17376, 39586, 33166, 235365, 69311, 42081, 19484, 235581, 71846, 235362, 24494, 235365, 50443, 13585, 19891, 25451, 34384, 152832, 235365, 75923, 31326, 235581, 65581, 236216, 152832, 235362, 109, 194816, 235971, 235444, 46645, 235365, 11875, 107177, 236548, 235759, 236122, 236095, 29255, 19162, 235365, 30697, 15409, 58692, 235581, 27865, 235362, 54433, 235735, 97613, 23748, 13240, 236132, 235787, 121557, 19600, 235928, 157359, 235365, 236343, 153149, 235362, 109, 49680, 17153, 38768, 235365, 19878, 7078, 23809, 27616, 235607, 19162, 235482, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([2, 87139, 235365, 9979, 235482, 108, 12023, 235292, 235248, 4521, 235269, 2134, 235341, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
