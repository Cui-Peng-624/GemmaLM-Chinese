{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig  # 导入 BitsAndBytesConfig\n",
    "\n",
    "# 清理 GPU 缓存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 配置4-bit量化参数\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # 这是指模型的权重存储使用4位精度，可以大大减少模型占用的显存\n",
    "    bnb_4bit_compute_dtype=torch.float16, # 这是指模型在计算时使用16位精度。计算：训练和推理\n",
    "    bnb_4bit_use_double_quant=True, # 这是指在量化过程中使用双量化，可以进一步减少量化误差\n",
    "    bnb_4bit_quant_type=\"nf4\"  # 使用 normal float 4 量化类型。量化：将高精度（如32位）的浮点数转换为低精度（如4位）的浮点数，以减少模型占用的显存和提高计算速度。\n",
    ")\n",
    "\n",
    "# 指定自定义下载路径\n",
    "cache_dir = \"/root/autodl-tmp/gemma\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-2-9b\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# 使用新的量化配置方式加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b\",\n",
    "    cache_dir=cache_dir,\n",
    "    device_map=\"auto\", # 自动管理模型在可用设备上的分配\n",
    "    torch_dtype=torch.float16, # 使用float16精度\n",
    "    quantization_config=quantization_config  # 使用量化配置\n",
    ")\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,  # 使用已加载的模型\n",
    "#     tokenizer=tokenizer,  # 使用已加载的tokenizer\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# 确定设备（CPU或GPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# tokenizer(\"你好\"), tokenizer(\"你好\", return_tensors=\"pt\"), tokenizer.eos_token_id, tokenizer.decode(tokenizer.eos_token_id)\n",
    "print(tokenizer(\"<start_of_turn>\"), tokenizer(\"user\"))\n",
    "print(tokenizer(\"<end_of_turn>\"), tokenizer(\"<eos>\"))\n",
    "print(tokenizer.decode(torch.tensor([2, 106])))\n",
    "print(tokenizer.decode(torch.tensor([2, 1645])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gemma(prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device) # 返回的inputs是字典，包含input_ids, attention_mask。return_tensors=\"pt\"返回的是tensor\n",
    "    \n",
    "    # 关于参数的设置请参考：https://huggingface.co/learn/cookbook/en/prompt_tuning_peft#inference-with-the-pre-trained-bloom-model\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature = 0.2,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.5, # 对重复内容施加惩罚\n",
    "        early_stopping=True, # The model can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # 解码并处理回答\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True) # inputs['input_ids'].size(1)是输入序列的长度\n",
    "    return response\n",
    "\n",
    "# 测试对话\n",
    "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}\\n<end_of_turn><eos>\\n\"\n",
    "MODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}\\n<end_of_turn><eos>\\n\"\n",
    "\n",
    "user_prompt= \"你好，请介绍一下自己。\"\n",
    "prompt = (\n",
    "    USER_CHAT_TEMPLATE.format(prompt=\"你是谁？\")\n",
    "    + MODEL_CHAT_TEMPLATE.format(prompt=\"我是Gemma，一个由Google开发的智能助手。\")\n",
    "    # 开启新的对话\n",
    "    + USER_CHAT_TEMPLATE.format(prompt=user_prompt)\n",
    "    + \"<start_of_turn>model\\n\" # 等待模型生成回答\n",
    ")\n",
    "print(prompt)\n",
    "response = chat_with_gemma(prompt)\n",
    "print(\"模型回答:\\n\", response)\n",
    "\n",
    "# 简化prompt构造\n",
    "# user_prompt = \"你好，请介绍一下自己。\"\n",
    "# prompt = f\"\"\"<start_of_turn>user\\n{user_prompt}<end_of_turn><eos>\\n\n",
    "#              <start_of_turn>model\\n\"\"\"\n",
    "\n",
    "# response = chat_with_gemma(prompt)\n",
    "# print(\"模型回答:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig  # 导入 BitsAndBytesConfig\n",
    "\n",
    "# 清理 GPU 缓存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 配置4-bit量化参数\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # 这是指模型的权重存储使用4位精度，可以大大减少模型占用的显存\n",
    "    bnb_4bit_compute_dtype=torch.float16, # 这是指模型在计算时使用16位精度。计算：训练和推理\n",
    "    bnb_4bit_use_double_quant=True, # 这是指在量化过程中使用双量化，可以进一步减少量化误差\n",
    "    bnb_4bit_quant_type=\"nf4\"  # 使用 normal float 4 量化类型。量化：将高精度（如32位）的浮点数转换为低精度（如4位）的浮点数，以减少模型占用的显存和提高计算速度。\n",
    ")\n",
    "\n",
    "# 指定自定义下载路径\n",
    "cache_dir = \"/root/autodl-tmp/gemma\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-2-9b\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# 使用新的量化配置方式加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b\",\n",
    "    cache_dir=cache_dir,\n",
    "    device_map=\"auto\", # 自动管理模型在可用设备上的分配\n",
    "    torch_dtype=torch.float16, # 使用float16精度\n",
    "    quantization_config=quantization_config  # 使用量化配置\n",
    ")\n",
    "\n",
    "# 确定设备（CPU或GPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gemma(prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device) # 返回的inputs是字典，包含input_ids, attention_mask。return_tensors=\"pt\"返回的是tensor\n",
    "    \n",
    "    # 关于参数的设置请参考：https://huggingface.co/learn/cookbook/en/prompt_tuning_peft#inference-with-the-pre-trained-bloom-model\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature = 0.2,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.5, # 对重复内容施加惩罚\n",
    "        early_stopping=True, # The model can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # 解码并处理回答\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True) # inputs['input_ids'].size(1)是输入序列的长度\n",
    "    return response\n",
    "\n",
    "# 测试对话\n",
    "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn><eos>\\n\"\n",
    "MODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}\\n<end_of_turn><eos>\\n\"\n",
    "user_prompt = \"你好，请介绍一下自己。\"\n",
    "prompt = USER_CHAT_TEMPLATE.format(prompt=user_prompt)\n",
    "\n",
    "# 单论对话\n",
    "response = chat_with_gemma(prompt)\n",
    "print(response)\n",
    "\n",
    "# 多轮对话\n",
    "# while True:\n",
    "#     user_prompt = input(\"你: \")\n",
    "#     prompt = USER_CHAT_TEMPLATE.format(prompt=user_prompt)\n",
    "#     response = chat_with_gemma(prompt)\n",
    "#     print(\"模型:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
