{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test Gemma from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup the environment\n",
    "# !pip install -q -U immutabledict sentencepiece \n",
    "# !git clone https://github.com/google/gemma_pytorch.git\n",
    "# !mkdir /kaggle/working/gemma/\n",
    "# !mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"gemma_pytorch-main\") \n",
    "from gemma.config import GemmaConfig, get_model_config\n",
    "from gemma.model import GemmaForCausalLM\n",
    "from gemma.tokenizer import Tokenizer\n",
    "import contextlib\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "VARIANT = \"9b\" \n",
    "MACHINE_TYPE = \"cuda\" \n",
    "weights_dir = '/root/autodl-tmp/gemma-2-pytorch-gemma-2-9b-pt-v1' \n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
    "  torch.set_default_dtype(dtype)\n",
    "  yield\n",
    "  torch.set_default_dtype(torch.float)\n",
    "\n",
    "model_config = get_model_config(VARIANT)\n",
    "model_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\n",
    "\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "with _set_default_tensor_type(model_config.get_dtype()):\n",
    "  model = GemmaForCausalLM(model_config)\n",
    "  model.load_weights(weights_dir)\n",
    "  model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['device', 'output_len'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m MODEL_CHAT_TEMPLATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<start_of_turn>model\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{prompt}\u001b[39;00m\u001b[38;5;124m<end_of_turn><eos>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      7\u001b[0m     USER_CHAT_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is a good place for travel in the US?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m+\u001b[39m MODEL_CHAT_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalifornia.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<start_of_turn>model\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# 等待模型生成回答\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     15\u001b[0m     USER_CHAT_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(prompt\u001b[38;5;241m=\u001b[39mprompt),\n\u001b[1;32m     16\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     17\u001b[0m     output_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/Gemma/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/Gemma/lib/python3.12/site-packages/transformers/generation/utils.py:2009\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2006\u001b[0m assistant_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n\u001b[1;32m   2008\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2009\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_model_kwargs(model_kwargs\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Gemma/lib/python3.12/site-packages/transformers/generation/utils.py:1388\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1389\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1390\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1391\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['device', 'output_len'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "# Use the model\n",
    "\n",
    "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn><eos>\\n\"\n",
    "MODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}<end_of_turn><eos>\\n\"\n",
    "\n",
    "prompt = (\n",
    "    USER_CHAT_TEMPLATE.format(prompt=\"What is a good place for travel in the US?\")\n",
    "    + MODEL_CHAT_TEMPLATE.format(prompt=\"California.\")\n",
    "    # 开启新的对话\n",
    "    + USER_CHAT_TEMPLATE.format(prompt=\"What can I do in California?\")\n",
    "    + \"<start_of_turn>model\\n\" # 等待模型生成回答\n",
    ")\n",
    "\n",
    "model.generate(\n",
    "    USER_CHAT_TEMPLATE.format(prompt=prompt),\n",
    "    device=device,\n",
    "    output_len=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 初始化Gemma tokenizer\n",
    "tokenizer_path = \"gemma_pytorch-main/tokenizer/tokenizer.model\"\n",
    "gemma_tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "# 初始化HuggingFace tokenizer\n",
    "# hf_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "\n",
    "# 示例使用方法\n",
    "def tokenize_text(text, use_hf=False):\n",
    "    if use_hf:\n",
    "        # 使用HuggingFace tokenizer\n",
    "        tokens = hf_tokenizer(text, return_tensors=\"pt\")\n",
    "        return tokens\n",
    "    else:\n",
    "        # 使用Gemma原生tokenizer\n",
    "        tokens = gemma_tokenizer.encode(text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_text(\"Hello, how are you?\", use_hf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoDL官方学术资源加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"SetFit/mrpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# 替换 YOUR_TOKEN 为你的 Hugging Face token\n",
    "# 可以从 https://huggingface.co/settings/tokens 获取\n",
    "login(token=\"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\", write_permission=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6c565a3d0148078cc1b3b6132ae23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import torch\n",
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# # 指定自定义下载路径\n",
    "# cache_dir = \"/root/autodl-tmp/gemma\"  # 替换为你想要的路径\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"google/gemma-2-9b\",\n",
    "#     cache_dir=cache_dir\n",
    "# )\n",
    "# # model = AutoModelForCausalLM.from_pretrained(\n",
    "# #     \"google/gemma-2-9b\",\n",
    "# #     cache_dir=cache_dir\n",
    "# # )\n",
    "# # 使用 4-bit 量化加载模型\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"google/gemma-2-9b\",\n",
    "#     cache_dir=cache_dir,\n",
    "#     device_map=\"auto\",\n",
    "#     load_in_4bit=True,  # 4-bit 量化可以将显存需求降至约 8GB\n",
    "#     torch_dtype=torch.float16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig  # 导入 BitsAndBytesConfig\n",
    "\n",
    "# 清理 GPU 缓存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 配置4-bit量化参数\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"  # 使用 normal float 4 量化类型\n",
    ")\n",
    "\n",
    "# 指定自定义下载路径\n",
    "cache_dir = \"/root/autodl-tmp/gemma\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-2-9b\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# 使用新的量化配置方式加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b\",\n",
    "    cache_dir=cache_dir,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config  # 使用量化配置\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型回答: user\n",
      "What can I do in California?\n",
      "model\n",
      "What can I do in California?\n",
      "What can I do in California?\n",
      "What can I do in California?\n",
      "What can I do in California?\n",
      "What can I do in California?\n",
      "What can I do in California?\n",
      "What can\n"
     ]
    }
   ],
   "source": [
    "def chat_with_gemma(prompt, max_length=256):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        # 添加以下参数来更好地控制生成\n",
    "        max_new_tokens=50,  # 限制新生成的token数量\n",
    "        eos_token_id=tokenizer.eos_token_id,  # 设置结束标记\n",
    "        early_stopping=True  # 启用早停\n",
    "    )\n",
    "    \n",
    "    # 只返回模型的回答部分\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # 如果回答中包含了用户输入，只保留模型回答部分\n",
    "    if \"<start_of_turn>user\" in response:\n",
    "        response = response.split(\"<start_of_turn>user\")[0]\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# 测试对话\n",
    "# prompt = \"\"\"<start_of_turn>user\n",
    "# 你好，请介绍一下自己。\n",
    "# <end_of_turn>\n",
    "# <start_of_turn>model\"\"\"\n",
    "\n",
    "response = chat_with_gemma(prompt)\n",
    "print(\"模型回答:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model\n",
    "\n",
    "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn><eos>\\n\"\n",
    "MODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}<end_of_turn><eos>\\n\"\n",
    "\n",
    "prompt = (\n",
    "    USER_CHAT_TEMPLATE.format(prompt=\"What is a good place for travel in the US?\")\n",
    "    + MODEL_CHAT_TEMPLATE.format(prompt=\"California.\")\n",
    "    # 开启新的对话\n",
    "    + USER_CHAT_TEMPLATE.format(prompt=\"What can I do in California?\")\n",
    "    + \"<start_of_turn>model\\n\" # 等待模型生成回答\n",
    ")\n",
    "\n",
    "# model.generate(\n",
    "#     USER_CHAT_TEMPLATE.format(prompt=prompt),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PromptEncoderConfig, get_peft_model\n",
    "\n",
    "# 将 GemmaConfig 转换为字典格式\n",
    "model_config_dict = {\n",
    "    \"num_hidden_layers\": model_config.num_hidden_layers,\n",
    "    \"hidden_size\": model_config.hidden_size,\n",
    "    \"num_attention_heads\": model_config.num_attention_heads\n",
    "}\n",
    "\n",
    "peft_config = PromptEncoderConfig(\n",
    "    task_type=\"CAUSAL_LM\",  # 改为因果语言模型任务\n",
    "    num_virtual_tokens=20,\n",
    "    encoder_hidden_size=model_config.hidden_size,  # 使用与基础模型相同的隐藏层大小\n",
    "    encoder_num_layers=2\n",
    ")\n",
    "\n",
    "# 使用转换后的配置\n",
    "model = get_peft_model(model, peft_config, model_config_dict)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
