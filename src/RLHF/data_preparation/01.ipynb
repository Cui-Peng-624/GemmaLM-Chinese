{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„\n",
    "project_root = \"/home/cuipeng/Gemma\"\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# å¯¼å…¥å¿…è¦æ¨¡å—\n",
    "from src.core.model.model_initializer import initialize_model_and_tokenizer\n",
    "from src.core.utils.model_utils import generate_response, apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "\n",
    "# åŠ è½½ .env æ–‡ä»¶\n",
    "load_dotenv()\n",
    "\n",
    "# è¯»å–\n",
    "ZetaTechs_api_key = os.getenv('ZETATECHS_API_KEY')\n",
    "ZetaTechs_api_base = os.getenv('ZETATECHS_API_BASE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. æ„å»ºbaseæ•°æ®é›† - æ‰€æœ‰valæ•°æ®é›†çš„é›†åˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ceval_val_datasets():\n",
    "    \"\"\"\n",
    "    å¤„ç†CEVALéªŒè¯é›†æ•°æ®,å°†æ‰€æœ‰CSVæ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ªJSONæ–‡ä»¶\n",
    "    \n",
    "    æ•°æ®ç›®å½•ç»“æ„:\n",
    "    src/RLHF/data_preparation/ceval-exam/val/*.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    # è®¾ç½®æ•°æ®æ ¹ç›®å½•\n",
    "    base_path = Path(\"ceval-exam/val\")\n",
    "    \n",
    "    # å­˜å‚¨æ‰€æœ‰å¤„ç†åçš„æ•°æ®\n",
    "    all_data = []\n",
    "    \n",
    "    # éå†æ‰€æœ‰CSVæ–‡ä»¶\n",
    "    for csv_file in base_path.glob(\"*.csv\"):\n",
    "        # è¯»å–CSVæ–‡ä»¶\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # è·å–æ•°æ®ç±»åˆ«(æ–‡ä»¶å)\n",
    "        category = csv_file.stem\n",
    "        \n",
    "        # å¤„ç†æ¯ä¸€è¡Œæ•°æ®\n",
    "        for _, row in df.iterrows():\n",
    "            item = {\n",
    "                \"category\": category,\n",
    "                \"question\": row[\"question\"],\n",
    "                \"A\": row[\"A\"],\n",
    "                \"B\": row[\"B\"],\n",
    "                \"C\": row[\"C\"],\n",
    "                \"D\": row[\"D\"],\n",
    "                \"answer\": row[\"answer\"]\n",
    "            }\n",
    "            all_data.append(item)\n",
    "    \n",
    "    # å°†æ•°æ®ä¿å­˜ä¸ºJSONæ–‡ä»¶\n",
    "    output_path = base_path.parent / \"merged_val_data.json\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"data\": all_data\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"å¤„ç†å®Œæˆ,å…±æ•´åˆäº†{len(all_data)}æ¡æ•°æ®\")\n",
    "    print(f\"æ•°æ®å·²ä¿å­˜è‡³: {output_path}\")\n",
    "    \n",
    "    # è¿”å›ä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n",
    "    categories = {}\n",
    "    for item in all_data:\n",
    "        categories[item[\"category\"]] = categories.get(item[\"category\"], 0) + 1\n",
    "        \n",
    "    print(\"\\nå„ç±»åˆ«æ•°æ®ç»Ÿè®¡:\")\n",
    "    for category, count in categories.items():\n",
    "        print(f\"{category}: {count}æ¡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_ceval_val_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"ceval-exam/merged_val_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "dataset = data[\"data\"]\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. å°† merged_val_data.json ä¸­æ·»åŠ æ•´åˆåçš„prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm # type: ignore\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prompts_to_dataset(dataset: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ä¸ºæ•°æ®é›†ä¸­çš„æ¯ä¸€é¡¹æ·»åŠ æ ¼å¼åŒ–çš„prompt\n",
    "    \n",
    "    Args:\n",
    "        dataset: åŸå§‹æ•°æ®é›†åˆ—è¡¨\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: æ·»åŠ äº†promptçš„æ•°æ®é›†\n",
    "    \"\"\"\n",
    "    system_context = \"\"\"ä½ æ˜¯ä¸€ä½é€»è¾‘æ¨ç†ä¸“å®¶ã€‚è¯·ä»…ç”¨ä¸€ä¸ªå­—æ¯(A/B/C/D)å›ç­”é—®é¢˜,ä¸éœ€è¦è§£é‡Šã€‚\"\"\"\n",
    "    \n",
    "    for item in dataset:\n",
    "        # æ„å»ºuser prompt\n",
    "        user_prompt = f\"\"\"é¢˜ç›®ï¼š{item['question']}\n",
    "\n",
    "A. {item['A']}\n",
    "B. {item['B']}\n",
    "C. {item['C']}\n",
    "D. {item['D']}\n",
    "\n",
    "è¯·ç›´æ¥å›ç­”é€‰é¡¹å­—æ¯ã€‚\"\"\"\n",
    "\n",
    "        # åˆ›å»ºdialogueåˆ—è¡¨\n",
    "        dialogue = [\n",
    "            {\"role\": \"system\", \"content\": system_context},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        \n",
    "        # ä½¿ç”¨apply_chat_templateæ ¼å¼åŒ–å¯¹è¯\n",
    "        formatted_prompt = apply_chat_template(dialogue)\n",
    "        \n",
    "        # å°†æ ¼å¼åŒ–åçš„promptæ·»åŠ åˆ°æ•°æ®é¡¹ä¸­\n",
    "        item[\"prompt\"] = formatted_prompt\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_merged_dataset():\n",
    "    \"\"\"\n",
    "    è¯»å–merged_val_data.json,æ·»åŠ promptåé‡æ–°ä¿å­˜\n",
    "    \"\"\"\n",
    "    # è¯»å–åŸå§‹æ•°æ®\n",
    "    with open(\"ceval-exam/merged_val_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # è·å–æ•°æ®åˆ—è¡¨\n",
    "    dataset = data[\"data\"]\n",
    "    \n",
    "    # æ·»åŠ prompts\n",
    "    dataset_with_prompts = add_prompts_to_dataset(dataset)\n",
    "    \n",
    "    # æ›´æ–°åŸå§‹æ•°æ®\n",
    "    data[\"data\"] = dataset_with_prompts\n",
    "    \n",
    "    # ä¿å­˜æ›´æ–°åçš„æ•°æ®\n",
    "    with open(\"ceval-exam/merged_val_data_with_prompts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"æ•°æ®æ›´æ–°å®Œæˆ,å·²ä¿å­˜åˆ°merged_val_data_with_prompts.json\")\n",
    "    print(f\"å…±å¤„ç†{len(dataset)}æ¡æ•°æ®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®æ›´æ–°å®Œæˆ,å·²ä¿å­˜åˆ°merged_val_data_with_prompts.json\n",
      "å…±å¤„ç†1346æ¡æ•°æ®\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    update_merged_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. è®©å¤§æ¨¡å‹é€ä¸ªå›ç­”é—®é¢˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm # type: ignore\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_model_responses(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    input_file: str = \"ceval-exam/merged_val_data_with_prompts.json\",\n",
    "    output_file: str = \"ceval-exam/merged_val_data_with_prompts_and_responses.json\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æ¨¡å‹å¤„ç†æ•°æ®é›†ä¸­çš„æ¯ä¸ªé—®é¢˜ï¼Œè®°å½•å›ç­”å’Œè¯„ä¼°ç»“æœ\n",
    "    \n",
    "    Args:\n",
    "        model: åŠ è½½çš„æ¨¡å‹å®ä¾‹\n",
    "        tokenizer: åŠ è½½çš„åˆ†è¯å™¨å®ä¾‹\n",
    "        input_file: è¾“å…¥çš„JSONæ–‡ä»¶è·¯å¾„\n",
    "        output_file: è¾“å‡ºçš„JSONæ–‡ä»¶è·¯å¾„\n",
    "    \"\"\"\n",
    "    # è¯»å–æ•°æ®\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    dataset = data[\"data\"]\n",
    "    \n",
    "    # ç”¨äºå­˜å‚¨æ‰€æœ‰å¤„ç†ç»“æœçš„å­—å…¸\n",
    "    results = {}\n",
    "    \n",
    "    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦\n",
    "    for idx, item in enumerate(tqdm(dataset, desc=\"å¤„ç†é—®é¢˜\")):\n",
    "        prompt = item[\"prompt\"]\n",
    "        \n",
    "        try:\n",
    "            # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”\n",
    "            full_response = generate_response(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                prompt=prompt,\n",
    "                temperature=0.2,  # é™ä½éšæœºæ€§\n",
    "                max_new_tokens=32  # ç”±äºåªéœ€è¦å›ç­”é€‰é¡¹ï¼Œå¯ä»¥è®¾ç½®è¾ƒå°çš„å€¼\n",
    "            )\n",
    "            \n",
    "            # æå–å›ç­”(ä»…ä¿ç•™A/B/C/D)\n",
    "            extracted_answer = \"\"\n",
    "            for char in full_response:\n",
    "                if char in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                    extracted_answer = char\n",
    "                    break\n",
    "            \n",
    "            # åˆ¤æ–­å›ç­”æ˜¯å¦æ­£ç¡®\n",
    "            is_correct = extracted_answer == item[\"answer\"]\n",
    "            \n",
    "            # æ›´æ–°æ•°æ®é¡¹\n",
    "            item.update({\n",
    "                \"model_response\": full_response,\n",
    "                \"extracted\": extracted_answer,\n",
    "                \"is_correct\": is_correct\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"å¤„ç†ç¬¬{idx}ä¸ªé—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\")\n",
    "            item.update({\n",
    "                \"model_response\": \"\",\n",
    "                \"extracted\": \"\",\n",
    "                \"is_correct\": False\n",
    "            })\n",
    "    \n",
    "    # æ›´æ–°åŸå§‹æ•°æ®\n",
    "    data[\"data\"] = dataset\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # è®¡ç®—å‡†ç¡®ç‡\n",
    "    correct_count = sum(1 for item in dataset if item[\"is_correct\"])\n",
    "    accuracy = correct_count / len(dataset)\n",
    "    \n",
    "    print(f\"\\nè¯„ä¼°å®Œæˆ!\")\n",
    "    print(f\"æ€»é—®é¢˜æ•°: {len(dataset)}\")\n",
    "    print(f\"æ­£ç¡®æ•°é‡: {correct_count}\")\n",
    "    print(f\"å‡†ç¡®ç‡: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3319a1258c174f0d9ab129134bbc53d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/Gemma/lib/python3.12/site-packages/peft/tuners/adalora/config.py:78: UserWarning: Note that `r` is not used in AdaLora and will be ignored.If you intended to set the initial rank, use `init_r` instead.\n",
      "  warnings.warn(\n",
      "å¤„ç†é—®é¢˜:   0%|          | 0/1346 [00:00<?, ?it/s]/root/miniconda3/envs/Gemma/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n",
      "å¤„ç†é—®é¢˜: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1346/1346 [23:03<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è¯„ä¼°å®Œæˆ!\n",
      "æ€»é—®é¢˜æ•°: 1346\n",
      "æ­£ç¡®æ•°é‡: 708\n",
      "å‡†ç¡®ç‡: 52.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ç¤ºä¾‹:\n",
    "model_path = \"google/gemma-2-9b\"\n",
    "cache_dir = \"/root/autodl-tmp/gemma\"\n",
    "lora_path = \"/root/autodl-tmp/models/stage1/checkpoints/gemma-base-zh/checkpoint-43500\"\n",
    "model, tokenizer = initialize_model_and_tokenizer(\n",
    "    model_path=model_path,\n",
    "    cache_dir=cache_dir,\n",
    "    lora_path=lora_path,\n",
    "    use_quantization=True\n",
    ")\n",
    "\n",
    "process_dataset_with_model_responses(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. åˆ¤æ–­å¤§æ¨¡å‹æ˜¯å¦éµä»äº†æˆ‘ä»¬çš„æŒ‡ä»¤ï¼Œåªè¾“å‡ºABCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_strict_compliance_check(\n",
    "    input_file: str = \"ceval-exam/merged_val_data_with_prompts_and_responses.json\",\n",
    "    output_file: str = \"ceval-exam/merged_val_data_final.json\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    ä¸ºæ•°æ®é›†æ·»åŠ strict_complianceå­—æ®µï¼Œåˆ¤æ–­æ¨¡å‹å›ç­”æ˜¯å¦ä¸¥æ ¼éµå®ˆåªè¾“å‡ºABCDçš„è¦æ±‚\n",
    "    \n",
    "    Args:\n",
    "        input_file: è¾“å…¥çš„JSONæ–‡ä»¶è·¯å¾„\n",
    "        output_file: è¾“å‡ºçš„JSONæ–‡ä»¶è·¯å¾„\n",
    "    \"\"\"\n",
    "    # è¯»å–æ•°æ®\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    dataset = data[\"data\"]\n",
    "    \n",
    "    # ç”¨äºç»Ÿè®¡çš„è®¡æ•°å™¨\n",
    "    strict_compliance_count = 0\n",
    "    total_count = len(dataset)\n",
    "    \n",
    "    # å¤„ç†æ¯æ¡æ•°æ®\n",
    "    for item in tqdm(dataset, desc=\"æ£€æŸ¥ç­”æ¡ˆåˆè§„æ€§\"):\n",
    "        response = item[\"model_response\"].strip()\n",
    "        \n",
    "        # åˆ¤æ–­æ˜¯å¦ä¸¥æ ¼éµå®ˆè§„åˆ™ï¼ˆåªè¾“å‡ºA/B/C/Dä¸­çš„ä¸€ä¸ªï¼‰\n",
    "        is_strict = False\n",
    "        if response in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            is_strict = True\n",
    "            strict_compliance_count += 1\n",
    "            \n",
    "        # æ·»åŠ æ–°å­—æ®µ\n",
    "        item[\"strict_compliance\"] = is_strict\n",
    "    \n",
    "    # ä¿å­˜æ›´æ–°åçš„æ•°æ®\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯\n",
    "    compliance_rate = strict_compliance_count / total_count\n",
    "    print(\"\\nåˆè§„æ€§æ£€æŸ¥å®Œæˆ!\")\n",
    "    print(f\"æ€»æ ·æœ¬æ•°: {total_count}\")\n",
    "    print(f\"ä¸¥æ ¼åˆè§„æ ·æœ¬æ•°: {strict_compliance_count}\")\n",
    "    print(f\"åˆè§„ç‡: {compliance_rate:.2%}\")\n",
    "    \n",
    "    # è¾“å‡ºä¸€äº›ä¸åˆè§„çš„ç¤ºä¾‹ï¼Œæ–¹ä¾¿åˆ†æ\n",
    "    print(\"\\nä¸åˆè§„ç¤ºä¾‹:\")\n",
    "    non_compliant_samples = [item for item in dataset if not item[\"strict_compliance\"]]\n",
    "    for i, sample in enumerate(non_compliant_samples[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª\n",
    "        print(f\"\\nç¤ºä¾‹{i}:\")\n",
    "        print(f\"æ¨¡å‹å›ç­”: {sample['model_response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ£€æŸ¥ç­”æ¡ˆåˆè§„æ€§: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1346/1346 [00:00<00:00, 731950.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "åˆè§„æ€§æ£€æŸ¥å®Œæˆ!\n",
      "æ€»æ ·æœ¬æ•°: 1346\n",
      "ä¸¥æ ¼åˆè§„æ ·æœ¬æ•°: 1\n",
      "åˆè§„ç‡: 0.07%\n",
      "\n",
      "ä¸åˆè§„ç¤ºä¾‹:\n",
      "\n",
      "ç¤ºä¾‹1:\n",
      "æ¨¡å‹å›ç­”: ç­”æ¡ˆæ˜¯ Dã€‚\n",
      "\n",
      "ç¤ºä¾‹2:\n",
      "æ¨¡å‹å›ç­”: ç­”æ¡ˆä¸º C. ç´§ç¼©ä¸é›†ä¸­æˆ˜ç•¥ã€‚\n",
      "\n",
      "ç¤ºä¾‹3:\n",
      "æ¨¡å‹å›ç­”: D. åˆåŒçš„å½“äº‹äºº\n",
      "\n",
      "ç¤ºä¾‹4:\n",
      "æ¨¡å‹å›ç­”: ç­”æ¡ˆæ˜¯ B. 30\n",
      "\n",
      "ç¤ºä¾‹5:\n",
      "æ¨¡å‹å›ç­”: ç­”æ¡ˆæ˜¯ Cï¼Œå› ä¸ºæˆ˜ç•¥ç®¡ç†ä¸æ˜¯ä¸€æ¬¡æ€§çš„å·¥ä½œï¼Œè€Œæ˜¯éœ€è¦æŒç»­è¿›è¡Œçš„åŠ¨æ€è¿‡ç¨‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_strict_compliance_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. è¿‡æ»¤å¹¶åˆ’åˆ†æ•°æ®é›† "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(\n",
    "    input_file: str = \"ceval-exam/merged_val_data_final.json\",\n",
    "    train_file: str = \"ceval-exam/train_data.json\",\n",
    "    val_file: str = \"ceval-exam/val_data.json\",\n",
    "    val_ratio: float = 0.2,\n",
    "    random_seed: int = 42\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "    - è¿‡æ»¤æ‰strict_complianceä¸ºtrueçš„æ•°æ®\n",
    "    - ä¿æŒå„ç±»åˆ«æ•°æ®çš„æ¯”ä¾‹\n",
    "    - è¾“å‡ºè¯¦ç»†çš„ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡\n",
    "    \"\"\"\n",
    "    # è®¾ç½®éšæœºç§å­\n",
    "    import random\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # è¯»å–æ•°æ®\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # è¿‡æ»¤æ‰strict_complianceä¸ºtrueçš„æ•°æ®\n",
    "    dataset = [item for item in data[\"data\"] if not item[\"strict_compliance\"]]\n",
    "    print(f\"è¿‡æ»¤å‰æ•°æ®é‡: {len(data['data'])}\")\n",
    "    print(f\"è¿‡æ»¤åæ•°æ®é‡: {len(dataset)}\")\n",
    "    \n",
    "    # æŒ‰ç±»åˆ«åˆ†ç»„\n",
    "    category_data = {}\n",
    "    for item in dataset:\n",
    "        category = item[\"category\"]\n",
    "        if category not in category_data:\n",
    "            category_data[category] = []\n",
    "        category_data[category].append(item)\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªç±»åˆ«åˆ’åˆ†æ•°æ®\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    \n",
    "    for category, items in category_data.items():\n",
    "        # æ‰“ä¹±è¯¥ç±»åˆ«çš„æ•°æ®\n",
    "        random.shuffle(items)\n",
    "        \n",
    "        # è®¡ç®—éªŒè¯é›†å¤§å°\n",
    "        val_size = max(1, int(len(items) * val_ratio))  # ç¡®ä¿æ¯ä¸ªç±»åˆ«è‡³å°‘æœ‰1ä¸ªéªŒè¯æ ·æœ¬\n",
    "        \n",
    "        # åˆ’åˆ†æ•°æ®\n",
    "        val_data.extend(items[:val_size])\n",
    "        train_data.extend(items[val_size:])\n",
    "    \n",
    "    # æœ€åå†æ¬¡æ‰“ä¹±æ•´ä¸ªè®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "    random.shuffle(train_data)\n",
    "    random.shuffle(val_data)\n",
    "    \n",
    "    # ä¿å­˜æ•°æ®é›†\n",
    "    with open(train_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"data\": train_data}, f, ensure_ascii=False, indent=2)\n",
    "    with open(val_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"data\": val_data}, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯\n",
    "    print(\"\\næ•°æ®é›†åˆ’åˆ†å®Œæˆ!\")\n",
    "    print(f\"æ€»æ ·æœ¬æ•°: {len(dataset)}\")\n",
    "    print(f\"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_data)}\")\n",
    "    print(f\"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_data)}\")\n",
    "    \n",
    "    print(\"\\nç±»åˆ«åˆ†å¸ƒç»Ÿè®¡:\")\n",
    "    for category in category_data.keys():\n",
    "        # è®¡ç®—è¯¥ç±»åˆ«åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­çš„æ•°é‡\n",
    "        train_count = len([x for x in train_data if x[\"category\"] == category])\n",
    "        val_count = len([x for x in val_data if x[\"category\"] == category])\n",
    "        total_category_count = train_count + val_count\n",
    "        \n",
    "        # è®¡ç®—è¯¥ç±»åˆ«å æ€»ä½“çš„æ¯”ä¾‹\n",
    "        train_category_ratio = train_count / len(train_data)\n",
    "        val_category_ratio = val_count / len(val_data)\n",
    "        \n",
    "        print(f\"\\n{category}:\")\n",
    "        print(f\"  è®­ç»ƒé›†: {train_count}æ¡\")\n",
    "        print(f\"    - å è¯¥ç±»åˆ«æ€»æ•°çš„ {train_count/total_category_count:.1%}\")\n",
    "        print(f\"    - å è®­ç»ƒé›†æ€»æ•°çš„ {train_category_ratio:.1%}\")\n",
    "        print(f\"  éªŒè¯é›†: {val_count}æ¡\")\n",
    "        print(f\"    - å è¯¥ç±»åˆ«æ€»æ•°çš„ {val_count/total_category_count:.1%}\")\n",
    "        print(f\"    - å éªŒè¯é›†æ€»æ•°çš„ {val_category_ratio:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¿‡æ»¤å‰æ•°æ®é‡: 1346\n",
      "è¿‡æ»¤åæ•°æ®é‡: 1345\n",
      "\n",
      "æ•°æ®é›†åˆ’åˆ†å®Œæˆ!\n",
      "æ€»æ ·æœ¬æ•°: 1345\n",
      "è®­ç»ƒé›†æ ·æœ¬æ•°: 1107\n",
      "éªŒè¯é›†æ ·æœ¬æ•°: 238\n",
      "\n",
      "ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡:\n",
      "\n",
      "accountant_val:\n",
      "  è®­ç»ƒé›†: 40æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.6%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 3.6%\n",
      "  éªŒè¯é›†: 9æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.4%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 3.8%\n",
      "\n",
      "advanced_mathematics_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "art_studies_val:\n",
      "  è®­ç»ƒé›†: 27æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.8%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 2.4%\n",
      "  éªŒè¯é›†: 6æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.2%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 2.5%\n",
      "\n",
      "basic_medicine_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "business_administration_val:\n",
      "  è®­ç»ƒé›†: 27æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.8%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 2.4%\n",
      "  éªŒè¯é›†: 6æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.2%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 2.5%\n",
      "\n",
      "chinese_language_and_literature_val:\n",
      "  è®­ç»ƒé›†: 19æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 82.6%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.7%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 17.4%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "civil_servant_val:\n",
      "  è®­ç»ƒé›†: 38æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 80.9%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 3.4%\n",
      "  éªŒè¯é›†: 9æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 19.1%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 3.8%\n",
      "\n",
      "clinical_medicine_val:\n",
      "  è®­ç»ƒé›†: 18æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.8%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.6%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.2%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "college_chemistry_val:\n",
      "  è®­ç»ƒé›†: 20æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 83.3%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.8%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 16.7%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "college_economics_val:\n",
      "  è®­ç»ƒé›†: 44æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.5%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 4.0%\n",
      "  éªŒè¯é›†: 10æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.5%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 4.2%\n",
      "\n",
      "college_physics_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "college_programming_val:\n",
      "  è®­ç»ƒé›†: 30æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.1%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 2.7%\n",
      "  éªŒè¯é›†: 7æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.9%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 2.9%\n",
      "\n",
      "computer_architecture_val:\n",
      "  è®­ç»ƒé›†: 17æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.0%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.5%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 19.0%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "computer_network_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "discrete_mathematics_val:\n",
      "  è®­ç»ƒé›†: 13æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.2%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "education_science_val:\n",
      "  è®­ç»ƒé›†: 24æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 82.8%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 2.2%\n",
      "  éªŒè¯é›†: 5æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 17.2%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 2.1%\n",
      "\n",
      "electrical_engineer_val:\n",
      "  è®­ç»ƒé›†: 30æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.1%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 2.7%\n",
      "  éªŒè¯é›†: 7æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.9%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 2.9%\n",
      "\n",
      "environmental_impact_assessment_engineer_val:\n",
      "  è®­ç»ƒé›†: 25æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 80.6%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 2.3%\n",
      "  éªŒè¯é›†: 6æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 19.4%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 2.5%\n",
      "\n",
      "fire_engineer_val:\n",
      "  è®­ç»ƒé›†: 25æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 80.6%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 2.3%\n",
      "  éªŒè¯é›†: 6æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 19.4%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 2.5%\n",
      "\n",
      "high_school_biology_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "high_school_chemistry_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "high_school_chinese_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "high_school_geography_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "high_school_history_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 80.0%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 20.0%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "high_school_mathematics_val:\n",
      "  è®­ç»ƒé›†: 15æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 83.3%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 16.7%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "high_school_physics_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "high_school_politics_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "ideological_and_moral_cultivation_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "law_val:\n",
      "  è®­ç»ƒé›†: 20æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 83.3%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.8%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 16.7%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "legal_professional_val:\n",
      "  è®­ç»ƒé›†: 19æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 82.6%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.7%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 17.4%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "logic_val:\n",
      "  è®­ç»ƒé›†: 18æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.8%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.6%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.2%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "mao_zedong_thought_val:\n",
      "  è®­ç»ƒé›†: 20æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 83.3%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.8%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 16.7%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "marxism_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "metrology_engineer_val:\n",
      "  è®­ç»ƒé›†: 20æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 83.3%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.8%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 16.7%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "middle_school_biology_val:\n",
      "  è®­ç»ƒé›†: 17æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.0%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.5%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 19.0%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "middle_school_chemistry_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 80.0%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 20.0%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "middle_school_geography_val:\n",
      "  è®­ç»ƒé›†: 10æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 83.3%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 0.9%\n",
      "  éªŒè¯é›†: 2æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 16.7%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 0.8%\n",
      "\n",
      "middle_school_history_val:\n",
      "  è®­ç»ƒé›†: 18æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.8%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.6%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.2%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "middle_school_mathematics_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "middle_school_physics_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "middle_school_politics_val:\n",
      "  è®­ç»ƒé›†: 17æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.0%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.5%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 19.0%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "modern_chinese_history_val:\n",
      "  è®­ç»ƒé›†: 19æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 82.6%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.7%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 17.4%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "operating_system_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "physician_val:\n",
      "  è®­ç»ƒé›†: 40æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.6%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 3.6%\n",
      "  éªŒè¯é›†: 9æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.4%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 3.8%\n",
      "\n",
      "plant_protection_val:\n",
      "  è®­ç»ƒé›†: 18æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.8%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.6%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.2%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n",
      "\n",
      "probability_and_statistics_val:\n",
      "  è®­ç»ƒé›†: 15æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 83.3%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 16.7%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "professional_tour_guide_val:\n",
      "  è®­ç»ƒé›†: 24æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 82.8%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 2.2%\n",
      "  éªŒè¯é›†: 5æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 17.2%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 2.1%\n",
      "\n",
      "sports_science_val:\n",
      "  è®­ç»ƒé›†: 16æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 84.2%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.4%\n",
      "  éªŒè¯é›†: 3æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 15.8%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.3%\n",
      "\n",
      "tax_accountant_val:\n",
      "  è®­ç»ƒé›†: 40æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.6%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 3.6%\n",
      "  éªŒè¯é›†: 9æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.4%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 3.8%\n",
      "\n",
      "teacher_qualification_val:\n",
      "  è®­ç»ƒé›†: 36æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 81.8%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 3.3%\n",
      "  éªŒè¯é›†: 8æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 18.2%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 3.4%\n",
      "\n",
      "urban_and_rural_planner_val:\n",
      "  è®­ç»ƒé›†: 37æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 80.4%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 3.3%\n",
      "  éªŒè¯é›†: 9æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 19.6%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 3.8%\n",
      "\n",
      "veterinary_medicine_val:\n",
      "  è®­ç»ƒé›†: 19æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 82.6%\n",
      "    - å è®­ç»ƒé›†æ€»æ•°çš„ 1.7%\n",
      "  éªŒè¯é›†: 4æ¡\n",
      "    - å è¯¥ç±»åˆ«æ€»æ•°çš„ 17.4%\n",
      "    - å éªŒè¯é›†æ€»æ•°çš„ 1.7%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    split_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„\n",
    "project_root = \"/home/cuipeng/Gemma\"\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# å¯¼å…¥å¿…è¦æ¨¡å—\n",
    "from src.core.model.model_initializer import initialize_model_and_tokenizer\n",
    "from src.core.utils.model_utils import generate_response, apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import DPOTrainer, DPOConfig # type: ignore\n",
    "from transformers import TrainingArguments\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dpo_dataset(\n",
    "    train_file: str = \"ceval-exam/train_data.json\",\n",
    "    val_file: str = \"ceval-exam/val_data.json\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    å‡†å¤‡DPOè®­ç»ƒæ‰€éœ€çš„æ•°æ®é›†\n",
    "    \n",
    "    Args:\n",
    "        train_file: è®­ç»ƒé›†æ–‡ä»¶è·¯å¾„\n",
    "        val_file: éªŒè¯é›†æ–‡ä»¶è·¯å¾„\n",
    "    Returns:\n",
    "        åŒ…å«å¤„ç†åæ•°æ®é›†çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # ç›´æ¥è¯»å–JSONæ–‡ä»¶\n",
    "    import json\n",
    "    \n",
    "    # åŠ è½½è®­ç»ƒé›†\n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    # åŠ è½½éªŒè¯é›†\n",
    "    with open(val_file, 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "    \n",
    "    def process_function(examples):\n",
    "        return {\n",
    "            \"prompt\": examples[\"prompt\"],\n",
    "            \"chosen\": examples[\"extracted\"], \n",
    "            \"rejected\": examples[\"model_response\"],  # æ¨¡å‹çš„å®é™…å›ç­”ä½œä¸ºrejected response\n",
    "        }\n",
    "    \n",
    "    # print(type(train_dataset), train_dataset) # <class 'datasets.arrow_dataset.Dataset'> Dataset({features: ['data'], num_rows: 1107})\n",
    "\n",
    "    # å¤„ç†æ•°æ®é›†\n",
    "    # è½¬æ¢ä¸ºDatasetå¯¹è±¡\n",
    "    from datasets import Dataset\n",
    "    train_dataset = Dataset.from_list(train_data[\"data\"]).map(process_function)\n",
    "    eval_dataset = Dataset.from_list(val_data[\"data\"]).map(process_function)\n",
    "    # train_dataset = train_dataset.map(process_function)\n",
    "    # eval_dataset = eval_dataset.map(process_function)\n",
    "    \n",
    "    return {\n",
    "        \"train_dataset\": train_dataset,\n",
    "        \"eval_dataset\": eval_dataset\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_dpo(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_file: str = \"ceval-exam/train_data.json\",\n",
    "    val_file: str = \"ceval-exam/val_data.json\",\n",
    "    output_dir: str = \"../../../../../../../../../root/autodl-tmp/models/dpo_finetuned\",\n",
    "    batch_size: int = 2,\n",
    "    gradient_accumulation_steps: int = 4,\n",
    "    num_train_epochs: int = 5,\n",
    "    learning_rate: float = 5e-5,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨DPOæ–¹æ³•å¾®è°ƒæ¨¡å‹\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    # ç¡®ä¿æ¨¡å‹å®Œå…¨åŠ è½½åˆ°GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ®é›†\n",
    "    datasets = prepare_dpo_dataset(train_file, val_file)\n",
    "    \n",
    "    # è®¾ç½®DPOè®­ç»ƒå‚æ•°\n",
    "    training_args = DPOConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        remove_unused_columns=False,\n",
    "        logging_steps=10,\n",
    "\n",
    "        # æ·»åŠ ä¿å­˜æ£€æŸ¥ç‚¹\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=69,\n",
    "        # ä¿®æ”¹è¯„ä¼°ç­–ç•¥\n",
    "        evaluation_strategy=\"steps\",  # æ”¹ä¸ºæŒ‰æ­¥æ•°è¯„ä¼°\n",
    "        eval_steps=69,  # æ¯69æ­¥è¯„ä¼°ä¸€æ¬¡\n",
    "\n",
    "        # æ··åˆç²¾åº¦è®­ç»ƒè®¾ç½®\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        tf32=True,\n",
    "        # æ¨¡å‹å‚æ•°\n",
    "        max_prompt_length=512,\n",
    "        max_length=1024,\n",
    "        # è®­ç»ƒä¼˜åŒ–\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        max_grad_norm=1.0,\n",
    "        # æ˜¾å­˜ä¼˜åŒ–\n",
    "        deepspeed=None,\n",
    "        local_rank=-1,\n",
    "    )\n",
    "    \n",
    "    print(\"åˆå§‹åŒ–DPO Trainer...\")\n",
    "    # åˆå§‹åŒ–DPO Trainer\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=datasets[\"train_dataset\"],\n",
    "        eval_dataset=datasets[\"eval_dataset\"],\n",
    "    )\n",
    "    \n",
    "    # å¼€å§‹è®­ç»ƒ\n",
    "    print(\"å¼€å§‹DPOè®­ç»ƒ...\")\n",
    "    dpo_trainer.train()\n",
    "    \n",
    "    # ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "    dpo_trainer.save_model(output_dir)\n",
    "    print(f\"è®­ç»ƒå®Œæˆ! æ¨¡å‹å·²ä¿å­˜åˆ° {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62f4ae556f14c15a0c3fea0380030cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/Gemma/lib/python3.12/site-packages/peft/tuners/adalora/config.py:78: UserWarning: Note that `r` is not used in AdaLora and will be ignored.If you intended to set the initial rank, use `init_r` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0bbb1604f64df597ef376c6071b494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7095df6ed07348cca853338611c512ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆå§‹åŒ–DPO Trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/Gemma/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1777/1874018997.py:60: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  dpo_trainer = DPOTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e6a831d41c4fb1a4c7dde15a4118b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt from train dataset:   0%|          | 0/1107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25715dfb59e46c3a6624695837c549a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/1107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe6052e67d54b268ad7cc2d25377397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt from eval dataset:   0%|          | 0/238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2efe53811b0462c937502e849e56d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46adbcf0ff894ec69b396d544519ae69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbe46378aff43a68d6749a783ef420a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹DPOè®­ç»ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/Gemma/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 38/690 01:18 < 23:36, 0.46 it/s, Epoch 0.27/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "    model_path = \"google/gemma-2-9b\"\n",
    "    cache_dir = \"/root/autodl-tmp/gemma\"\n",
    "    lora_path = \"/root/autodl-tmp/models/stage1/checkpoints/gemma-base-zh/checkpoint-43500\"\n",
    "    model, tokenizer = initialize_model_and_tokenizer(\n",
    "        model_path=model_path,\n",
    "        cache_dir=cache_dir,\n",
    "        lora_path=lora_path,\n",
    "        use_quantization=False, # å¼€å¯é‡åŒ–ä¼šå¯¼è‡´ï¼šRuntimeError: value cannot be converted to type at::Half without overflow\"\n",
    "        device_map = \"cuda:0\"\n",
    "    )\n",
    "    \n",
    "    # å¼€å§‹DPOè®­ç»ƒ\n",
    "    train_with_dpo(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. è¯„ä¼° - åœ¨valæ•°æ®é›†ä¸Šéµå®ˆæŒ‡ä»¤çš„ç¨‹åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_improvement(\n",
    "    base_model_path: str,\n",
    "    dpo_model_path: str,\n",
    "    val_file: str,\n",
    "    cache_dir: str,\n",
    "    batch_size: int = 4\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    è¯„ä¼°DPOå¾®è°ƒå‰åæ¨¡å‹çš„è¡¨ç°\n",
    "    \n",
    "    Args:\n",
    "        base_model_path: åŸå§‹æ¨¡å‹è·¯å¾„\n",
    "        dpo_model_path: DPOå¾®è°ƒåçš„æ¨¡å‹è·¯å¾„\n",
    "        val_file: éªŒè¯é›†æ–‡ä»¶è·¯å¾„\n",
    "        cache_dir: ç¼“å­˜ç›®å½•\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # åŠ è½½éªŒè¯é›†æ•°æ®\n",
    "    with open(val_file, 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)[\"data\"]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # åŠ è½½åŸå§‹æ¨¡å‹å’Œå¾®è°ƒåçš„æ¨¡å‹\n",
    "    print(\"åŠ è½½åŸå§‹æ¨¡å‹...\")\n",
    "    base_model, tokenizer = initialize_model_and_tokenizer(\n",
    "        model_path=base_model_path,\n",
    "        cache_dir=cache_dir,\n",
    "        use_quantization=False\n",
    "    )\n",
    "    \n",
    "    print(\"åŠ è½½DPOå¾®è°ƒåçš„æ¨¡å‹...\")\n",
    "    dpo_model, _ = initialize_model_and_tokenizer(\n",
    "        model_path=base_model_path,\n",
    "        cache_dir=cache_dir,\n",
    "        lora_path=dpo_model_path,\n",
    "        use_quantization=False\n",
    "    )\n",
    "    \n",
    "    # ç”Ÿæˆå‡½æ•°\n",
    "    def generate_answer(model, prompt):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # åœ¨éªŒè¯é›†ä¸Šæµ‹è¯•ä¸¤ä¸ªæ¨¡å‹\n",
    "    print(\"å¼€å§‹è¯„ä¼°...\")\n",
    "    for idx, item in enumerate(tqdm(val_data[:batch_size])):  # ä¸ºäº†å¿«é€Ÿæµ‹è¯•ï¼Œåªå–batch_sizeä¸ªæ ·æœ¬\n",
    "        prompt = item[\"prompt\"]\n",
    "        golden_answer = item[\"answer\"]\n",
    "        \n",
    "        # ç”Ÿæˆä¸¤ä¸ªæ¨¡å‹çš„å›ç­”\n",
    "        base_answer = generate_answer(base_model, prompt)\n",
    "        dpo_answer = generate_answer(dpo_model, prompt)\n",
    "        \n",
    "        results.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"golden_answer\": golden_answer,\n",
    "            \"base_model_answer\": base_answer,\n",
    "            \"dpo_model_answer\": dpo_answer\n",
    "        })\n",
    "        \n",
    "        print(f\"\\næ ·æœ¬ {idx + 1}:\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"æ ‡å‡†ç­”æ¡ˆ: {golden_answer}\")\n",
    "        print(f\"åŸå§‹æ¨¡å‹ç­”æ¡ˆ: {base_answer}\")\n",
    "        print(f\"DPOæ¨¡å‹ç­”æ¡ˆ: {dpo_answer}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    output_file = \"model_comparison_results.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ° {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ç¤ºä¾‹ï¼š\n",
    "if __name__ == \"__main__\":\n",
    "    base_model_path = \"google/gemma-2b-it\"\n",
    "    cache_dir = \"/root/autodl-tmp/gemma\"\n",
    "    dpo_model_path = \"../../../../../../../../../../../../root/autodl-tmp/models/dpo_finetuned\"\n",
    "    val_file = \"ceval-exam/val_data.json\"\n",
    "    \n",
    "    evaluate_model_improvement(\n",
    "        base_model_path=base_model_path,\n",
    "        dpo_model_path=dpo_model_path,\n",
    "        val_file=val_file,\n",
    "        cache_dir=cache_dir,\n",
    "        batch_size=4  # å¯ä»¥è°ƒæ•´æµ‹è¯•æ ·æœ¬æ•°é‡\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
