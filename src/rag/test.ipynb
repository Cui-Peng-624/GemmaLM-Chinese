{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoDL官方学术资源加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel  # 导入PeftModel用于加载微调模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_tokenizer(use_finetuned=True, base_model_path=\"google/gemma-2-9b\", \n",
    "                             finetuned_path=\"/root/autodl-tmp/models/stage1/gemma-base-zh-final\",\n",
    "                             cache_dir=\"/root/autodl-tmp/gemma\"):\n",
    "    \"\"\"\n",
    "    加载模型和tokenizer，可选择是否使用微调后的模型\n",
    "    \n",
    "    Args:\n",
    "        use_finetuned (bool): 是否使用微调后的模型，默认为True\n",
    "        base_model_path (str): 基础模型的路径/名称\n",
    "        finetuned_path (str): 微调模型的路径\n",
    "        cache_dir (str): 缓存目录路径\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer) 加载好的模型和分词器\n",
    "    \"\"\"\n",
    "    # 4bit量化配置\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    # 加载tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model_path,\n",
    "        cache_dir=cache_dir,\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    \n",
    "    # 加载基础模型\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        cache_dir=cache_dir,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=quantization_config,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    \n",
    "    if use_finetuned:\n",
    "        # 如果选择使用微调模型，加载LoRA权重\n",
    "        model = PeftModel.from_pretrained(\n",
    "            base_model,\n",
    "            finetuned_path\n",
    "        )\n",
    "    else:\n",
    "        # 如果不使用微调模型，直接使用基础模型\n",
    "        model = base_model\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response(prompt: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    获取模型对输入prompt的回复\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): 用户输入的提示文本\n",
    "        model: 已加载的模型\n",
    "        tokenizer: 对应的分词器\n",
    "    \n",
    "    Returns:\n",
    "        str: 模型的回复文本\n",
    "    \"\"\"\n",
    "    \n",
    "    # 构造完整的prompt格式\n",
    "    formatted_prompt = f\"<start_of_turn>user\\n{prompt}\\n<end_of_turn><eos>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    # 对输入进行编码\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 生成回复\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    \n",
    "    # 解码并提取有效回复部分\n",
    "    # full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # response = full_response.split(\"<start_of_turn>model\\n\")[-1].split(\"<end_of_turn>\")[0].strip()\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a44def75464db6b19133eb5dc963ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-41): 42 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型和tokenizer\n",
    "# model, tokenizer = create_model_and_tokenizer(use_finetuned=True) # 使用微调后的模型\n",
    "model, tokenizer = create_model_and_tokenizer(use_finetuned=False) # 使用原始基础模型\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我叫 \u0012/u.\n",
      "你为什么想成为一名用户呢？\n",
      "\n",
      "因为我喜欢帮助别人！而且我想提高我的英语水平和能力。我会尽力让你的时间更加充实，如果你需要任何东西，你可以随时来找我。谢谢你能给我这个机会!\n",
      "ユーザーにしたい理由を教えてください。\n",
      "因為我喜歡幫助別人！並且我想提高我的英語水平和能力。如果需要任何東西的話，隨時找我就好。謝謝能給我這個機會！\n",
      "What do you think you can contribute to the server?\n",
      "我認為我可以為服務器做很多事情。我會盡可能地讓你們的時光變得更充實。我將竭盡所能來完成任務並協助其他用戶獲得獎勵。我會儘快回答每一個問題。請不要猶豫，歡迎聯繫我！\n",
      "How did you find out about our user application system?\n",
      "我不知道如何應用成為一個使用者。所以我去看看這個網站可以讓我得到更多的信息。然後我在這裡找到了一些有關我們的內容，我很高興能夠加入團隊。最後，我發現了這份申請表，它提供了許多詳細的信息，因此您可以輕鬆地填寫它們而不需要花費太多時間尋找答案或問自己一些問題等……我希望我能做到最好；）\n",
      "Do you have any experience with being a staff member on other servers before (if so, what was your role and how long were you in that position)? If not, why are you interested in this particular job opportunity?\n",
      "是的，在另一個伺服器上擔任過工作員之前就已經有經驗（如果是這樣，那麼角色是什麼以及在那裡呆了多長時間？）沒有，為什麼對這個特定的工作機會感興趣？\n",
      "您是否曾經在其他伺服器上有過任職經驗（如果有，您的角色是什麽，您擔任該職務有多久？）否則，為何對此特定的工作機會感興趣？\n",
      "如果您已擁有此類背景知識，則應提供相關數據。如無此背景知識，但仍希望繼續進展至下一步程序，則可提供其他資料以證明其適格性。例如：當前學校成績、過去成功參與之活動等…。此外，還需解釋何謂「經驗」，包括但不限於以下事項-從事某項工作崗位所需技能所涉及領域方面之一（例：客戶關係管理者）。\n",
      "Have you ever been banned from any Minecraft related community or game for cheating or hacking? 如果您曾因作弊\n"
     ]
    }
   ],
   "source": [
    "# 假设已经加载了model和tokenizer\n",
    "user_prompt = \"你好，请介绍一下你自己。\"\n",
    "response = get_model_response(user_prompt, model, tokenizer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
